@inproceedings{Sudarsanam2023_EUSIPCO,
    author = "Sudarsanam, Parthasaarathy and Virtanen, Tuomas",
    title = "Attention-Based Methods For Audio Question Answering",
    booktitle = "2023 31st European Signal Processing Conference (EUSIPCO)",
    pages = "750--754",
    year = "2023",
    organization = "IEEE"
}

@inbook{Heittola2018,
    author = "Heittola, Toni and Cakir, Emre and Virtanen, Tuomas",
    abstract = "This chapter explains the basic concepts in computational methods used for analysis of sound scenes and events. Even though the analysis tasks in many applications seem different, the underlying computational methods are typically based on the same principles. We explain the commonalities between analysis tasks such as sound event detection, sound scene classification, or audio tagging. We focus on the machine learning approach, where the sound categories (i.e., classes) to be analyzed are defined in advance. We explain the typical components of an analysis system, including signal pre-processing, feature extraction, and pattern classification. We also preset an example system based on multi-label deep neural networks, which has been found to be applicable in many analysis tasks discussed in this book. Finally, we explain the whole processing chain that involves developing computational audio analysis systems.",
    booktitle = "Computational Analysis of Sound Scenes and Events",
    doi = "10.1007/978-3-319-63450-0\_2",
    editor2 = "Tuomas Virtanen and Plumbley, Mark D. and Dan Ellis",
    isbn = "978-3-319-63449-4",
    month = "9",
    pages = "13--40",
    publisher = "Springer",
    title = "The machine learning approach for analysis of sound scenes and events",
    year = "2018"
}

@inproceedings{Silen2012_InterSpecch,
    author = "Silen, Hanna and Helander, Elina and Nurminen, Jani and Gabbouj, Moncef",
    booktitle = "Proceedings of 13th Annual Conference of the International Speech Communication Association, Interspeech 2012, September 9 - 13, Portland, Oregon, USA",
    pages = "1--4",
    publisher = "International Speech Communication Association ISCA",
    series = "Interspeech",
    title = "{W}ays to {I}mplement {G}lobal {V}ariance in {S}tatistical {S}peech {S}ynthesis",
    year = "2012"
}

@article{Purwins2019_JSTSP,
    author = {Purwins, Hendrik and Li, Bo and Virtanen, Tuomas and Sch{\"u}ller, Jan and Chang, Shuo-yiin and Sainath, Tara},
    abstract = "Given the recent surge in developments of deep learning, this paper provides a review of the state-of-the-art deep learning techniques for audio signal processing. Speech, music, and environmental sound processing are considered side-by-side, in order to point out similarities and differences between the domains, highlighting general methods, problems, key references, and potential for cross fertilization between areas. The dominant feature representations (in particular, log-mel spectra and raw waveform) and deep learning models are reviewed, including convolutional neural networks, variants of the long short-term memory architecture, as well as more audio-specific neural network models. Subsequently, prominent deep learning application areas are covered, i.e., audio recognition (automatic speech recognition, music information retrieval, environmental sound detection, localization and tracking) and synthesis and transformation (source separation, audio enhancement, generative models for speech, sound, and music synthesis). Finally, key issues and future questions regarding deep learning applied to audio signal processing are identified.",
    day = "1",
    doi = "10.1109/JSTSP.2019.2908700",
    issn = "1932-4553",
    journal = "IEEE Journal of Selected Topics in Signal Processing",
    keywords = "audio enhancement;automatic speech recognition;connectionist temporal memory;Deep learning;environmental sounds;music information retrieval;source separation",
    month = "5",
    number = "2",
    pages = "206--219",
    publisher = "Institute of Electrical and Electronics Engineers",
    title = "Deep Learning for Audio Signal Processing",
    volume = "13",
    year = "2019",
    url = "https://arxiv.org/abs/1905.00078"
}

@inproceedings{Mimilakis2017_MLSP,
    author = "Mimilakis, Stylianos Ioannis and Drossos, Konstantinos and Virtanen, Tuomas and Schuller, Gerald",
    abstract = "The objective of deep learning methods based on encoder-decoder architectures for music source separation is to approximate either ideal time-frequency masks or spectral representations of the target music source(s). The spectral representations are then used to derive time-frequency masks. In this work we introduce a method to directly learn time-frequency masks from an observed mixture magnitude spectrum. We employ recurrent neural networks and train them using prior knowledge only for the magnitude spectrum of the target source. To assess the performance of the proposed method, we focus on the task of singing voice separation. The results from an objective evaluation show that our proposed method provides comparable results to deep learning based methods which operate over complicated signal representations. Compared to previous methods that approximate time-frequency masks, our method has increased performance of signal to distortion ratio by an average of 3.8 dB.",
    booktitle = "27th IEEE International Workshop on Machine Learning for Signal Processing (MLSP)",
    doi = "10.1109/MLSP.2017.8168117",
    keywords = "singing voice separation;",
    publisher = "IEEE",
    title = "A Recurrent Encoder-Decoder Approach With Skip-Filtering Connections for Monaural Singing Voice Separation",
    year = "2017",
    url = "https://arxiv.org/pdf/1709.00611.pdf"
}

@inproceedings{Rosti2000_EUSIPCO,
    author = "Rosti, Antti-Veikko and Koivunen, Visa",
    editor = "Gabbouj, M. and Kuosmanen, P.",
    abstract = "Modulation classification has many important applications in communications, e.g., reconfigurable receivers, spectrum managment and interference cancellation. In this paper we address the problem of classifying digitally modulated signals using cyclostationary statistics. We derive the first-order moments of the complex envelope of digitally modulated signals and verify their periodicity. A novel feature for the classification of the frequency shift keyed signals is proposed. The performance of this feature in distinguishing among different FSK constellations is studied in simulation. Some comparisons to commonly used features are performed.",
    booktitle = "Signal Processing X Theories and Applications, Proceedings of EUSIPCO 2000, tenth European Signal processing Conference, 4-8 September 2000, Tampere, Finland",
    isbn = "952-15-0443-9",
    pages = "581--584",
    title = "Classification of mfsk modulated signals using the mean of complex envelope",
    year = "2000"
}

@inproceedings{Gemmeke2010_Interspeech 2010,
    author = "Gemmeke, Jort and Virtanen, Tuomas",
    booktitle = "Proceedings of the 11th Annual Conference of the International Speech Communication Association (Interspeech 2010)",
    pages = "2082--2085",
    title = "Artificial and online acquired noise dictionaries for noise robust {ASR}",
    year = "2010",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/noise\_dictionaries.pdf"
}

@inproceedings{Mahkonen2016_EUSIPCO,
    author = {Mahkonen, Katariina and Hurmalainen, Antti and Virtanen, Tuomas and K{\"a}m{\"a}r{\"a}inen, Joni-Kristian},
    abstract = "Sparse representations have been found to provide high classification accuracy in many fields. Their drawback is the high computational load. In this work, we propose a novel cascaded classifier structure to speed up the decision process while utilizing sparse signal representation. In particular, we apply the cascaded decision process for noise robust automatic speech recognition task. The cascaded decision process is implemented using a feedforward neural network (NN) and time sparse versions of a non-negative matrix factorization (NMF) based sparse classification method of [1]. The recognition accuracy of our cascade is among the three best in the recent CHiME2013 benchmark and obtains six times faster the accuracy of NMF alone as in [1].",
    booktitle = "European Signal Processing Conference (EUSIPCO), 2016",
    doi = "10.1109/EUSIPCO.2016.7760660",
    publisher = "IEEE",
    title = "Cascade processing for speeding up sliding window sparse classification",
    year = "2016",
    url = "http://vision.cs.tut.fi/data/publications/eusipco2016\_cascaded\_nmf.pdf"
}

@book{Klapuri2006,
    author = "Klapuri, Anssi and Davy, Manuel",
    title = "Signal Processing Methods for Music Transcription",
    year = "2006",
    isbn = "0387306676",
    publisher = "Springer-Verlag",
    address = "Berlin, Heidelberg"
}

@mastersthesis{Ryynänen2004_master,
    author = {Ryyn{\"a}nen, Matti},
    abstract = "This thesis concerns the problem of automatic transcription of music and proposes a method for transcribing monophonic melodies. Recently, computational music content analysis has received considerable attention among researchers due to the rapid growth of music databases. In this area of research, melody transcription plays an important role. Automatic melody transcription can beneﬁt professional musicians and provide interesting applications for consumers, including music retrieval and interactive music applications. The proposed method is based on two probabilistic models: a note event model and a musicological model. The note event model is used to represent note candidates in melodies, and it is based on hidden Markov models. The note event model is trained with an acoustic database of singing sequences, but the training can be done for any melodic instrument and for the desired front-end feature extractors. The musicological model is used to control the transitions between the note candidates by using musical key estimation and by computing the likelihoods of note sequences. The two models are combined to constitute a melody transcription system with a modular architecture. The transcription system is evaluated, and the results are good. Our system transcribes correctly over 90 \% of notes, thus halving the amount of errors compared to a simple rounding of pitch estimates to the nearest MIDI note numbers. Particularly, using the note event model signiﬁcantly improves the system performance.",
    title = "{P}robabilistic {M}odelling of {N}ote {E}vents in the {T}ranscription of {M}onophonic {M}elodies",
    year = "2004",
    school = "Tampere University of Technology"
}

@phdthesis{Ryynänen2009_phd,
    author = {Ryyn{\"a}nen, Matti},
    abstract = "Transcription of music refers to the analysis of a music signal in order to produce a parametric representation of the sounding notes in the signal. This is conventionally carried out by listening to a piece of music and writing down the symbols of common musical notation to represent the occurring notes in the piece. Automatic transcription of music refers to the extraction of such representations using signal-processing methods. This thesis concerns the automatic transcription of pitched notes in musical audio and its applications. Emphasis is laid on the transcription of realistic polyphonic music, where multiple pitched and percussive instruments are sounding simultaneously. The methods included in this thesis are based on a framework which combines both low-level acoustic modeling and high-level musicological modeling. The emphasis in the acoustic modeling has been set to note events so that the methods produce discrete-pitch notes with onset times and durations as output. Such transcriptions can be efficiently represented as MIDI files, for example, and the transcriptions can be converted to common musical notation via temporal quantization of the note onsets and durations. The musicological model utilizes musical context and trained models of typical note sequences in the transcription process. Based on the framework, this thesis presents methods for generic polyphonic transcription, melody transcription, and bass line transcription. A method for chord transcription is also presented. All the proposed methods have been extensively evaluated using realistic polyphonic music. In our evaluations with 91 half-a-minute music excerpts, the generic polyphonic transcription method correctly found 39\% of all the pitched notes (recall) where 41\% of the transcribed notes were correct (precision). Despite the seemingly low recognition rates in our simulations, this method was top-ranked in the polyphonic note tracking task in the international MIREX evaluation in 2007 and 2008. The methods for the melody, bass line, and chord transcription were evaluated using hours of music, where F-measure of 51\% was achieved for both melodies and bass lines. The chord transcription method was evaluated using the first eight albums by The Beatles and it produced correct frame-based labeling for about 70\% of the time. The transcriptions are not only useful as human-readable musical notation but in several other application areas too, including music information retrieval and content-based audio modification. This is demonstrated by two applications included in this thesis. The first application is a query by humming system which is capable of searching melodies similar to a user query directly from commercial music recordings. In our evaluation with a database of 427 full commercial audio recordings, the method retrieved the correct recording in the topthree list for the 58\% of 159 hummed queries. The method was also top-ranked in “query by singing/humming” task in MIREX 2008 for a database of 2048 MIDI melodies and 2797 queries. The second application uses automatic melody transcription for accompaniment and vocals separation. The transcription also enables tuning the user singing to the original melody in a novel karaoke application.",
    title = "{A}utomatic {T}ranscription of {P}itch {C}ontent in {M}usic and {S}elected {A}pplications",
    year = "2009",
    school = "Tampere University of Technology"
}

@inproceedings{Heittola2011,
    author = "Heittola, Toni and Mesaros, Annamaria and Virtanen, Tuomas and Eronen, Antti",
    booktitle = {Akustiikkap{\"a}iv{\"a}t 2011},
    pages = "51--56",
    title = "Sound event detection and context recognition",
    year = "2011"
}

@article{Nikunen2016_ICA,
    author = "Nikunen, Joonas and Diment, Aleksandr and Virtanen, Tuomas and Vilermo, Miikka",
    journal = "Speech Communication",
    pages = "157--169",
    title = "Binaural rendering of microphone array captures based on source separation",
    url = "http://www.sciencedirect.com/science/article/pii/S0167639315001004",
    volume = "76",
    year = "2016"
}

@inproceedings{Drossos2017_WASPAA,
    author = "Drossos, Konstantinos and Adavanne, Sharath and Virtanen, Tuomas",
    abstract = "We present the first approach to automated audio captioning. We employ an encoder-decoder scheme with an alignment model in between. The input to the encoder is a sequence of log mel-band energies calculated from an audio file, while the output is a sequence of words, i.e. a caption. The encoder is a multi-layered, bi-directional gated recurrent unit (GRU) and the decoder a multi-layered GRU with a classification layer connected to the last GRU of the decoder. The classification layer and the alignment model are fully connected layers with shared weights between timesteps. The proposed method is evaluated using data drawn from a commercial sound effects library, ProSound Effects. The resulting captions were rated through metrics utilized in machine translation and image captioning fields. Results from metrics show that the proposed method can predict words appearing in the original caption, but not always correctly ordered.",
    booktitle = "IEEE Workshop on Applications of Signal Processing to Audio and Acoustics",
    doi = "10.1109/WASPAA.2017.8170058",
    isbn = "978-1-5386-1632-1",
    keywords = "audio captioning",
    publisher = "IEEE",
    title = "Automated Audio Captioning with Recurrent Neural Networks",
    year = "2017",
    url = "https://arxiv.org/abs/1706.10006"
}

@phdthesis{Bilcu2008_phd,
    author = {Bilcu, Enik{\"o} Beatrice},
    abstract = "Text-to-phoneme (TTP) mapping, also called grapheme-to-phoneme (GTP) conversion, defines the process of transforming a written text into its corresponding phonetic transcription. Text-to-phoneme mapping is a necessary step in any state-of-the-art automatic speech recognition (ASR) and text-to-speech (TTS) system, where the textual information changes dynamically (i.e., new contact entries for name dialing, or new short messages or emails to be read out by a device). There are significant differences between the implementation requirements of a text-to-phoneme mapping module embedded into the automatic speech recognition and into the text-to-speech systems: in automatic speech recognition systems the errors of the text-to-phoneme mapping module are tolerated better (leading to occasional recognition errors) than in the text-to-speech applications, where the effect is immediately and in all cases audible. Automatic speech recognition systems typically use text-to-phoneme mapping to lower the footprint (to avoid storing the lexicon), while maintaining quality. The use of text-to-phoneme mapping in the text-to-speech systems is different. In addition to the phonetic information, the text-to-speech systems also need prosodic information to be able to produce high quality speech, which cannot be predicted by text-to-phoneme mapping. Most state-of-the-art text-to-speech systems use explicit pronunciation lexicon, which is aimed at providing the widest possible coverage, in the order of 100K words, with high quality pronunciation information. Because of this reason, text-to-phoneme mapping is typically used as a fall-back strategy, when the system encounters very rare or non-native words and the quality of a ext-to-speech system is indirectly affected by the quality of the grapheme-to-phoneme conversion. Another important issue is the question of training the text-to-phoneme mapping module. The problem of grapheme-to-phoneme conversion is a static one and such a system is trained off-line. The correspondence between the written and spoken form of a language is usually unchanged in the lifetime of an application. So the complexity/speed of the model training is of secondary importance compared to e.g., the speed of convergence or model size. In this thesis, the problem of text-to-phoneme mapping using neural networks is studied. One of the main goals of the thesis is to provide a comprehensive analysis of different neural network structures which can be implemented to convert a written text into its corresponding phonetic transcription. Another important target, of this work, is to provide new solutions that improve the performance of the existing algorithms, in terms of convergence speed and phoneme accuracy. Three main neural network classes are studied in this thesis: the multilayer perceptron (MLP) neural network, the recurrent neural network (RNN) and the bidirectional recurrent neural network (BRNN). Due to their ability of self adaptation, neural networks have been shown to be a viable solution in applications that require modeling abilities. Such an application is the text-tophoneme mapping where the correspondence between letters of a written text and their corresponding phonetic transcription must be modeled. One of the main concerns in all practical implementations, where neural networks are used, is to develop algorithms which provide fast convergence of the synaptic weights and in the same time good mapping performances. When a neural network is trained for text-to-phoneme mapping, at every iteration, a letter-phoneme pair is presented to the network such that, the number of letters and the number of training iterations are equal. As a result, fast convergence of the neural network means smaller size of the training dictionary since fast convergence is in fact similar to less necessary training letters1. A fast convergence speed is important in applications where only a small linguistic database is available. Of course, one solution could be to use a small dictionary (with very few words) which is presented at the input of the neural network many times until the convergence of the synaptic weights is reached. In this case the time of training becomes more important. Taking into account these two sides of the convergence speed (the size of the training dictionary and the processing time during training) one can understand the importance of having algorithms that ensure fast convergence of the neural network. It is well known that the error back-propagation algorithm which is used to train the MLP neural network, possess sometimes a quite slow convergence (a very large number of iterations required to reach the stability point). In order to increase the convergence speed two novel alternative solutions are proposed in this thesis: one using an adaptive learning rate in the training process and another which is a transform domain implementation of the multilayer perceptron neural network. The computational complexity of the two proposed training algorithms is slightly higher than the computational complexity of the error back-propagation algorithm but the number of training iterations is highly reduced. Due to this fact, although the three algorithms might have the same training time, the novel algorithms necessitate smaller training dictionary. Due to the limitations of the processing power that usually are encountered in real devices, another very important requirement for a text-to-phoneme mapping system is to have low computational and memory costs. In the case of text-to-phoneme mapping systems based in neural networks, the computational complexity is mainly linked to the mathematical complexity of the training algorithm as well as to the number of the synaptic weights of the neural network. Memory load is due to the number of synaptic weights of the neural network which must be stored. Taking into account all these limitations and implementation requirements, in this thesis, several neural network structures with different number of synaptic weights and trained with various training algorithms, are studied. The modeling capability of the neural networks is addressed, which is translated in the text-to-phoneme mapping case into the phoneme accuracy. Different neural network structures, training algorithms and network complexities are analyzed also from this point of view. As a remark here, we mention that input letter encoding plays a very important role in the phoneme accuracy of the grapheme-to-phoneme conversion system. This is why special attention has been paid to the comparative analysis of the performances (in terms of phoneme accuracy) obtained with several orthogonal and non-orthogonal encoding of the input letters. The thesis is structured into four main parts. Chapter 1 brings the reader into the world of text-to-phoneme mapping. In Chapter 2 several different neural network structures and their corresponding training algorithms are described and two new training algorithms are introduced and analyzed. In Chapter 3 the experimental results, for the problem of monolingual text-to-phoneme mapping, obtained with the neural networks described in Chapter 2 are shown. Chapter 4 is dedicated to the problem of bilingual grapheme-to-phoneme conversion and Chapter 5 concludes the thesis.",
    title = "{T}ext-{T}o-{P}honeme {M}apping {U}sing {N}eural {N}etworks",
    school = "Tampere University of Technology",
    url = "https://cris.tuni.fi/ws/portalfiles/portal/2313141/bilcub.pdf",
    year = "2008"
}

@article{Pirinen2008_SJ,
    author = "Pirinen, Tuomo",
    doi = "10.1109/JSEN.2008.2007677",
    issn = "1530-437X",
    journal = "IEEE Sensors Journal",
    number = "12",
    pages = "2008--2015",
    publisher = "IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC",
    title = "{A} confidence statistic and an outlier detector for difference estimates in sensor arrays",
    volume = "8",
    year = "2008"
}

@mastersthesis{Eronen2001_master,
    author = "Eronen, Antti",
    abstract = "This thesis concerns the automatic recognition of musical instruments, where the idea is to build computer systems that “listen” to musical sounds and recognize which instrument is playing. Experimental material consisted of 5286 single notes from Western orchestral instruments, the timbre of which have been studied in great depth. The literature review part of this thesis introduces the studies on the sound of musical instruments, as well as related knowledge on instrument acoustics. Together with the state-of-the-art in automatic sound source recognition systems, these form the foundation for the most important part of this thesis: the extraction of perceptually relevant features from acoustic musical signals. Several different feature extraction algorithms were implemented and developed, and used as a front-end for a pattern recognition system. The performance of the system was evaluated in several experiments. Using feature vectors that included cepstral coefﬁcients and features relating to the type of excitation, brightness, modulations, asynchronity and fundamental frequency of tones, an accuracy of 35 \% was obtained on a database including several examples of 29 instruments. The recognition of the family of the instrument between six possible classes was successful in 77 \% of the cases. The performance of the system and the confusions it made were compared to the results reported for human perception. The comparison shows that the performance of the system is worse than that of humans in a similar task (46 \% in individual instrument and 92 \% in instrument family recognition [Martin99]), although it is comparable to the performance of other reported systems. Confusions of the system resemble those of human subjects, indicating that the feature extraction algorithms have managed to capture perceptually relevant information from the acoustic signals.",
    title = "{A}utomatic musical instrument recognition",
    year = "2001",
    school = "Tampere University of Technology"
}

@phdthesis{Eronen2009_phd,
    author = "Eronen, Antti",
    abstract = "Signal processing methods for audio classification and music content analysis are developed in this thesis. Audio classification is here understood as the process of assigning a discrete category label to an unknown recording. Two specific problems of audio classification are considered: musical instrument recognition and context recognition. In the former, the system classifies an audio recording according to the instrument, e.g. violin, flute, piano, that produced the sound. The latter task is about classifying an environment, such a car, restaurant, or library, based on its ambient audio background. In the field of music content analysis, methods are presented for music meter analysis and chorus detection. Meter analysis methods consider the estimation of the regular pattern of strong and weak beats in a piece of music. The goal of chorus detection is to locate the chorus segment in music which is often the catchiest and most memorable part of a song. These are among the most important and readily commercially applicable content attributes that can be automatically analyzed from music signals. For audio classification, several features and classification methods are proposed and evaluated. In musical instrument recognition, we consider methods to improve the performance of a baseline audio classification system that uses mel-frequency cepstral coefficients and their first derivatives as features, and continuous-density hidden Markov models (HMMs) for modeling the feature distributions. Two improvements are proposed to increase the performance of this baseline system. First, transforming the features to a base with maximal statistical independence using independent component analysis. Secondly, discriminative training is shown to further improve the recognition accuracy of the system. For musical meter analysis, three methods are proposed. The first performs meter analysis jointly at three different time scales: at the temporally atomic tatum pulse level, at the tactus pulse level, which corresponds to the tempo of a piece, and at the musical measure level. The features obtained from an accent feature analyzer and a bank of combfilter resonators are processed by a novel probabilistic model which represents primitive musical knowledge and performs joint estimation of the tatum, tactus, and measure pulses. The second method focuses on estimating the beat and the tatum. The design goal was to keep the method computationally very efficient while retaining sufficient analysis accuracy. Simplified probabilistic modeling is proposed for beat and tatum period and phase estimation, and ensuring the continuity of the estimates. A novel phase-estimator based on adaptive comb filtering is presented. The accuracy of the method is close to the first method but with a fraction of the computational cost. The third method for music rhythm analysis focuses on improving the accuracy in music tempo estimation. The method is based on estimating the tempo of periodicity vectors using locally weighted k-Nearest Neighbors (k-NN) regression. Regression closely relates to classification, the difference being that the goal of regression is to estimate the value of a continuous variable (the tempo), whereas in classification the value to be assigned is a discrete category label. We propose a resampling step applied to an unknown periodicity vector before finding the nearest neighbors to increase the likelihood of finding a good match from the training set. This step improves the performance of the method significantly. The tempo estimate is computed as a distance-weighted median of the nearest neighbor tempi. Experimental results show that the proposed method provides significantly better tempo estimation accuracies than three reference methods. Finally, we describe a computationally efficient method for detecting a chorus section in popular and rock music. The method utilizes a self-dissimilarity representation that is obtained by summing two separate distance matrices calculated using the mel-frequency cepstral coefficient and pitch chroma features. This is followed by the detection of off-diagonal segments of small distance in the distance matrix. From the detected segments, an initial chorus section is selected using a scoring mechanism utilizing several heuristics, and subjected to further processing.",
    title = "{S}ignal {P}rocessing {M}ethods for {A}udio {C}lassification and {M}usic {C}ontent {A}nalysis",
    url = "https://tutcris.tut.fi/portal/files/2308383/eronen.pdf",
    year = "2009",
    school = "Tampere University of Technology"
}

@mastersthesis{Seppänen2001_master,
    author = {Sepp{\"a}nen, Jarno},
    title = "Computational Models of Musical Meter Recognition",
    year = "2001",
    school = "Tampere University of Technology"
}

@inproceedings{Bilcu2002_CSCC,
    author = {Bilcu, Enik{\"o} Beatrice and Suontausta, Janne and Saarinen, Jukka},
    booktitle = "Proceedings of the 6th WSEAS International Multiconference on Circuits, Systems, Communications and Computers, CSCC 2002, July 7-14, 2002, Grete, Greece",
    pages = "4591--4596",
    title = "A New Transform Domain Neural Network for Text-To-Phoneme Mapping",
    year = "2002"
}

@inproceedings{Green2019_WASPAA,
    author = "Green, {Marc C.} and Adavanne, Sharath and Murphy, Damian and Virtanen, Tuomas",
    abstract = "This paper investigates the potential of using higher-order Ambisonic features to perform acoustic scene classification. We compare the performance of systems trained using first-order and fourth-order spatial features extracted from the EigenScape database. Using both Gaussian mixture model and convolutional neural network classifiers, we show that features extracted from higher-order Ambisonics can yield increased classification accuracies relative to first-order features. Diffuseness-based features seem to describe scenes particularly well relative to direction-of-arrival based features. With specific feature subsets, however, differences in classification accuracy between first and fourth-order features become negligible.",
    booktitle = "2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)",
    doi = "10.1109/WASPAA.2019.8937282",
    isbn = "978-1-7281-1124-7",
    keywords = "acoustic scene classification; ambisonics; spatial audio; convolutional neural networks; gaussian mixture models",
    month = "10",
    pages = "328--332",
    publisher = "IEEE",
    series = "IEEE Workshop on Applications of Signal Processing to Audio and Acoustics",
    title = "Acoustic Scene Classification Using Higher-Order Ambisonic Features",
    year = "2019"
}

@inproceedings{Peltonen2001_AES,
    author = "Peltonen, Vesa and Eronen, Antti and Parviainen, Mikko and Klapuri, Anssi",
    address = "Amsterdam, Netherlands",
    booktitle = "110th Audio Engineering Society Convention",
    keywords = "context recognition",
    title = "Recognition of everyday auditory scenes: potentials, latencies and cues",
    year = "2001"
}

@mastersthesis{Peltonen2001_master,
    author = "Peltonen, Vesa",
    abstract = "An acoustic environment surrounding a listener, an auditory scene, can provide contextual cues that enable the recognition of the scene. This thesis concerns the problem of computational auditory scene recognition, which is a subproblem of computational auditory scene analysis. Computational auditory scene analysis refers to the computational analysis of an acoustic environment, and the recognition of distinct sound events in it. In this study, the focus is not in analyzing and recognizing discrete sound events (although they may be used in the recognition process), but in the classiﬁcation of acoustic environments as whole. This thesis covers all the different phases of a study that was made at the Signal Processing Laboratory of Tampere University of Technology: a literature review on auditory scene recognition and related ﬁelds of research, acoustic measurements that were made in a number of everyday auditory environments, design and implementation of the audio database access software, a listening test examining human abilities in auditory scene recognition, audio signal classiﬁcation theory, algorithm development and simulations. The core of this thesis is in the computational audio classiﬁcation and signal processing algorithm development part. Auditory scene recognition involves correct grouping of similar environments, feature selection and extraction, and the use of a suitable classiﬁcation algorithm. A crucial step in solving the problem is to determine appropriate features that can discriminatebetween the acoustic data associated with pre-deﬁned scene classes. The conducted listening tests show that, on average, humans are able to recognize 25 different scenes with 70 \% accuracy. The scenes included everyday outside and inside environments,such as streets, market places, restaurants, and family homes. The performance of the computational classiﬁcation methods was investigated by conducting Matlab simulations. The bestobtained recognition rate for 13 different scenes was 56\%, where the classiﬁed scenes wereselected so that from each one there were at least three recordings from different locations. We also did an experiment of recognizing more general classes (meta-classes), and for certain categorizations of the scenes we obtained relatively good classiﬁcation results. For example, themeta-class car vs. other was classiﬁed correctly in 95\% of the cases.",
    title = "{C}omputational {A}uditory {S}cene {R}ecognition",
    year = "2001",
    school = "Tampere University of Technology"
}

@article{Baby2015_TASLP,
    author = "Baby, Deepak and Virtanen, Tuomas and Gemmeke, Jort and Hamme, Hugo Van",
    abstract = "Exemplar-based speech enhancement systems work by decomposing the noisy speech as a weighted sum of speech and noise exemplars stored in a dictionary and use the resulting speech and noise estimates to obtain a time-varying filter in the full-resolution frequency domain to enhance the noisy speech. To obtain the decomposition, exemplars sampled in lower dimensional spaces are preferred over the full-resolution frequency domain for their reduced computational complexity and the ability to better generalize to unseen cases. But the resulting filter may be sub-optimal as the mapping of the obtained speech and noise estimates to the full-resolution frequency domain yields a low-rank approximation. This paper proposes an efficient way to directly compute the full-resolution frequency estimates of speech and noise using coupled dictionaries: an input dictionary containing atoms from the desired exemplar space to obtain the decomposition and a coupled output dictionary containing exemplars from the full-resolution frequency domain. We also introduce modulation spectrogram features for the exemplar-based tasks using this approach. The proposed system was evaluated for various choices of input exemplars and yielded improved speech enhancement performances on the AURORA-2 and AURORA-4 databases. We further show that the proposed approach also results in improved word error rates (WERs) for the speech recognition tasks using HMM-GMM and deep-neural network (DNN) based systems.",
    doi = "10.1109/TASLP.2015.2450491",
    issn = "2329-9290",
    journal = "IEEE-ACM Transactions on Audio Speech and Language Processing",
    keywords = "Exemplar-based;Modulation envelope;Noise robust automatic speech recognition;Non-negative sparse coding",
    month = "11",
    number = "11",
    pages = "1788--1799",
    publisher = "IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC",
    title = "Coupled dictionaries for exemplar-based speech enhancement and automatic speech recognition",
    volume = "23",
    year = "2015",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/dbaby\_aslp2015.pdf"
}

@mastersthesis{Rosti1999_master,
    author = "Rosti, Antti-Veikko",
    abstract = "The interest in modulation classification has recently emerged in the research of communication systems. This has been due to the advances in reconfigurable signal processing systems, especially in the study of software radio. Published methods can be divided into two groups: maximum likelihood and pattern recognition approaches. In the maximum likelihood approach, decision rules are often simple but the test statistics are complicated and assume prior knowledge about the signals. In the pattern recognition approach, decision rules are often complicated whereas the features are simple and fast to calculate. Communication signals contain a vast amount of uncertainty due to the unknown modulating signal, modulation type, and noise. Therefore the modulation classification problem has to be approached by using statistical methods. The features and the test statistics may be derived from the known statistical characteristics of the modulated signals. Either implicit or explicit use of higher-order statistics has been studied previously in many communication applications. The higher-order statistics are often more preferable because second-order statistics suppress the phase information of the signal. Nevertheless, the estimation of the higher-order statistics requires long sample sets and has a high computational complexity. To overcome these problems, second-order cyclostationary statistics have been studied and the results seem promising. In this thesis, the feature extraction problem of the modulation classification is discussed. Useful characteristics and representations of the communication signals are presented as well as the relevant knowledge of statistical signal processing. The previous methods are presented in a literature review of the modulation classification. The first and second-order statistics including the cyclostationary statistics of digital modulated signals are studied, and a novel feature is proposed. Some previous methods and this novel feature are compared by investigating their discrimination performance in Matlab simulations.",
    title = "{S}tatistical {M}ethods in {M}odulation {C}lassification",
    year = "1999",
    school = "Tampere University of Technology"
}

@ARTICLE{Eronen2006_TASLP,
    author = {Eronen, Antti and Peltonen, Vesa and Tuomi, Juha and Klapuri, Anssi and Fagerlund, Seppo and Sorsa, Timo and Lorho, Ga{\"e}tan and Huopaniemi, Jyri},
    journal = "IEEE Transactions on Audio, Speech, and Language Processing",
    title = "Audio-based context recognition",
    year = "2006",
    volume = "14",
    number = "1",
    pages = "321-329",
    keywords = "Humans;System testing;Hidden Markov models;Acoustic devices;Context awareness;Mobile handsets;Acoustic signal processing;Computational complexity;Vectors;Feature extraction;Audio classification;context awareness;feature extraction;hidden Markov models (HMMs)",
    doi = "10.1109/TSA.2005.854103"
}

@mastersthesis{Saarelainen1999_master,
    author = "Saarelainen, Teemu",
    title = "{B}lind {MIMO} {D}econvolution in {A}coustic {A}pplications",
    year = "1999",
    school = "Tampere University of Technology"
}

@INPROCEEDINGS{Paulus2006_ICASSP,
    author = "Paulus, J.",
    booktitle = "2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings",
    title = "Acoustic Modelling of Drum Sounds with Hidden Markov Models for Music Transcription",
    year = "2006",
    volume = "5",
    number = "",
    pages = "V-V",
    keywords = "Hidden Markov models;Multiple signal classification;Music;Instruments;Pattern recognition;Source separation;Signal analysis;Acoustic signal processing;Acoustic signal detection;Taxonomy",
    doi = "10.1109/ICASSP.2006.1661257"
}

@mastersthesis{Pasanen2002_master,
    author = "Pasanen, Antti",
    abstract = "In this thesis, Voice Activity Detection (VAD) algorithms are integrated in an ASR system. VAD is assumed to give additional information to the ASR system about the presence of speech, thus increasing the robustness of the ASR system. Two standard VAD algorithms (G.729b and GSM) are described and a statistical Gaussian Mixture Model (GMM) based VAD is introduced. For the GMM based VAD, different adaptation techniques are employed to track the changing background noise statistics. The VAD algorithms are integrated with the ASR system, with explicit and implicit approaches. The explicit approach means that the VAD is a separate module in the front end of the ASR system, while in the implicit approach the VAD decision is included in the decoding stage of the speech recognition unit. The performance of the VAD algorithms are compared directly using frame classification rates and indirectly using recognition rates. Recognition is performed as a small vocabulary isolated word recognition task with a Hidden Model based ASR system using normalized Mel-frequency cepstral coefficients. According to our simulations, ideal information about the word boundaries increases significantly the recognition accuracy of the ASR system. However, the described VAD algorithms were not able to increase the recognition accuracy significantly.",
    title = "{V}oice {A}ctivity {D}etection in {N}oise {R}obust {S}peech {R}ecognition",
    year = "2002",
    school = "Tampere University of Technology"
}

@mastersthesis{Pirinen2002_master,
    author = "Pirinen, Tuomo",
    title = "{R}eliability evaluation of time delay based direction of arrival estimate",
    year = "2002",
    school = "Tampere University of Technology"
}

@inproceedings{Mesaros2013_ICASSP,
    author = {Mesaros, Annamaria and Heittola, Toni and Palom{\"a}ki, Kalle},
    abstract = "A common problem of freely annotated or user contributed audio databases is the high variability of the labels, related to homonyms, synonyms, plurals, etc. Automatically re-labeling audio data based on audio similarity could offer a solution to this problem. This paper studies the relationship between audio and labels in a sound event database, by evaluating semantic similarity of labels of acoustically similar sound event instances. The assumption behind the study is that acoustically similar events are annotated with semantically similar labels. Indeed, for 43\% of the tested data, there was at least one in ten acoustically nearest neighbors having a synonym as label, while the closest related term is on average one level higher or lower in the semantic hierarchy.",
    booktitle = "Proceedings of the 2013 IEEE International Conference on Acoustics, Speech and Signal Processing",
    doi = "http://dx.doi.org/10.1109/ICASSP.2013.6637761",
    isbn = "978-1-4799-0356-6",
    keywords = "audio similarity;semantic similarity;sound events",
    pages = "813-817",
    publisher = "IEEE Computer Society",
    title = "Analysis of acoustic-semantic relationship for diversely annotated real-world audio data",
    year = "2013"
}

@article{Popa2011_JSIP,
    author = "Popa, Victor and Nurminen, Jani and Gabbouj, Moncef",
    doi = "10.4236/jsip.2011.22017",
    issn = "2159-4465",
    journal = "Journal of Signal and Information Processing",
    number = "2",
    pages = "125--139",
    publisher = "Scientific Research Publishing",
    title = "{A} {S}tudy of {B}ilinear {M}odels in {V}oice {C}onversion",
    volume = "2",
    year = "2011"
}

@inproceedings{Niemistö2003_IWAENC,
    author = {Niemist{\"o}, Riitta and M{\"a}kel{\"a}, Tuomo},
    editor = "Makino, S. and Miyoshi, M.",
    booktitle = "Proceedings of the Eight International Workshop on Acoustic Echo and Noise Control, IWAENC 2003, Kyoto, Japan, 8-11 September 2003",
    pages = "79--82",
    title = "{O}n performance of linear adaptive filtering algorithms in acoustic echo control in presence of distorting loudspeakers",
    year = "2003"
}

@inproceedings{Herrera-Boyer2006_SPMMT,
    author = "Herrera-Boyer, P. and Klapuri, Anssi and Davy, Manuel",
    editor = "Klapuri, A. and Davy, M.",
    booktitle = "Signal Processing Methods for Music Transcription",
    doi = "10.1007/0-387-32845-9\_6",
    isbn = "978-0-387-30667-4",
    pages = "163--200",
    publisher = "Springer",
    title = "{A}utomatic classification of pitched musical instrument sounds",
    year = "2006"
}

@mastersthesis{Kuja-Halkola2002_master,
    author = "Kuja-Halkola, Sami",
    abstract = "This thesis concerns the problem of automatic recognition of a person based on his or her voice. The main objective is on the text-independent speaker identification task. A few different feature extraction algorithms are presented and evaluated using the KING and POLYCOST speech corpora. The principal feature used are the mel-frequency cepstral coefficients, but also other feature sets are considered including the spectral slope and the fundamental frequency.The work concentrates mostly on different classification algorithms used in speaker recognition. The biggest attention is on the Gaussian mixture speaker models (GMM) which have been widely used in many text-independent speaker recognition studies. Conventionally, the GMM parameters are trained with the well-known expectation maximization (EM) algorithm. An identification accuracy of 95.8\% is achieved using this method for a population of 25 speakers with 60 seconds of training speech and a test sequence of 5 seconds.A drawback of the conventional EM training is the need for selecting the number of the mixture components, i.e. the model order, before the actual training procedure. Moreover, the number of components is usually the same for each speaker. The most important part of this thesis concerns simultaneous order selection and parameter training of GMMs. We evaluate a couple of recently proposed algorithms capable of selecting the order of each speaker model individually during a single training procedure. The algorithms provide a straightforward way of adjusting the number of components of each GMM to the acoustic characteristics and amount of available training data for each speaker. The methods are based on integrating some model complexity criterion, such as the minimum descripition length, into the EM training process. It is observed that when the amount of available training data varies between speakers, a relative reduction of 13\% in error rate is obtained using an algorithm proposed by Figueiredo and Jain [Figueiredo02]. If the speech samples are recorded over a telephone line, a reduction of 11\% in error rate is observed using the agglomerative EM algorithm [Figueiredo99].",
    title = "{T}ext-independent speaker identification",
    year = "2002",
    school = "Tampere University of Technology"
}

@mastersthesis{Viitaniemi2003_master,
    author = "Viitaniemi, Timo",
    abstract = "The thesis proposed a method for the automatic transcription of single-voice melodies from an acoustic waveform into a symbolic musical notation. The system consisted of a signal processing front-end which calculated a continuous pitch track and of a probabilistic model which converted the pitch track into a discrete musical notation. The proposed probabilistic model consisted of three parts operating in parallel: a pitch trajectory model, a musicological model, and a duration model. The first handled imperfections in the performed/estimated pitch values using a hidden Markov model, the second estimated the musical key signature to improve the transcription accuracy ant the last models the duration of the notes. The thesis covered a literature review on human voice production and on the theory of pitch estimation. In addition, an inspection of an acoustic database recorded for the training and the testing of the proposed model was introduced. The most important part of the thesis is the chapter with the three probabilistic models.",
    title = "{P}robabilistic {M}odels for the {T}ranscription of {S}ingle-{V}oice {M}elodies",
    year = "2003",
    school = "Tampere University of Technology"
}

@inproceedings{Pirinen2004_SAM,
    author = "Pirinen, Tuomo and Yli-Hietanen, Jari",
    booktitle = "Proceedings of 2004 IEEE Sensor Array and Multichannel Signal Processing Workshop, SAM 2004, Barcelona, Spain, 18-21 July 2004",
    pages = "5 p",
    title = "{T}ime delay based failure-robust direction of arrival estimation",
    year = "2004"
}

@inproceedings{Cakir2018_AES,
    author = "Cakir, Emre and Virtanen, Tuomas",
    abstract = "In this work we propose a deep learning based method—namely, variational, convolutional recurrent autoencoders (VCRAE)—for musical instrument synthesis. This method utilizes the higher level time-frequency representations extracted by the convolutional and recurrent layers to learn a Gaussian distribution in the training stage, which will be later used to infer unique samples through interpolation of multiple instruments in the usage stage. The reconstruction performance of VCRAE is evaluated by proxy through an instrument classifier and provides significantly better accuracy than two other baseline autoencoder methods. The synthesized samples for the combinations of 15 different instruments are available on the companion website.",
    booktitle = "Proceedings of the Audio Engineering Society 145th Convention",
    publisher = "AES Audio Engineering Society",
    title = "Musical Instrument Synthesis and Morphing in Multidimensional Latent Space Using Variational, Convolutional Recurrent Autoencoders",
    year = "2018",
    url = "https://trepo.tuni.fi/bitstream/handle/10024/129505/AES\_2018\_Musical\_Instrument\_Synthesis\_and\_Morphing\_in\_Multidimensional\_Latent\_Space\_Using\_Variational\_Convolutional\_Recurrent\_Autoencoders.pdf?sequence=1\&isAllowed=y"
}

@mastersthesis{Heittola2004_master,
    author = "Heittola, Toni",
    abstract = "Collections of digital music have become increasingly common over the recent years. As the amount of data increases, digital content management is becoming more important. In this thesis, we are studying content-based classification of acoustic musical signals according to their musical genre (e.g., classical, rock) and the instruments used. A listening experiment is conducted to study human abilities to recognise musical genres. This thesis covers a literature review on human musical genre recognition, state-of-the-art musical genre recognition systems, and related fields of research. In addition, a general-purpose music database consisting of recordings and their manual annotations is introduced. The theory behind the used features and classifiers is reviewed and the results from the simulations are presented. The developed musical genre recognition system uses mel-frequency cepstral coefficients to represent the time-varying magnitude spectrum of a music signal. The class-conditional feature densities are modelled with hidden Markov models. Musical instrument detection for a few pitched instruments from music signals is also studied using the same structure. Furthermore, this thesis proposes a method for the detection of drum instruments. The presence of drums is determined based on the periodicity of the amplitude envelopes of the signal at subbands. The conducted listening experiment shows that the recognition of musical genres is not a trivial task even for humans. On the average, humans are able to recognise the correct genre in 75 \% of cases (given five-second samples). Results also indicate that humans can do rather accurate musical genre recognition without long-term temporal features, such as rhythm. For the developed automatic recognition system, the obtained recognition accuracy for six musical genres was around 60 \%, which is comparable to the state-of-the-art systems. Detection accuracy of 81 \% was obtained with the proposed drum instrument detection method.",
    keywords = "Musical genre classification",
    title = "{A}utomatic {C}lassification of {M}usic {S}ignals",
    year = "2004",
    school = "Tampere University of Technology"
}

@inproceedings{Heittola2009_ISMIR 2009,
    author = "Heittola, Toni and Klapuri, Anssi and Virtanen, Tuomas",
    abstract = "This paper proposes a novel approach to musical instrument recognition in polyphonic audio signals by using a source-filter model and an augmented non-negative matrix factorization algorithm for sound separation. The mixture signal is decomposed into a sum of spectral bases modeled as a product of excitations and filters. The excitations are restricted to harmonic spectra and their fundamental frequencies are estimated in advance using a multipitch estimator, whereas the filters are restricted to have smooth frequency responses by modeling them as a sum of elementary functions on the Mel-frequency scale. The pitch and timbre information are used in organizing individual notes into sound sources. In the recognition, Mel-frequency cepstral coefficients are used to represent the coarse shape of the power spectrum of sound sources and Gaussian mixture models are used to model instrument-conditional densities of the extracted features. The method is evaluated with polyphonic signals, randomly generated from 19 instrument classes. The recognition rate for signals having six note polyphony reaches 59\%.",
    address = "Kobe, Japan",
    booktitle = "in Proc. 10th Int. Society for Music Information Retrieval Conf. (ISMIR 2009)",
    keywords = "instruments;separation;source-filter model",
    organization = "International Society for Music Information Retrieval (ISMIR)",
    pages = "327-332",
    title = "Musical Instrument Recognition in Polyphonic Audio Using Source-Filter Model for Sound Separation",
    year = "2009",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/ismir09-heittola.pdf"
}

@article{Heittola2013_JASM,
    author = "Heittola, Toni and Mesaros, Annamaria and Eronen, Antti and Virtanen, Tuomas",
    abstract = "The work presented in this article studies how the context information can be used in the automatic sound event detection process, and how the detection system can benefit from such information. Humans are using context information to make more accurate predictions about the sound events and ruling out unlikely events given the context. We propose a similar utilization of context information in the automatic sound event detection process. The proposed approach is composed of two stages: automatic context recognition stage and sound event detection stage. Contexts are modeled using Gaussian mixture models and sound events are modeled using three-state left-to-right hidden Markov models. In the first stage, audio context of the tested signal is recognized. Based on the recognized context, a context-specific set of sound event classes is selected for the sound event detection stage. The event detection stage also uses context-dependent acoustic models and count-based event priors. Two alternative event detection approaches are studied. In the first one, a monophonic event sequence is outputted by detecting the most prominent sound event at each time instance using Viterbi decoding. The second approach introduces a new method for producing polyphonic event sequence by detecting multiple overlapping sound events using multiple restricted Viterbi passes. A new metric is introduced to evaluate the sound event detection performance with various level of polyphony. This combines the detection accuracy and coarse time-resolution error into one metric, making the comparison of the performance of detection algorithms simpler. The two-step approach was found to improve the results substantially compared to the context-independent baseline system. In the block-level, the detection accuracy can be almost doubled by using the proposed context-dependent event detection.",
    journal = "EURASIP Journal on Audio, Speech and Music Processing",
    keywords = "CASA;Sound event detection",
    title = "Context-Dependent Sound Event Detection",
    doi = "10.1186/1687-4722-2013-1",
    year = "2013"
}

@inproceedings{Heittola2013_ICASSP,
    author = "Heittola, Toni and Mesaros, Annamaria and Virtanen, Tuomas and Gabbouj, Moncef",
    abstract = "Sound event detection is addressed in the presence of overlapping sounds. Unsupervised sound source separation into streams is used as a preprocessing step to minimize the interference of overlapping events. This poses a problem in supervised model training, since there is no knowledge about which separated stream contains the targeted sound source. We propose two iterative approaches based on EM algorithm to select the most likely stream to contain the target sound: one by selecting always the most likely stream and another one by gradually eliminating the most unlikely streams from the training. The approaches were evaluated with a database containing recordings from various contexts, against the baseline system trained without applying stream selection. Both proposed approaches were found to give a reasonable increase of 8 percentage units in the detection accuracy.",
    address = "Vancouver, Canada",
    booktitle = "2013 IEEE International Conference on Acoustics, Speech and Signal Processing",
    pages = "8677-8681",
    keywords = "sound event detection;casa",
    publisher = "IEEE Computer Society",
    title = "Supervised Model Training for Overlapping Sound Events Based on Unsupervised Source Separation",
    year = "2013",
    doi = "10.1109/ICASSP.2013.6639360",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/icassp2013\_heittola.pdf"
}

@article{Heittola2014_JASM,
    author = "Heittola, Toni and Mesaros, Annamaria and Korpi, Dani and Eronen, Antti and Virtanen, Tuomas",
    abstract = "An approach is proposed for creating location-specific audio textures for virtual location-exploration services. The presented approach creates audio textures by processing a small amount of audio recorded at a given location, providing a cost-effective way to produce a versatile audio signal that characterizes the location. The resulting texture is non-repetitive and conserves the location-specific characteristics of the audio scene, without the need of collecting large amount of audio from each location. The method consists of two stages: analysis and synthesis. In the analysis stage, the source audio recording is segmented into homogeneous segments. In the synthesis stage, the audio texture is created by randomly drawing segments from the source audio so that the consecutive segments will have timbral similarity near the segment boundaries. Results obtained in listening experiments show that there is no statistically significant difference in the audio quality or location-specificity of audio when the created audio textures are compared to excerpts of the original recordings. Therefore, the proposed audio textures could be utilized in virtual location-exploration services. Examples of source signals and audio textures created from them are available at www.cs.tut.fi/\textasciitilde heittolt/audiotexture.",
    journal = "EURASIP Journal on Audio, Speech and Music Processing",
    number = "9",
    title = "Method for creating location-specific audio textures",
    volume = "2014",
    year = "2014",
    doi = "10.1186/1687-4722-2014-9"
}

@mastersthesis{Tervo2006_master,
    author = "Tervo, Sakari",
    title = "{A}ikaeron estimointimenetelmien suorituskyky reaalitilanteessa",
    year = "2006",
    school = "Tampere University of Technology"
}

@inproceedings{Diment2017_WASPAA,
    author = "Diment, Aleksandr and Virtanen, Tuomas",
    booktitle = "IEEE Workshop on Applications of Signal Processing to Audio and Acoustics",
    doi = "10.1109/WASPAA.2017.8169984",
    isbn = "978-1-5386-1631-4",
    pages = "6--10",
    series = "IEEE Workshop on Applications of Signal Processing to Audio and Acoustics",
    title = "Transfer Learning of Weakly Labelled Audio",
    year = "2017",
    url = "http://diment.kapsi.fi/papers/Diment17\_TL.pdf"
}

@phdthesis{Niemistö2003_phd,
    author = {Niemist{\"o}, Riitta},
    abstract = "This thesis proposes algorithms to be utilized for iterative design of linear filters and for adaptive polynomial filtering. The goal is to improve existing algorithms in terms of filtering performance, time and complexity requirements. First, a family of fast algorithms suitable for adaptive polynomial filtering, namely LS-LMS algorithm and its square root variants, is introduced and its connection with the family of RLS algorithms is analyzed. The algorithms are derived by combining the triangularization and backsolving steps of QR-RLS algorithm. In the derivation we allow a modification of the cost function in order to decrease the complexity of the algorithm from O(M^2) to O(M), where M is the number of filter parameters. Second, adaptive filtering algorithms are proposed for the problem of acoustic echo control in the case of strong acoustic distortion in the loudspeaker that decreases the echo attenuation obtained using adaptive linear filtering algorithms in echo cancelers. Several existing algorithms were tested using both linear and homogeneous Volterra filters, but their performance was not found satisfactory either in terms of performance or complexity. A polynomial preprocessor followed by a linear filter was found to perform well when the preprocessor was adapted by the robust, but computationally demanding, QR-RLS algorithm and the linear part was adapted by the normalized LMS algorithm. Third, the problem of optimum energy compaction for FIR and IIR filters is addressed. The analytical method for optimum compaction FIR filter design is analyzed and shown to provide a very fast algorithm under certain conditions. For the design of optimum compaction IIR filters two new methods are proposed. The first design method operates with the numerator and the denominator in the causal part of the associated product filter, which are in turn optimized via iterative relaxations. The second method is similar, except that the angles of the poles are set to fixed values, situated at the transitions in the frequency representation of the corresponding ideal brickwall filter. Thus, the second method is faster than the first, and, moreover, it was observed that when the ideal brickwall filter does not have too many transitions, the second method provides filters with better performance. Both methods make use of semidefinite programming optimization using an appropriate parameterization of positive real polynomials. The last part of the thesis provides contributions to the frequency selective IIR filter design. The frequency response of the filter is fitted to an ideal frequency response by optimizing either a weighted sum of magnitude squares (least squares) criterion or a minimax (Chebyshev) criterion. For the least squares criterion a new convex stability domain is used and experimental evidence is given that the best designs are usually obtained with a multistage algorithm where three methods, Steiglitz-McBride, Gauss-Newton and classical descent, are used consecutively in that order. The result of multistage least squares IIR design is used to initialize the optimization procedure based on a Chebyshev criterion, for which a simplified iterative procedure is proposed. In the simplified procedure only the numerator is updated after initialization. The simplified procedure is compared experimentally with the complete procedure and the results show only very small differences in the criterion.",
    title = "{F}ast {A}lgorithms for {I}terative {D}esign of {L}inear {F}ilters and {A}daptive {P}olynomial {F}iltering",
    year = "2003",
    school = "Tampere University of Technology"
}

@mastersthesis{Tuomi2004_master,
    author = "Tuomi, Juha",
    title = "{A}udio-{B}ased {C}ontext {T}racking",
    year = "2004",
    school = "Tampere University of Technology"
}

@inproceedings{Mahkonen2013_DAFx,
    author = {Mahkonen, Katariina and Eronen, Antti and Virtanen, Tuomas and Helander, Elina and Popa, Victor and Lepp{\"a}nen, Jussi and Curcio, Igor},
    booktitle = "16th International Conference on Digital Audio Effects, Ireland, 2-5.9,2013",
    publisher = "International Conference on Digital Audio Effects",
    series = "International Conference on Digital Audio Effects",
    title = "Music Dereverberation by Spectral Linear Prediction in Live Recordings",
    year = "2013",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/Mahkonen\_DAFX2013.pdf"
}

@inbook{Nikunen2017,
    author = "Nikunen, Joonas and Virtanen, Tuomas",
    abstract = "This chapter introduces methods for factorizing the spectrogram of multichannel audio into repetitive spectral objects and apply the introduced models to the analysis of spatial audio and modification of spatial sound through source separation. The purpose of decomposing an audio spectrogram using spectral templates is to learn the underlying structures (audio objects) from the observed data. The chapter discusses two main scenarios such as parameterization of multichannel surround sound and parameterization of microphone array signals. It explains the principles of source separation by time-frequency filtering using separation masks constructed from the spectrogram models. The chapter introduces a spatial covariance matrix model based on the directions of arrival of sound events and spectral templates, and discusses its relationship to conventional spatial audio signal processing. Source separation using spectrogram factorization models is achieved via time- frequency filtering of the original observation short-time Fourier transform (STFT) by a generalized Wiener filter obtained from the spectrogram model parameters.",
    booktitle = "Parametric time-frequency-domain spatial audio",
    doi = "10.1002/9781119252634.ch9",
    editor2 = "Ville Pulkki and Symeon Delikaris-Manias and Archontis Politis",
    isbn = "978-1-119-25259-7",
    month = "10",
    pages = "215--250",
    publisher = "John Wiley {\\&} Sons",
    title = "Source Separation and Reconstruction of Spatial Audio Using Spectrogram Factorization",
    year = "2017"
}

@inproceedings{Mesaros2017_WASPAA,
    author = "Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas",
    abstract = "Human and machine performance in acoustic scene classification is examined through a parallel experiment using TUT Acoustic Scenes 2016 dataset. The machine learning perspective is presented based on the systems submitted for the 2016 challenge on Detection and Classification of Acoustic Scenes and Events. The human performance, assessed through a listening experiment, was found to be significantly lower than machine performance. Test subjects exhibited different behavior throughout the experiment, leading to significant differences in performance between groups of subjects. An expert listener trained for the task obtained similar accuracy to the average of submitted systems, comparable also to previous studies of human abilities in recognizing everyday acoustic scenes.",
    address = "United States",
    booktitle = "2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)",
    doi = "10.1109/WASPAA.2017.8170047",
    isbn = "978-1-5386-1631-4",
    keywords = "acoustic scene classification; machine learning; human performance; listening experiment",
    pages = "319–323",
    publisher = "IEEE Computer Society",
    title = "Assessment of human and machine performance in acoustic scene classification: {DCASE} 2016 case study",
    year = "2017",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/mesaros-waspaa2017-humans-vs-machines-asc.pdf"
}

@mastersthesis{Tsoumanis2002_master,
    author = "Tsoumanis, Andreas",
    title = "{F}ace {F}eature {T}racking {U}sing {G}abor {W}avelets",
    year = "2002",
    school = "Tampere University of Technology"
}

@inproceedings{Lipping2019_DCASE2019,
    author = "Lipping, Samuel and Drossos, Konstantinos and Virtanen, Tuomas",
    abstract = {Audio captioning is a novel field of multi-modal translation and it is the task of creating a textual description of the content of an audio signal (e.g. {"}people talking in a big room{"}). The creation of a dataset for this task requires a considerable amount of work, rendering the crowdsourcing a very attractive option. In this paper we present a three steps based framework for crowdsourcing an audio captioning dataset, based on concepts and practises followed for the creation of widely used image captioning and machine translations datasets. During the first step initial captions are gathered. A grammatically corrected and/or rephrased version of each initial caption is obtained in second step. Finally, the initial and edited captions are rated, keeping the top ones for the produced dataset. We objectively evaluate the impact of our framework during the process of creating an audio captioning dataset, in terms of diversity and amount of typographical errors in the obtained captions. The obtained results show that the resulting dataset has less typographical errors than the initial captions, and on average each sound in the produced dataset has captions with a Jaccard similarity of 0.24, roughly equivalent to two ten-word captions having in common four words with the same root, indicating that the captions are dissimilar while they still contain some of the same information.},
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019)",
    day = "26",
    keywords = "audio captioning; captioning; amt; crowdsourcing; Amazon Mechanical Turk",
    month = "10",
    title = "Crowdsourcing a Dataset of Audio Captions",
    year = "2019",
    url = "https://arxiv.org/abs/1907.09238"
}

@inproceedings{Hurmalainen2011,
    author = "Hurmalainen, Antti and Virtanen, Tuomas and Gemmeke, Jort and Mahkonen, Katariina",
    booktitle = {Akustiikkap{\"a}iv{\"a}t 2011, Tampere, 11.-12.5.2011, Akustinen Seura ry},
    pages = "1--5",
    publisher = "Akustinen seura",
    series = {Akustiikkap{\"a}iv{\"a}t},
    title = "{E}simerkkipohjainen meluisan puheen automaattinen tunnistus",
    year = "2011"
}

@article{Cakir2017_TASLP,
    author = "Cakir, Emre and Parascandolo, Giambattista and Heittola, Toni and Huttunen, Heikki and Virtanen, Tuomas",
    abstract = "Sound events often occur in unstructured environments where they exhibit wide variations in their frequency content and temporal structure. Convolutional neural networks (CNNs) are able to extract higher level features that are invariant to local spectral and temporal variations. Recurrent neural networks (RNNs) are powerful in learning the longer term temporal context in the audio signals. CNNs and RNNs as classifiers have recently shown improved performances over established methods in various sound recognition tasks. We combine these two approaches in a convolutional recurrent neural network (CRNN) and apply it on a polyphonic sound event detection task. We compare the performance of the proposed CRNN method with CNN, RNN, and other established methods, and observe a considerable improvement for four different datasets consisting of everyday sound events.",
    doi = "10.1109/TASLP.2017.2690575",
    issn = "2329-9290",
    journal = "IEEE-ACM Transactions on Audio Speech and Language Processing",
    keywords = "deep neural networks;sound event detection",
    month = "6",
    number = "6",
    pages = "1291--1303",
    publisher = "IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC",
    title = "Convolutional Recurrent Neural Networks for Polyphonic Sound Event Detection",
    volume = "25",
    year = "2017",
    url = "https://arxiv.org/abs/1702.06286"
}

@inproceedings{Gemmeke2013_in conjuction with ICASSP,
    author = "Gemmeke, Jort and Hurmalainen, Antti and Virtanen, Tuomas",
    booktitle = "The 2nd International Workshop on Machine Listening in Multisource Environments CHiME Workshop, 1st June 2013, Vancouver, Canada (in conjuction with ICASSP)",
    pages = "47--52",
    keywords = "speech enhancement;exemplar-based;noise robustness;Non-Negative Matrix Factorization;Hidden Markov Models",
    publisher = "International Workshop on Machine Listening in Multisource Environments",
    series = "International Workshop on Machine Listening in Multisource Environments",
    title = "{HMM}-regularization for {NMF}-based noise robust {ASR}",
    year = "2013",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/pP5\_gemmeke.pdf"
}

@mastersthesis{Mäkelä2006_master,
    author = {M{\"a}kel{\"a}, Tuomo},
    abstract = "Acoustic echo is an annoying, nevertheless common, phenomenon in the modern communication systems such as cellular telephones or video conference systems.This thesis introduces methods for polynomial-based acoustic echo cancellation which are used in telecommunication systems, how acoustic echo is formed and how to model and reduce it with the means of signal proeessing. The author describes the principles of adaptive filtering, presents the best known adaptive filtering algorithms and introduces some more sophisticated algorithms based on these. The aim is to develop non-linear models for modelling echo path alongside traditional linear models. These non-linear models try to model the non-linear distortion, which is inflicted into the sound by the small and inexpensive audio equipment. The aim is also to consider side-effects of these non-linear models and how these side-effeets eould be avoided.Since these non-linear models are quite sensitive to the input signals, the mechanisms for controlling adaptive filtering algorithms are studied.",
    title = "Polynomial-based acoustic echo cancellation",
    year = "2006",
    school = "Tampere University of Technology"
}

@inproceedings{Shuyang2017_ICASSP,
    author = "Shuyang, Zhao and Heittola, Toni and Virtanen, Tuomas",
    keywords = "active learning;sound event classification;K-medoids clustering",
    title = "Active Learning for Sound Event Classification by Clustering Unlabeled Data",
    url = "https://trepo.tuni.fi/handle/10024/129132",
    booktitle = "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    pages = "751--755",
    year = "2017",
    organization = "IEEE"
}

@inproceedings{Paulus2002_ISMIR,
    author = "Paulus, Jouni and Klapuri, Anssi",
    editor = "Fingerhut, Michael",
    address = "Paris, France",
    booktitle = "Proc. of the Third International Conference on Music Information Retrieval",
    month = "Oct",
    pages = "150--156",
    title = "Measuring the Similarity of Rhythmic Patterns",
    year = "2002"
}

@inproceedings{Paulus2003_ICME,
    author = "Paulus, Jouni and Klapuri, Anssi",
    address = "Baltimore, Maryland, USA",
    booktitle = "Proc. of the IEEE International Conference on Multimedia and Expo",
    month = "Jul",
    pages = "737--740",
    title = "Conventional and Periodic {N}-grams in the Transcription of Drum Sequences",
    volume = "2",
    year = "2003"
}

@inproceedings{Paulus2003_DAFx,
    author = "Paulus, Jouni and Klapuri, Anssi",
    editor = "Davies, Mike",
    address = "London, UK",
    booktitle = "Proc. of the 6th International Conference on Digital Audio Effects",
    month = "Sep",
    pages = "73--77",
    title = "Model-based Event Labeling in the Transcription of Percussive Audio Signals",
    year = "2003"
}

@inproceedings{Paulus2005_EUSIPCO,
    author = "Paulus, Jouni and Virtanen, Tuomas",
    address = "Antalya, Turkey",
    booktitle = "Proc. of the 13th European Signal Processing Conference",
    keywords = "drums; NMF",
    month = "Sep",
    title = "Drum Transcription with Non-negative Spectrogram Factorisation",
    year = "2005",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/eusipco05\_paulus.pdf"
}

@inproceedings{Paulus2005_MIREX,
    author = "Paulus, Jouni",
    address = "London, UK",
    booktitle = "Proc. of the First Annual Music Information Retrieval Evaluation eXchange",
    keywords = "HMM",
    month = "Sep",
    title = "{D}rum {T}ranscription from {P}olyphonic {M}usic with {I}nstrument-wise {H}idden {M}arkov {M}odels",
    year = "2005"
}

@inproceedings{Paulus2006,
    author = "Paulus, Jouni and Klapuri, Anssi",
    address = "Santa Barbara, CA, USA",
    booktitle = "Proc. of the 1st ACM Audio and Music Computing Multimedia Workshop",
    month = "Oct",
    pages = "59--68",
    title = "Music Structure Analysis by Finding Repeated Parts",
    year = "2006"
}

@inproceedings{Paulus2007_ISMIR,
    author = "Paulus, Jouni and Klapuri, Anssi",
    address = "Vienna, Austria",
    booktitle = "Proc. of the 8th International Conference on Music Information Retrieval",
    keywords = "drums; HMM",
    month = "Sep",
    pages = "225--228",
    title = "{C}ombining temporal and spectral features in {HMM}-based drum transcription",
    year = "2007"
}

@inproceedings{Paulus2008_CMMR,
    author = "Paulus, Jouni and Klapuri, Anssi",
    editor = "Jensen, Kristoffer",
    address = "Copenhagen, Denmark",
    booktitle = "Proc. of the 2008 Computers in Music Modeling and Retrieval Conference",
    keywords = "Music structure analysis",
    month = "May",
    pages = "137--147",
    title = "{L}abelling the {S}tructural {P}arts of a {M}usic {P}iece with {M}arkov {M}odels",
    year = "2008"
}

@inproceedings{Paulus2008_DAFx,
    author = "Paulus, Jouni and Klapuri, Anssi",
    address = "Espoo, Finland",
    booktitle = "Proc. of the 11th International Conference on Digital Audio Effects",
    keywords = "Music structure analysis",
    month = "Sep",
    pages = "309--312",
    title = "{A}coustic {F}eatures for {M}usic {P}iece {S}tructure {A}nalysis",
    year = "2008"
}

@inproceedings{Paulus2008_ISMIR,
    author = "Paulus, Jouni and Klapuri, Anssi",
    address = "Philadelphia, PA, USA",
    booktitle = "Proc. of the Ninth International Conference on Music Information Retrieval",
    keywords = "Music structure analysis",
    month = "Sep",
    number = "369-374",
    series = {""},
    title = "{M}usic {S}tructure {A}nalysis {U}sing a {P}robabilistic {F}itness {M}easure {A}nd an {I}ntegrated {M}usicological {M}odel",
    year = "2008"
}

@article{Paulus2009_TASLP,
    author = "Paulus, Jouni and Klapuri, Anssi",
    journal = "IEEE Transactions on Audio, Speech, and Language Processing",
    keywords = "Music structure analysis",
    month = "Aug",
    number = "6",
    pages = "1159--1170",
    title = "{M}usic {S}tructure {A}nalysis {U}sing a {P}robabilistic {F}itness {M}easure and a {G}reedy {S}earch {A}lgorithm",
    volume = "17",
    year = "2009"
}

@incollection{Paulus2009,
    author = "Paulus, Jouni and Klapuri, Anssi",
    editor = "Ystad, S{\o}lvi and Kronland-Martinet, Richard and Jensen, Kristoffer",
    booktitle = "Computer Music Modeling and Retrieval: Genesis of Meaning in Sound and Music - 5th International Symposium, CMMR 2008 Copenhagen, Denmark, May 19-23, 2008, Revised Papers",
    keywords = "Music structure analysis",
    pages = "166--176",
    publisher = "Springer Berlin / Heidelberg",
    title = "{L}abelling the {S}tructural {P}arts of a {M}usic {P}iece with {M}arkov {M}odels",
    year = "2009"
}

@inproceedings{Paulus2009_MIREX,
    author = "Paulus, Jouni and Klapuri, Anssi",
    address = "Kobe, Japan",
    booktitle = "Proc. of the Fifth Annual Music Information Retrieval Evaluation eXchange",
    keywords = "Music structure analysis",
    month = "Oct",
    title = "{M}usic structure analysis with a probabilistic fitness function in {MIREX}2009",
    year = "2009"
}

@phdthesis{Paulus2010_phd,
    author = "Paulus, Jouni",
    abstract = "This thesis proposes signal processing methods for the analysis of musical audio on two time scales: drum transcription on a ﬁner time scale and music structure analysis on the time scale of entire pieces. The former refers to the process of locating drum sounds in the input and recognising the instruments that were used to produce the sounds. The latter refers to the temporal segmentation of a musical piece into parts, such as chorus and verse. For drum transcription, both low-level acoustic recognition and high-level musicological modelling methods are presented. A baseline acoustic recognition method with a large number of features using Gaussian mixture models for the recognition of drum combinations is presented. Since drums occur in structured patterns, modelling of the sequential dependencies with N-grams is proposed. In addition to the conventional N-grams, periodic N-grams are proposed to model the dependencies between events that occur one pattern length apart. The evaluations show that incorporating musicological modelling improves the performance considerably. As some drums are more probable to occur at certain points in a pattern, this dependency is utilised for producing transcriptions of signals produced with arbitrary sounds, such as beatboxing. A supervised source separation method using non-negative matrix factorisation is proposed for transcribing mixtures of drum sounds. Despite the simple signal model, a high performance is obtained for signals without other instruments. Most of the drum transcription methods operate only on single-channel inputs, but multichannel signals are available in recording studios. A multichannel extension of the source separation method is proposed, and an increase in performance is observed in evaluations. Many of the drum transcription methods rely on detecting sound onsets for the segmentation of the signal. Detection errors will then decrease the overall performance of the system. To overcome this problem, a method utilising a network of connected hidden Markov models is proposed to perform the event segmentation and recognition jointly. The system is shown to be able to perform the transcription even from polyphonic music. The second main topic of this thesis is music structure analysis. Two methods are proposed for this purpose. The ﬁrst relies on deﬁning a cost function for a description of the repeated parts. The second method deﬁnes a ﬁtness function for descriptions covering the entire piece. The abstract cost (and ﬁtness) functions are formulated in terms that can be determined from the input signal algorithmically, and optimisation problems are formulated. In both cases, an algorithm is proposed for solving the optimisation problems. The ﬁrst method is evaluated on a small data set, and the relevance of the cost function terms is shown. The latter method is evaluated on three large data sets with a total of 831 (557+174+100) songs. This is to date the largest evaluation of a structure analysis method. The evaluations show that the proposed method outperforms a reference system on two of the data sets. Music structure analysis methods rarely provide musically meaningful names for the parts in the result. A method is proposed to label the parts in descriptions based on a statistical model of the sequential dependencies between musical parts. The method is shown to label the main parts relatively reliably without any additional information. The labelling model is further integrated into the ﬁtness function based structure analysis method.",
    address = "Tampere, Finland",
    month = "Dec",
    school = "Tampere University of Technology",
    title = "{S}ignal {P}rocessing {M}ethods for {D}rum {T}ranscription and {M}usic {S}tructure {A}nalysis",
    year = "2010"
}

@mastersthesis{Rehtonen2002_master,
    author = "Rehtonen, Petri",
    abstract = "This thesis concerns the development and evaluation of signal processing methods that compensate for the hearing disorders caused by ageing. The global ageing increases the number of elderly people with hearing impairment. In most cases this impairment is irrecoverable and cannot be medically or surgically treated. The only option is to process the sound going to the ear, so that it would be as clear and intelligible as possible. The multichannel dynamic compression is such a processing. With the multichannel compression the need for amplified sound and the problems of redused dynamic range can be solved. The compression reduces the loudness fluctuations in the sound so that when sound is amplified, loud sounds do not sound too loud. As a degree of the hearing impairment is more severe in high frequency bands and by processing the bands individually. In this work, I have implemented a three- channel dynamic compression system controllable in real-time. The system was used to conduct listening tests for hearing impaired: The goal was to found a relationship between the audiogram (which represents the hearing threshold level and uncomfortable listening level measured by using sinusoids as stimuli) and the parameters of the multichannel dynamic compression system. In the first listening test, only a week relationship between the two was found. A second test was conducted in order to find out if the adjusted parameter were actually the most intelligible. This was realised using a paired comparison test. In this tests, four different processing were compared. The results shows that the spectral shaping was judged to be the processing which made speech the most intelligible. The compression was disliked and the possible reason for this is that the stimuli were very intelligible and power-normalised already before processing. There was no room for further improvement in intelligibility and thus the distortions caused by compression were perceived unpleasant",
    title = "{C}ompensation of {H}earing {D}efects {U}sing {M}ultichannel {D}ynamic {C}ompression",
    year = "2002",
    school = "Tampere University of Technology"
}

@inproceedings{Silen2013_Interspeech 2013,
    author = "Silen, Hanna and Nurminen, Jani and Helander, Elina and Gabbouj, Moncef",
    booktitle = "Proceedings of the 14th Annual Conference of the International Speech Communication Association (Interspeech 2013), 25-29 August, Lyon, France",
    pages = "373--377",
    publisher = "International Speech Communication Association",
    series = "Interspeech",
    title = "{V}oice {C}onversion for {N}on-{P}arallel {D}atasets {U}sing {D}ynamic {K}ernel {P}artial {L}east {S}quares {R}egression",
    year = "2013"
}

@phdthesis{Nikunen2015_phd,
    author = "Nikunen, Joonas",
    abstract = "This thesis studies several data decomposition algorithms for obtaining an object-based representation of an audio signal. The estimation of the representation parameters are coupled with audio-specific criteria, such as the spectral redundancy, sparsity, perceptual relevance and spatial position of sounds. The objective is to obtain an audio signal representation that is composed of meaningful entities called audio objects that reflect the properties of real-world sound objects and events. The estimation of the object-based model is based on magnitude spectrogram redundancy using non-negative matrix factorization with extensions to multichannel and complex-valued data. The benefits of working with object-based audio representations over the conventional time-frequency bin-wise processing are studied. The two main applications of the object-based audio representations proposed in this thesis are spatial audio coding and sound source separation from multichannel microphone array recordings. In the proposed spatial audio coding algorithm, the audio objects are estimated from the multichannel magnitude spectrogram. The audio objects are used for recovering the content of each original channel from a single downmixed signal, using time-frequency filtering. The perceptual relevance of modeling the audio signal is considered in the estimation of the parameters of the object-based model, and the sparsity of the model is utilized in encoding its parameters. Additionally, a quantization of the model parameters is proposed that reflects the perceptual relevance of each quantized element. The proposed object-based spatial audio coding algorithm is evaluated via listening tests and comparing the overall perceptual quality to conventional time-frequency block-wise methods at the same bitrates. The proposed approach is found to produce comparable coding efficiency while providing additional functionality via the object-based coding domain representation, such as the blind separation of the mixture of sound sources in the encoded channels. For the sound source separation from multichannel audio recorded by a microphone array, a method combining an object-based magnitude model and spatial covariance matrix estimation is considered. A direction of arrival-based model for the spatial covariance matrices of the sound sources is proposed. Unlike the conventional approaches, the estimation of the parameters of the proposed spatial covariance matrix model ensures a spatially coherent solution for the spatial parameterization of the sound sources. The separation quality is measured with objective criteria and the proposed method is shown to improve over the state-of-the-art sound source separation methods, with recordings done using a small microphone array.",
    isbn = "978-952-15-3438-6",
    month = "1",
    publisher = "Tampere University of Technology",
    series = "Tampere University of Technology. Publication",
    title = "{O}bject-based {M}odeling of {A}udio for {C}oding and {S}ource {S}eparation",
    url = "https://tutcris.tut.fi/portal/files/2460357/nikunen\_1276.pdf",
    year = "2015",
    school = "Tampere University of Technology"
}

@inproceedings{Mesaros2017_DCASE2017,
    author = "Mesaros, Annamaria and Heittola, Toni and Diment, Aleksandr and Elizalde, {Benjamin Martinez} and Shah, Ankit and Vincent, Emmanuel and Raj, Bhiksha and Virtanen, Tuomas",
    abstract = "DCASE 2017 Challenge consists of four tasks: acoustic scene classification, detection of rare sound events, sound event detection in real-life audio, and large-scale weakly supervised sound event detection for smart cars. This paper presents the setup of these tasks: task definition, dataset, experimental setup, and baseline system results on the development dataset. The baseline systems for all tasks rely on the same implementation using multilayer perceptron and log mel-energies, but differ in the structure of the output layer and the decision making process, as well as the evaluation of system output using task specific metrics.",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    keywords = "Sound scene analysis; Acoustic scene classification; Sound event detection; Audio tagging; Rare sound events",
    pages = "85--92",
    publisher = "Tampere University of Technology. Laboratory of Signal Processing",
    title = "{DCASE} 2017 challenge setup: tasks, datasets and baseline system",
    year = "2017",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/dcase-2017-challenge-paper.pdf"
}

@inproceedings{Mimilakis2018_ICASSP,
    author = "Mimilakis, Stylianos Ioannis and Drossos, Konstantinos and Santos, Jo{\\textasciitilde a}o F. and Schuller, Gerald and Virtanen, Tuomas and Bengio, Yoshua",
    abstract = "Singing voice separation based on deep learning relies on the usage of time-frequency masking. In many cases the masking process is not a learnable function or is not encapsulated into the deep learning optimization. Consequently, most of the existing methods rely on a post processing step using the generalized Wiener filtering. This work proposes a method that learns and optimizes (during training) a source-dependent mask and does not need the aforementioned post processing step. We introduce a recurrent inference algorithm, a sparse transformation step to improve the mask generation process, and a learned denoising filter. Obtained results show an increase of 0.49 dB for the signal to distortion ratio and 0.30 dB for the signal to interference ratio, compared to previous state-of-the-art approaches for monaural singing voice separation.",
    title = "Monaural Singing Voice Separation with Skip-Filtering Connections and Recurrent Inference of Time-Frequency Mask",
    booktitle = "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    pages = "721--725",
    year = "2018",
    organization = "IEEE",
    url = "https://arxiv.org/abs/1711.01437"
}

@inproceedings{Naithani2018_IWAENC,
    author = "Naithani, Gaurav and Nikunen, Joonas and Bramslow, Lars and Virtanen, Tuomas",
    abstract = "Mean square error (MSE) has been the preferred choice as loss function in the current deep neural network (DNN) based speech separation techniques. In this paper, we propose a new cost function with the aim of optimizing the extended short time objective intelligibility (ESTOI) measure. We focus on applications where low algorithmic latency (≤ 10 ms) is important. We use long short-term memory networks (LSTM) and evaluate our proposed approach on four sets of two-speaker mixtures from extended Danish hearing in noise (HINT) dataset. We show that the proposed loss function can offer improved or at par objective intelligibility (in terms of ESTOI) compared to an MSE optimized baseline while resulting in lower objective separation performance (in terms of the source to distortion ratio (SDR)). We then proceed to propose an approach where the network is first initialized with weights optimized for MSE criterion and then trained with the proposed ESTOI loss criterion. This approach mitigates some of the losses in objective separation performance while preserving the gains in objective intelligibility.",
    booktitle = "16th International Workshop on Acoustic Signal Enhancement, IWAENC 2018",
    day = "2",
    doi = "10.1109/IWAENC.2018.8521379",
    keywords = "Deep neural networks; Low latency; Speech intelligibility; Speech separation",
    month = "11",
    pages = "386--390",
    publisher = "IEEE",
    title = "{D}eep neural network based speech separation optimizing an objective estimator of intelligibility for low latency applications",
    year = "2018",
    url = "https://arxiv.org/pdf/1807.06899.pdf"
}

@mastersthesis{Yli-Hietanen1995_master,
    author = "Yli-Hietanen, Jari",
    title = {{T}ulosuunnan estimointi k{\"a}ytt{\"a}en pient{\"a} kolmiulotteista sensorij{\"a}rjestelm{\"a}{\"a}},
    year = "1995",
    school = "Tampere University of Technology"
}

@inproceedings{Yli-Hietanen1996_NORSIG,
    author = {Yli-Hietanen, Jari and Kallioj{\"a}rvi, Kari and Astola, Jaakko},
    booktitle = "Proceedings of Norsig'96",
    title = "{R}obust {T}ime-{D}elay {B}ased {A}ngle of {A}rrival {E}stimation",
    year = "1996"
}

@inproceedings{Yli-Hietanen1998_ICSPAT,
    author = "Yli-Hietanen, Jari and Koppinen, Konsta and Paajanen, Erkki",
    address = "Toronto, Canada",
    booktitle = "ICSPAT98",
    month = "September",
    pages = "1277-1280",
    title = "{S}iren {S}ound {S}uppression for {S}peech {E}nhancement in {M}obile {C}ommunications",
    year = "1998"
}

@inproceedings{Yli-Hietanen1999_SIP99,
    author = "Yli-Hietanen, Jari and Koppinen, Konsta and Astola, Jaakko",
    address = "Nassau, Bahamas",
    booktitle = "Proceedings of the IASTED Internatioanl Conference Signal and Image Processing (SIP99)",
    month = "October",
    title = "{T}ime-delay {S}election for {R}obust {A}ngle of {A}rrival {E}stimation",
    year = "1999"
}

@inproceedings{Yli-Hietanen2000_NORSIG,
    author = "Yli-Hietanen, Jari and Saarelainen, Teemu and Routakangas, Jussi",
    address = "Kolm{\aa}rden, Sweden",
    booktitle = "In Proceedings of the IEEE Nordic Signal Processing Symposium (NORSIG)",
    month = "June",
    pages = "65-68",
    title = "Robust Angle-of-Arrival Estimation of Transient Signals",
    year = "2000"
}

@inproceedings{Helén2009_ICA,
    author = "Hel{\'e}n, Marko and Lahti, Tommi and Klapuri, Anssi",
    editor = "Niiranen, S.",
    booktitle = "Open Information Management: applications of interconnectivity and collaboration",
    isbn = "978-1-60566-246-6",
    pages = "244--265",
    title = "{T}ools for automatic audio management",
    year = "2009"
}

@inproceedings{Kivimäki2000_EUSIPCO,
    author = {Kivim{\"a}ki, Jukka and Lahti, Tommi and Koppinen, Konsta},
    address = "Tampere, Finland",
    booktitle = "Proceedings of the X European Signal Processing Conference (EUSIPCO)",
    month = "September",
    pages = "1301-1304",
    title = "{A} Phonetic Vocoder for Finnish",
    year = "2000"
}

@article{Bilcu2008,
    author = {Bilcu, Enik{\"o} Beatrice and Astola, Jaakko},
    issn = "0353-3670",
    journal = "Facta Universitatis, Series: Electronics and Energetics",
    number = "1",
    pages = "91--105",
    title = "{A} hybrid approach to bilingual text-to-phoneme mapping",
    volume = "21",
    year = "2008"
}

@inproceedings{Diment2019_IJCNN,
    author = "Diment, Aleksandr and Fagerlund, Eemi and Benfield, Adrian and Virtanen, Tuomas",
    abstract = "A machine learning method for the automatic detection of pronunciation errors made by non-native speakers of English is proposed. It consists of training word-specific binary classifiers on a collected dataset of isolated words with possible pronunciation errors, typical for Finnish native speakers. The classifiers predict whether the typical error is present in the given word utterance. They operate on sequences of acoustic features, extracted from consecutive frames of an audio recording of a word utterance. The proposed architecture includes a convolutional neural network, a recurrent neural network, or a combination of the two. The optimal topology and hyperpa-rameters are obtained in a Bayesian optimisation setting using a tree-structured Parzen estimator. A dataset of 80 words uttered naturally by 120 speakers is collected. The performance of the proposed system, evaluated on a well-represented subset of the dataset, shows that it is capable of detecting pronunciation errors in most of the words (46/49) with high accuracy (mean accuracy gain over the zero rule 12.21 percent points).",
    booktitle = "2019 International Joint Conference on Neural Networks, IJCNN 2019",
    day = "1",
    doi = "10.1109/IJCNN.2019.8851963",
    keywords = "Computer-assisted language learning; computer-assisted pronunciation training CNN; CRNN; GRU; pronunciation learning",
    month = "7",
    publisher = "IEEE",
    title = "Detection of Typical Pronunciation Errors in Non-native English Speech Using Convolutional Recurrent Neural Networks",
    year = "2019",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/Diment19\_PL.pdf"
}

@mastersthesis{Rämö1999_master,
    author = {R{\"a}m{\"o}, Anssi},
    title = "{P}itch {M}odification and {Q}uantization for {O}ffline {S}peech {C}oding",
    year = "1999",
    school = "Tampere University of Technology"
}

@inproceedings{Magron2018_InterSpecch,
    author = "Magron, Paul and Virtanen, Tuomas",
    abstract = "This paper presents novel expectation-maximization (EM) algorithms for estimating the nonnegative matrix factorization model with Itakura-Saito divergence. Indeed, the common EM-based approach exploits the space-alternating generalized EM (SAGE) variant of EM but it usually performs worse than the conventional multiplicative algorithm. We propose to explore more exhaustively those algorithms, in particular the choice of the methodology (standard EM or SAGE variant) and the latent variable set (full or reduced). We then derive four EM-based algorithms, among which three are novel. Speech separation experiments show that one of those novel algorithms using a standard EM methodology and a reduced set of latent variables outperforms its SAGE variants and competes with the conventional multiplicative algorithm.",
    booktitle = "Interspeech 2018",
    keywords = "source separation",
    publisher = "Interspeech",
    series = "Interspeech",
    title = "{E}xpectation-maximization algorithms for {I}takura-{S}aito nonnegative matrix factorization",
    year = "2018",
    url = "https://hal.archives-ouvertes.fr/hal-01632082/document"
}

@article{Mesaros2018_TASLP,
    author = "Mesaros, Annamaria and Heittola, Toni and Benetos, Emmanouil and Foster, Peter and Lagrange, Mathieu and Virtanen, Tuomas and Plumbley, Mark D.",
    abstract = "Public evaluation campaigns and datasets promote active development in target research areas, allowing direct comparison of algorithms. The second edition of the challenge on Detection and Classification of Acoustic Scenes and Events (DCASE 2016) has offered such an opportunity for development of state-of-the-art methods, and succeeded in drawing together a large number of participants from academic and industrial backgrounds. In this paper, we report on the tasks and outcomes of the DCASE 2016 challenge. The challenge comprised four tasks: acoustic scene classification, sound event detection in synthetic audio, sound event detection in real-life audio, and domestic audio tagging. We present in detail each task and analyse the submitted systems in terms of design and performance. We observe the emergence of deep learning as the most popular classification method, replacing the traditional approaches based on Gaussian mixture models and support vector machines. By contrast, feature representations have not changed substantially throughout the years, as mel frequency-based representations predominate in all tasks. The datasets created for and used in DCASE 2016 are publicly available and are a valuable resource for further research.",
    doi = "10.1109/TASLP.2017.2778423",
    issn = "2329-9290",
    journal = "IEEE-ACM Transactions on Audio Speech and Language Processing",
    keywords = "Acoustic scene classification;Acoustics;audio datasets;Event detection;Hidden Markov models;pattern recognition;sound event detection;Speech;Speech processing;Tagging;audio tagging",
    month = "11",
    publisher = "IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC",
    title = "Detection and Classification of Acoustic Scenes and Events: Outcome of the {DCASE} 2016 Challenge",
    year = "2018",
    url = "https://trepo.tuni.fi//bitstream/handle/10024/126402/dcase2016\_taslp.pdf?sequence=1"
}

@ARTICLE{Virtanen2007_TASLP,
    author = "Virtanen, Tuomas",
    journal = "IEEE Transactions on Audio, Speech, and Language Processing",
    title = "Monaural Sound Source Separation by Nonnegative Matrix Factorization With Temporal Continuity and Sparseness Criteria",
    year = "2007",
    volume = "15",
    number = "3",
    pages = "1066-1074",
    keywords = "Source separation;Unsupervised learning;Multiple signal classification;Spectrogram;Machine learning algorithms;Music;Sparse matrices;Humans;Independent component analysis;Costs;Acoustic signal analysis;audio source separation;blind source separation;music;nonnegative matrix factorization;sparse coding;unsupervised learning",
    doi = "10.1109/TASL.2006.885253",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/virtanen\_taslp2007.pdf"
}

@book{Virtanen2017,
    author = "Virtanen, Tuomas and Plumbley, Mark D. and Ellis, Dan",
    abstract = "This book presents computational methods for extracting the useful information from audio signals, collecting the state of the art in the field of sound event and scene analysis. The authors cover the entire procedure for developing such methods, ranging from data acquisition and labeling, through the design of taxonomies used in the systems, to signal processing methods for feature extraction and machine learning methods for sound recognition. The book also covers advanced techniques for dealing with environmental variation and multiple overlapping sound sources, and taking advantage of multiple microphones or other modalities. The book gives examples of usage scenarios in large media databases, acoustic monitoring, bioacoustics, and context-aware devices. Graphical illustrations of sound signals and their spectrographic representations are presented, as well as block diagrams and pseudocode of algorithms.",
    doi = "10.1007/978-3-319-63450-0",
    isbn = "978-3-319-63449-4",
    month = "9",
    publisher = "Springer",
    title = "Computational analysis of sound scenes and events",
    year = "2017",
    url = "http://www.springer.com/us/book/9783319634494"
}

@inproceedings{Virtanen2000_ICASSP,
    author = "Virtanen, Tuomas and Klapuri, Anssi",
    address = "Istanbul, Turkey",
    booktitle = "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    keywords = "Separation; sinusoidal model",
    title = "{S}eparation of {H}armonic {S}ound {S}ources {U}sing {S}inusoidal {M}odeling",
    year = "2000"
}

@mastersthesis{Virtanen2001_master,
    author = "Virtanen, Tuomas",
    abstract = "In audio signal spectrum modeling, the aim is to transform a signal to a more easily applicable form, removing the information that is irrelevant in signal perception. Sinusoids plus noise model is a spectral model, in which the periodic components of the sound are represented with sinusoids with time-varying frequencies, amplitudes and phases. The remaining non-periodic components are represented with a ﬁltered noise. The sinusoidal model utilizes the physical properties of musical instruments and the noise model the humans’ inability to perceive the exact spectral shape or phase of stochastic signals. In the case of polyphonic music signals, the estimation of the parameters of sinusoids is a difficult task, since the periodic components are usually not stable. A sufficient time and frequency resolution is also difficult to achieve at the same time. A big part of this thesis discusses the detection and parameter estimation of periodic components with several algorithms. In addition to already existing algorithms, a new iterative algorithm is presented, which is based on the fusion of closely spaced sinusoids. The sinusoidal model is applied in the separation of overlapping sounds and manipulation. In the sound separation, a new perceptual distance measure between sinusoids is used. The perceptual distance measure is based on the humans’ way to associate spectral components into sound sources. Also a new separation method based on the multipitch estimation is explained. The modification of the pitch and time scale of sounds with the sinusoid plus noise model without affecting the quality of the sound is explained shortly, too.",
    title = "{A}udio {S}ignal {M}odeling with {S}inusoids {P}lus {N}oise",
    year = "2001",
    school = "Tampere University of Technology"
}

@inproceedings{Virtanen2003_ICMC,
    author = "Virtanen, Tuomas",
    booktitle = "International Computer Music Conference",
    keywords = "NMF;sparseness",
    title = "Sound Source Separation Using Sparse Coding with Temporal Continuity Objective",
    year = "2003",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/icmc2003.pdf"
}

@phdthesis{Virtanen2006_phd,
    author = "Virtanen, Tuomas",
    abstract = "Sound source separation refers to the task of estimating the signals produced by individual sound sources from a complex acoustic mixture. It has several applications, since monophonic signals can be processed more eﬃciently and ﬂexibly than polyphonic mixtures. This thesis deals with the separation of monaural, or, one-channel music recordings. We concentrate on separation methods, where the sources to be separated are not known beforehand. Instead, the separation is enabled by utilizing the common properties of real-world sound sources, which are their continuity, sparseness, and repetition in time and frequency, and their harmonic spectral structures. One of the separation approaches taken here use unsupervised learning and the other uses model-based inference based on sinusoidal modeling. Most of the existing unsupervised separation algorithms are based on a linear instantaneous signal model, where each frame of the input mixture signal is modeled as a weighted sum of basis functions. We review the existing algorithms which use independent component analysis, sparse coding, and non-negative matrix factorization to estimate the basis functions from an input mixture signal. Our proposed unsupervised separation algorithm based on the instantaneous model combines non-negative matrix factorization with sparseness and temporal continuity objectives. The algorithm is based on minimizing the reconstruction error between the magnitude spectrogram of the observed signal and the model, while restricting the basis functions and their gains to non-negative values, and the gains to be sparse and continuous in time. In the minimization, we consider iterative algorithms which are initialized with random values and updated so that the value of the total objective cost function decreases at each iteration. Both multiplicative update rules and a steepest descent algorithm are proposed for this task. To improve the convergence of the projected steepest descent algorithm, we propose an augmented divergence to measure the reconstruction error. Simulation experiments on generated mixtures of pitched instruments and drums were run to monitor the behavior of the proposed method. The proposed method enables average signal-to-distortion ratio (SDR) of 7.3 dB, which is higher than the SDRs obtained with the other tested methods based on the instantaneous signal model. To enable separating entities which correspond better to real-world sound objects, we propose two convolutive signal models which can be used to represent time-varying spectra and fundamental frequencies. We propose unsupervised learning algorithms extended from non-negative matrix factorization for estimating the model parameters from a mixture signal. The objective in them is to minimize the reconstruction error between the magnitude spectrogram of the observed signal and the model while restricting the parameters to non-negative values. Simulation experiments show that time-varying spectra enable better separation quality of drum sounds, and time-varying frequencies representing diﬀerent fundamental frequency values of pitched instruments conveniently. Another class of studied separation algorithms is based on the sinusoidal model, where the periodic components of a signal are represented as the sum of sinusoids with time-varying frequencies, amplitudes, and phases. The model provides a good representation for pitched instrument sounds, and the robustness of the parameter estimation is here increased by restricting the sinusoids of each source to harmonic frequency relationships. Our proposed separation algorithm based on sinusoidal modeling minimizes the reconstruction error between the observed signal and the model. Since the rough shape of spectrum of natural sounds is continuous as a function of frequency, the amplitudes of overlapping overtones can be approximated by interpolating from adjacent overtones, for which we propose several methods. Simulation experiments on generated mixtures of pitched musical instruments show that the proposed methods allow average SDR above 15 dB for two simultaneous sources, and the quality decreases gradually as the number of sources increases.",
    title = "Sound Source Separation in Monaural Music Signals",
    year = "2006",
    school = "Tampere University of Technology"
}
