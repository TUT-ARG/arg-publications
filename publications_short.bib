@inbook{Heittola2018,
    author = "Heittola, Toni and Cakir, Emre and Virtanen, Tuomas",
    abstract = "This chapter explains the basic concepts in computational methods used for analysis of sound scenes and events. Even though the analysis tasks in many applications seem different, the underlying computational methods are typically based on the same principles. We explain the commonalities between analysis tasks such as sound event detection, sound scene classification, or audio tagging. We focus on the machine learning approach, where the sound categories (i.e., classes) to be analyzed are defined in advance. We explain the typical components of an analysis system, including signal pre-processing, feature extraction, and pattern classification. We also preset an example system based on multi-label deep neural networks, which has been found to be applicable in many analysis tasks discussed in this book. Finally, we explain the whole processing chain that involves developing computational audio analysis systems.",
    booktitle = "Computational Analysis of Sound Scenes and Events",
    doi = "10.1007/978-3-319-63450-0\_2",
    editor2 = "Tuomas Virtanen and Plumbley, Mark D. and Dan Ellis",
    isbn = "978-3-319-63449-4",
    month = "9",
    pages = "13--40",
    publisher = "Springer",
    title = "The machine learning approach for analysis of sound scenes and events",
    year = "2018"
}

@inproceedings{Silen2012_InterSpecch,
    author = "Silen, Hanna and Helander, Elina and Nurminen, Jani and Gabbouj, Moncef",
    booktitle = "Proceedings of 13th Annual Conference of the International Speech Communication Association, Interspeech 2012, September 9 - 13, Portland, Oregon, USA",
    pages = "1--4",
    publisher = "International Speech Communication Association ISCA",
    series = "Interspeech",
    title = "{W}ays to {I}mplement {G}lobal {V}ariance in {S}tatistical {S}peech {S}ynthesis",
    year = "2012"
}

@article{Purwins2019_JSTSP,
    author = {Purwins, Hendrik and Li, Bo and Virtanen, Tuomas and Sch{\"u}ller, Jan and Chang, Shuo-yiin and Sainath, Tara},
    abstract = "Given the recent surge in developments of deep learning, this paper provides a review of the state-of-the-art deep learning techniques for audio signal processing. Speech, music, and environmental sound processing are considered side-by-side, in order to point out similarities and differences between the domains, highlighting general methods, problems, key references, and potential for cross fertilization between areas. The dominant feature representations (in particular, log-mel spectra and raw waveform) and deep learning models are reviewed, including convolutional neural networks, variants of the long short-term memory architecture, as well as more audio-specific neural network models. Subsequently, prominent deep learning application areas are covered, i.e., audio recognition (automatic speech recognition, music information retrieval, environmental sound detection, localization and tracking) and synthesis and transformation (source separation, audio enhancement, generative models for speech, sound, and music synthesis). Finally, key issues and future questions regarding deep learning applied to audio signal processing are identified.",
    day = "1",
    doi = "10.1109/JSTSP.2019.2908700",
    issn = "1932-4553",
    journal = "IEEE Journal of Selected Topics in Signal Processing",
    keywords = "audio enhancement;automatic speech recognition;connectionist temporal memory;Deep learning;environmental sounds;music information retrieval;source separation",
    month = "5",
    number = "2",
    pages = "206--219",
    publisher = "Institute of Electrical and Electronics Engineers",
    title = "Deep Learning for Audio Signal Processing",
    volume = "13",
    year = "2019",
    url = "https://arxiv.org/abs/1905.00078"
}

@inproceedings{Mimilakis2017_MLSP,
    author = "Mimilakis, Stylianos Ioannis and Drossos, Konstantinos and Virtanen, Tuomas and Schuller, Gerald",
    abstract = "The objective of deep learning methods based on encoder-decoder architectures for music source separation is to approximate either ideal time-frequency masks or spectral representations of the target music source(s). The spectral representations are then used to derive time-frequency masks. In this work we introduce a method to directly learn time-frequency masks from an observed mixture magnitude spectrum. We employ recurrent neural networks and train them using prior knowledge only for the magnitude spectrum of the target source. To assess the performance of the proposed method, we focus on the task of singing voice separation. The results from an objective evaluation show that our proposed method provides comparable results to deep learning based methods which operate over complicated signal representations. Compared to previous methods that approximate time-frequency masks, our method has increased performance of signal to distortion ratio by an average of 3.8 dB.",
    booktitle = "27th IEEE International Workshop on Machine Learning for Signal Processing (MLSP)",
    doi = "10.1109/MLSP.2017.8168117",
    keywords = "singing voice separation;",
    publisher = "IEEE",
    title = "A Recurrent Encoder-Decoder Approach With Skip-Filtering Connections for Monaural Singing Voice Separation",
    year = "2017",
    url = "https://arxiv.org/pdf/1709.00611.pdf"
}

@inproceedings{Rosti2000_EUSIPCO,
    author = "Rosti, Antti-Veikko and Koivunen, Visa",
    editor = "Gabbouj, M. and Kuosmanen, P.",
    abstract = "Modulation classification has many important applications in communications, e.g., reconfigurable receivers, spectrum managment and interference cancellation. In this paper we address the problem of classifying digitally modulated signals using cyclostationary statistics. We derive the first-order moments of the complex envelope of digitally modulated signals and verify their periodicity. A novel feature for the classification of the frequency shift keyed signals is proposed. The performance of this feature in distinguishing among different FSK constellations is studied in simulation. Some comparisons to commonly used features are performed.",
    booktitle = "Signal Processing X Theories and Applications, Proceedings of EUSIPCO 2000, tenth European Signal processing Conference, 4-8 September 2000, Tampere, Finland",
    isbn = "952-15-0443-9",
    pages = "581--584",
    title = "Classification of mfsk modulated signals using the mean of complex envelope",
    year = "2000"
}

@inproceedings{Gemmeke2010_Interspeech 2010,
    author = "Gemmeke, Jort and Virtanen, Tuomas",
    booktitle = "Proceedings of the 11th Annual Conference of the International Speech Communication Association (Interspeech 2010)",
    pages = "2082--2085",
    title = "Artificial and online acquired noise dictionaries for noise robust {ASR}",
    year = "2010",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/noise\_dictionaries.pdf"
}

@inproceedings{Mahkonen2016_EUSIPCO,
    author = {Mahkonen, Katariina and Hurmalainen, Antti and Virtanen, Tuomas and K{\"a}m{\"a}r{\"a}inen, Joni-Kristian},
    abstract = "Sparse representations have been found to provide high classification accuracy in many fields. Their drawback is the high computational load. In this work, we propose a novel cascaded classifier structure to speed up the decision process while utilizing sparse signal representation. In particular, we apply the cascaded decision process for noise robust automatic speech recognition task. The cascaded decision process is implemented using a feedforward neural network (NN) and time sparse versions of a non-negative matrix factorization (NMF) based sparse classification method of [1]. The recognition accuracy of our cascade is among the three best in the recent CHiME2013 benchmark and obtains six times faster the accuracy of NMF alone as in [1].",
    booktitle = "European Signal Processing Conference (EUSIPCO), 2016",
    doi = "10.1109/EUSIPCO.2016.7760660",
    publisher = "IEEE",
    title = "Cascade processing for speeding up sliding window sparse classification",
    year = "2016",
    url = "http://vision.cs.tut.fi/data/publications/eusipco2016\_cascaded\_nmf.pdf"
}

@book{Klapuri2006,
    author = "Klapuri, Anssi and Davy, Manuel",
    title = "Signal Processing Methods for Music Transcription",
    year = "2006",
    isbn = "0387306676",
    publisher = "Springer-Verlag",
    address = "Berlin, Heidelberg"
}

@mastersthesis{Ryynänen2004_master,
    author = {Ryyn{\"a}nen, Matti},
    abstract = "This thesis concerns the problem of automatic transcription of music and proposes a method for transcribing monophonic melodies. Recently, computational music content analysis has received considerable attention among researchers due to the rapid growth of music databases. In this area of research, melody transcription plays an important role. Automatic melody transcription can beneﬁt professional musicians and provide interesting applications for consumers, including music retrieval and interactive music applications. The proposed method is based on two probabilistic models: a note event model and a musicological model. The note event model is used to represent note candidates in melodies, and it is based on hidden Markov models. The note event model is trained with an acoustic database of singing sequences, but the training can be done for any melodic instrument and for the desired front-end feature extractors. The musicological model is used to control the transitions between the note candidates by using musical key estimation and by computing the likelihoods of note sequences. The two models are combined to constitute a melody transcription system with a modular architecture. The transcription system is evaluated, and the results are good. Our system transcribes correctly over 90 \% of notes, thus halving the amount of errors compared to a simple rounding of pitch estimates to the nearest MIDI note numbers. Particularly, using the note event model signiﬁcantly improves the system performance.",
    title = "{P}robabilistic {M}odelling of {N}ote {E}vents in the {T}ranscription of {M}onophonic {M}elodies",
    year = "2004",
    school = "Tampere University of Technology"
}

@phdthesis{Ryynänen2009_phd,
    author = {Ryyn{\"a}nen, Matti},
    abstract = "Transcription of music refers to the analysis of a music signal in order to produce a parametric representation of the sounding notes in the signal. This is conventionally carried out by listening to a piece of music and writing down the symbols of common musical notation to represent the occurring notes in the piece. Automatic transcription of music refers to the extraction of such representations using signal-processing methods. This thesis concerns the automatic transcription of pitched notes in musical audio and its applications. Emphasis is laid on the transcription of realistic polyphonic music, where multiple pitched and percussive instruments are sounding simultaneously. The methods included in this thesis are based on a framework which combines both low-level acoustic modeling and high-level musicological modeling. The emphasis in the acoustic modeling has been set to note events so that the methods produce discrete-pitch notes with onset times and durations as output. Such transcriptions can be efficiently represented as MIDI files, for example, and the transcriptions can be converted to common musical notation via temporal quantization of the note onsets and durations. The musicological model utilizes musical context and trained models of typical note sequences in the transcription process. Based on the framework, this thesis presents methods for generic polyphonic transcription, melody transcription, and bass line transcription. A method for chord transcription is also presented. All the proposed methods have been extensively evaluated using realistic polyphonic music. In our evaluations with 91 half-a-minute music excerpts, the generic polyphonic transcription method correctly found 39\% of all the pitched notes (recall) where 41\% of the transcribed notes were correct (precision). Despite the seemingly low recognition rates in our simulations, this method was top-ranked in the polyphonic note tracking task in the international MIREX evaluation in 2007 and 2008. The methods for the melody, bass line, and chord transcription were evaluated using hours of music, where F-measure of 51\% was achieved for both melodies and bass lines. The chord transcription method was evaluated using the first eight albums by The Beatles and it produced correct frame-based labeling for about 70\% of the time. The transcriptions are not only useful as human-readable musical notation but in several other application areas too, including music information retrieval and content-based audio modification. This is demonstrated by two applications included in this thesis. The first application is a query by humming system which is capable of searching melodies similar to a user query directly from commercial music recordings. In our evaluation with a database of 427 full commercial audio recordings, the method retrieved the correct recording in the topthree list for the 58\% of 159 hummed queries. The method was also top-ranked in “query by singing/humming” task in MIREX 2008 for a database of 2048 MIDI melodies and 2797 queries. The second application uses automatic melody transcription for accompaniment and vocals separation. The transcription also enables tuning the user singing to the original melody in a novel karaoke application.",
    title = "{A}utomatic {T}ranscription of {P}itch {C}ontent in {M}usic and {S}elected {A}pplications",
    year = "2009",
    school = "Tampere University of Technology"
}

@inproceedings{Heittola2011,
    author = "Heittola, Toni and Mesaros, Annamaria and Virtanen, Tuomas and Eronen, Antti",
    booktitle = {Akustiikkap{\"a}iv{\"a}t 2011},
    pages = "51--56",
    title = "Sound event detection and context recognition",
    year = "2011"
}

@article{Nikunen2016_ICA,
    author = "Nikunen, Joonas and Diment, Aleksandr and Virtanen, Tuomas and Vilermo, Miikka",
    journal = "Speech Communication",
    pages = "157--169",
    title = "Binaural rendering of microphone array captures based on source separation",
    url = "http://www.sciencedirect.com/science/article/pii/S0167639315001004",
    volume = "76",
    year = "2016"
}

@inproceedings{Drossos2017_WASPAA,
    author = "Drossos, Konstantinos and Adavanne, Sharath and Virtanen, Tuomas",
    abstract = "We present the first approach to automated audio captioning. We employ an encoder-decoder scheme with an alignment model in between. The input to the encoder is a sequence of log mel-band energies calculated from an audio file, while the output is a sequence of words, i.e. a caption. The encoder is a multi-layered, bi-directional gated recurrent unit (GRU) and the decoder a multi-layered GRU with a classification layer connected to the last GRU of the decoder. The classification layer and the alignment model are fully connected layers with shared weights between timesteps. The proposed method is evaluated using data drawn from a commercial sound effects library, ProSound Effects. The resulting captions were rated through metrics utilized in machine translation and image captioning fields. Results from metrics show that the proposed method can predict words appearing in the original caption, but not always correctly ordered.",
    booktitle = "IEEE Workshop on Applications of Signal Processing to Audio and Acoustics",
    doi = "10.1109/WASPAA.2017.8170058",
    isbn = "978-1-5386-1632-1",
    keywords = "audio captioning",
    publisher = "IEEE",
    title = "Automated Audio Captioning with Recurrent Neural Networks",
    year = "2017",
    url = "https://arxiv.org/abs/1706.10006"
}

@phdthesis{Bilcu2008_phd,
    author = {Bilcu, Enik{\"o} Beatrice},
    abstract = "Text-to-phoneme (TTP) mapping, also called grapheme-to-phoneme (GTP) conversion, defines the process of transforming a written text into its corresponding phonetic transcription. Text-to-phoneme mapping is a necessary step in any state-of-the-art automatic speech recognition (ASR) and text-to-speech (TTS) system, where the textual information changes dynamically (i.e., new contact entries for name dialing, or new short messages or emails to be read out by a device). There are significant differences between the implementation requirements of a text-to-phoneme mapping module embedded into the automatic speech recognition and into the text-to-speech systems: in automatic speech recognition systems the errors of the text-to-phoneme mapping module are tolerated better (leading to occasional recognition errors) than in the text-to-speech applications, where the effect is immediately and in all cases audible. Automatic speech recognition systems typically use text-to-phoneme mapping to lower the footprint (to avoid storing the lexicon), while maintaining quality. The use of text-to-phoneme mapping in the text-to-speech systems is different. In addition to the phonetic information, the text-to-speech systems also need prosodic information to be able to produce high quality speech, which cannot be predicted by text-to-phoneme mapping. Most state-of-the-art text-to-speech systems use explicit pronunciation lexicon, which is aimed at providing the widest possible coverage, in the order of 100K words, with high quality pronunciation information. Because of this reason, text-to-phoneme mapping is typically used as a fall-back strategy, when the system encounters very rare or non-native words and the quality of a ext-to-speech system is indirectly affected by the quality of the grapheme-to-phoneme conversion. Another important issue is the question of training the text-to-phoneme mapping module. The problem of grapheme-to-phoneme conversion is a static one and such a system is trained off-line. The correspondence between the written and spoken form of a language is usually unchanged in the lifetime of an application. So the complexity/speed of the model training is of secondary importance compared to e.g., the speed of convergence or model size. In this thesis, the problem of text-to-phoneme mapping using neural networks is studied. One of the main goals of the thesis is to provide a comprehensive analysis of different neural network structures which can be implemented to convert a written text into its corresponding phonetic transcription. Another important target, of this work, is to provide new solutions that improve the performance of the existing algorithms, in terms of convergence speed and phoneme accuracy. Three main neural network classes are studied in this thesis: the multilayer perceptron (MLP) neural network, the recurrent neural network (RNN) and the bidirectional recurrent neural network (BRNN). Due to their ability of self adaptation, neural networks have been shown to be a viable solution in applications that require modeling abilities. Such an application is the text-tophoneme mapping where the correspondence between letters of a written text and their corresponding phonetic transcription must be modeled. One of the main concerns in all practical implementations, where neural networks are used, is to develop algorithms which provide fast convergence of the synaptic weights and in the same time good mapping performances. When a neural network is trained for text-to-phoneme mapping, at every iteration, a letter-phoneme pair is presented to the network such that, the number of letters and the number of training iterations are equal. As a result, fast convergence of the neural network means smaller size of the training dictionary since fast convergence is in fact similar to less necessary training letters1. A fast convergence speed is important in applications where only a small linguistic database is available. Of course, one solution could be to use a small dictionary (with very few words) which is presented at the input of the neural network many times until the convergence of the synaptic weights is reached. In this case the time of training becomes more important. Taking into account these two sides of the convergence speed (the size of the training dictionary and the processing time during training) one can understand the importance of having algorithms that ensure fast convergence of the neural network. It is well known that the error back-propagation algorithm which is used to train the MLP neural network, possess sometimes a quite slow convergence (a very large number of iterations required to reach the stability point). In order to increase the convergence speed two novel alternative solutions are proposed in this thesis: one using an adaptive learning rate in the training process and another which is a transform domain implementation of the multilayer perceptron neural network. The computational complexity of the two proposed training algorithms is slightly higher than the computational complexity of the error back-propagation algorithm but the number of training iterations is highly reduced. Due to this fact, although the three algorithms might have the same training time, the novel algorithms necessitate smaller training dictionary. Due to the limitations of the processing power that usually are encountered in real devices, another very important requirement for a text-to-phoneme mapping system is to have low computational and memory costs. In the case of text-to-phoneme mapping systems based in neural networks, the computational complexity is mainly linked to the mathematical complexity of the training algorithm as well as to the number of the synaptic weights of the neural network. Memory load is due to the number of synaptic weights of the neural network which must be stored. Taking into account all these limitations and implementation requirements, in this thesis, several neural network structures with different number of synaptic weights and trained with various training algorithms, are studied. The modeling capability of the neural networks is addressed, which is translated in the text-to-phoneme mapping case into the phoneme accuracy. Different neural network structures, training algorithms and network complexities are analyzed also from this point of view. As a remark here, we mention that input letter encoding plays a very important role in the phoneme accuracy of the grapheme-to-phoneme conversion system. This is why special attention has been paid to the comparative analysis of the performances (in terms of phoneme accuracy) obtained with several orthogonal and non-orthogonal encoding of the input letters. The thesis is structured into four main parts. Chapter 1 brings the reader into the world of text-to-phoneme mapping. In Chapter 2 several different neural network structures and their corresponding training algorithms are described and two new training algorithms are introduced and analyzed. In Chapter 3 the experimental results, for the problem of monolingual text-to-phoneme mapping, obtained with the neural networks described in Chapter 2 are shown. Chapter 4 is dedicated to the problem of bilingual grapheme-to-phoneme conversion and Chapter 5 concludes the thesis.",
    title = "{T}ext-{T}o-{P}honeme {M}apping {U}sing {N}eural {N}etworks",
    school = "Tampere University of Technology",
    url = "https://cris.tuni.fi/ws/portalfiles/portal/2313141/bilcub.pdf",
    year = "2008"
}

@article{Pirinen2008_SJ,
    author = "Pirinen, Tuomo",
    doi = "10.1109/JSEN.2008.2007677",
    issn = "1530-437X",
    journal = "IEEE Sensors Journal",
    number = "12",
    pages = "2008--2015",
    publisher = "IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC",
    title = "{A} confidence statistic and an outlier detector for difference estimates in sensor arrays",
    volume = "8",
    year = "2008"
}

@mastersthesis{Eronen2001_master,
    author = "Eronen, Antti",
    abstract = "This thesis concerns the automatic recognition of musical instruments, where the idea is to build computer systems that “listen” to musical sounds and recognize which instrument is playing. Experimental material consisted of 5286 single notes from Western orchestral instruments, the timbre of which have been studied in great depth. The literature review part of this thesis introduces the studies on the sound of musical instruments, as well as related knowledge on instrument acoustics. Together with the state-of-the-art in automatic sound source recognition systems, these form the foundation for the most important part of this thesis: the extraction of perceptually relevant features from acoustic musical signals. Several different feature extraction algorithms were implemented and developed, and used as a front-end for a pattern recognition system. The performance of the system was evaluated in several experiments. Using feature vectors that included cepstral coefﬁcients and features relating to the type of excitation, brightness, modulations, asynchronity and fundamental frequency of tones, an accuracy of 35 \% was obtained on a database including several examples of 29 instruments. The recognition of the family of the instrument between six possible classes was successful in 77 \% of the cases. The performance of the system and the confusions it made were compared to the results reported for human perception. The comparison shows that the performance of the system is worse than that of humans in a similar task (46 \% in individual instrument and 92 \% in instrument family recognition [Martin99]), although it is comparable to the performance of other reported systems. Confusions of the system resemble those of human subjects, indicating that the feature extraction algorithms have managed to capture perceptually relevant information from the acoustic signals.",
    title = "{A}utomatic musical instrument recognition",
    year = "2001",
    school = "Tampere University of Technology"
}

@phdthesis{Eronen2009_phd,
    author = "Eronen, Antti",
    abstract = "Signal processing methods for audio classification and music content analysis are developed in this thesis. Audio classification is here understood as the process of assigning a discrete category label to an unknown recording. Two specific problems of audio classification are considered: musical instrument recognition and context recognition. In the former, the system classifies an audio recording according to the instrument, e.g. violin, flute, piano, that produced the sound. The latter task is about classifying an environment, such a car, restaurant, or library, based on its ambient audio background. In the field of music content analysis, methods are presented for music meter analysis and chorus detection. Meter analysis methods consider the estimation of the regular pattern of strong and weak beats in a piece of music. The goal of chorus detection is to locate the chorus segment in music which is often the catchiest and most memorable part of a song. These are among the most important and readily commercially applicable content attributes that can be automatically analyzed from music signals. For audio classification, several features and classification methods are proposed and evaluated. In musical instrument recognition, we consider methods to improve the performance of a baseline audio classification system that uses mel-frequency cepstral coefficients and their first derivatives as features, and continuous-density hidden Markov models (HMMs) for modeling the feature distributions. Two improvements are proposed to increase the performance of this baseline system. First, transforming the features to a base with maximal statistical independence using independent component analysis. Secondly, discriminative training is shown to further improve the recognition accuracy of the system. For musical meter analysis, three methods are proposed. The first performs meter analysis jointly at three different time scales: at the temporally atomic tatum pulse level, at the tactus pulse level, which corresponds to the tempo of a piece, and at the musical measure level. The features obtained from an accent feature analyzer and a bank of combfilter resonators are processed by a novel probabilistic model which represents primitive musical knowledge and performs joint estimation of the tatum, tactus, and measure pulses. The second method focuses on estimating the beat and the tatum. The design goal was to keep the method computationally very efficient while retaining sufficient analysis accuracy. Simplified probabilistic modeling is proposed for beat and tatum period and phase estimation, and ensuring the continuity of the estimates. A novel phase-estimator based on adaptive comb filtering is presented. The accuracy of the method is close to the first method but with a fraction of the computational cost. The third method for music rhythm analysis focuses on improving the accuracy in music tempo estimation. The method is based on estimating the tempo of periodicity vectors using locally weighted k-Nearest Neighbors (k-NN) regression. Regression closely relates to classification, the difference being that the goal of regression is to estimate the value of a continuous variable (the tempo), whereas in classification the value to be assigned is a discrete category label. We propose a resampling step applied to an unknown periodicity vector before finding the nearest neighbors to increase the likelihood of finding a good match from the training set. This step improves the performance of the method significantly. The tempo estimate is computed as a distance-weighted median of the nearest neighbor tempi. Experimental results show that the proposed method provides significantly better tempo estimation accuracies than three reference methods. Finally, we describe a computationally efficient method for detecting a chorus section in popular and rock music. The method utilizes a self-dissimilarity representation that is obtained by summing two separate distance matrices calculated using the mel-frequency cepstral coefficient and pitch chroma features. This is followed by the detection of off-diagonal segments of small distance in the distance matrix. From the detected segments, an initial chorus section is selected using a scoring mechanism utilizing several heuristics, and subjected to further processing.",
    title = "{S}ignal {P}rocessing {M}ethods for {A}udio {C}lassification and {M}usic {C}ontent {A}nalysis",
    url = "https://tutcris.tut.fi/portal/files/2308383/eronen.pdf",
    year = "2009",
    school = "Tampere University of Technology"
}

@mastersthesis{Seppänen2001_master,
    author = {Sepp{\"a}nen, Jarno},
    title = "Computational Models of Musical Meter Recognition",
    year = "2001",
    school = "Tampere University of Technology"
}

@inproceedings{Bilcu2002_CSCC,
    author = {Bilcu, Enik{\"o} Beatrice and Suontausta, Janne and Saarinen, Jukka},
    booktitle = "Proceedings of the 6th WSEAS International Multiconference on Circuits, Systems, Communications and Computers, CSCC 2002, July 7-14, 2002, Grete, Greece",
    pages = "4591--4596",
    title = "A New Transform Domain Neural Network for Text-To-Phoneme Mapping",
    year = "2002"
}

@inproceedings{Green2019_WASPAA,
    author = "Green, {Marc C.} and Adavanne, Sharath and Murphy, Damian and Virtanen, Tuomas",
    abstract = "This paper investigates the potential of using higher-order Ambisonic features to perform acoustic scene classification. We compare the performance of systems trained using first-order and fourth-order spatial features extracted from the EigenScape database. Using both Gaussian mixture model and convolutional neural network classifiers, we show that features extracted from higher-order Ambisonics can yield increased classification accuracies relative to first-order features. Diffuseness-based features seem to describe scenes particularly well relative to direction-of-arrival based features. With specific feature subsets, however, differences in classification accuracy between first and fourth-order features become negligible.",
    booktitle = "2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)",
    doi = "10.1109/WASPAA.2019.8937282",
    isbn = "978-1-7281-1124-7",
    keywords = "acoustic scene classification; ambisonics; spatial audio; convolutional neural networks; gaussian mixture models",
    month = "10",
    pages = "328--332",
    publisher = "IEEE",
    series = "IEEE Workshop on Applications of Signal Processing to Audio and Acoustics",
    title = "Acoustic Scene Classification Using Higher-Order Ambisonic Features",
    year = "2019"
}

@inproceedings{Peltonen2001_AES,
    author = "Peltonen, Vesa and Eronen, Antti and Parviainen, Mikko and Klapuri, Anssi",
    address = "Amsterdam, Netherlands",
    booktitle = "110th Audio Engineering Society Convention",
    keywords = "context recognition",
    title = "Recognition of everyday auditory scenes: potentials, latencies and cues",
    year = "2001"
}

@mastersthesis{Peltonen2001_master,
    author = "Peltonen, Vesa",
    abstract = "An acoustic environment surrounding a listener, an auditory scene, can provide contextual cues that enable the recognition of the scene. This thesis concerns the problem of computational auditory scene recognition, which is a subproblem of computational auditory scene analysis. Computational auditory scene analysis refers to the computational analysis of an acoustic environment, and the recognition of distinct sound events in it. In this study, the focus is not in analyzing and recognizing discrete sound events (although they may be used in the recognition process), but in the classiﬁcation of acoustic environments as whole. This thesis covers all the different phases of a study that was made at the Signal Processing Laboratory of Tampere University of Technology: a literature review on auditory scene recognition and related ﬁelds of research, acoustic measurements that were made in a number of everyday auditory environments, design and implementation of the audio database access software, a listening test examining human abilities in auditory scene recognition, audio signal classiﬁcation theory, algorithm development and simulations. The core of this thesis is in the computational audio classiﬁcation and signal processing algorithm development part. Auditory scene recognition involves correct grouping of similar environments, feature selection and extraction, and the use of a suitable classiﬁcation algorithm. A crucial step in solving the problem is to determine appropriate features that can discriminatebetween the acoustic data associated with pre-deﬁned scene classes. The conducted listening tests show that, on average, humans are able to recognize 25 different scenes with 70 \% accuracy. The scenes included everyday outside and inside environments,such as streets, market places, restaurants, and family homes. The performance of the computational classiﬁcation methods was investigated by conducting Matlab simulations. The bestobtained recognition rate for 13 different scenes was 56\%, where the classiﬁed scenes wereselected so that from each one there were at least three recordings from different locations. We also did an experiment of recognizing more general classes (meta-classes), and for certain categorizations of the scenes we obtained relatively good classiﬁcation results. For example, themeta-class car vs. other was classiﬁed correctly in 95\% of the cases.",
    title = "{C}omputational {A}uditory {S}cene {R}ecognition",
    year = "2001",
    school = "Tampere University of Technology"
}

@article{Baby2015_TASLP,
    author = "Baby, Deepak and Virtanen, Tuomas and Gemmeke, Jort and Hamme, Hugo Van",
    abstract = "Exemplar-based speech enhancement systems work by decomposing the noisy speech as a weighted sum of speech and noise exemplars stored in a dictionary and use the resulting speech and noise estimates to obtain a time-varying filter in the full-resolution frequency domain to enhance the noisy speech. To obtain the decomposition, exemplars sampled in lower dimensional spaces are preferred over the full-resolution frequency domain for their reduced computational complexity and the ability to better generalize to unseen cases. But the resulting filter may be sub-optimal as the mapping of the obtained speech and noise estimates to the full-resolution frequency domain yields a low-rank approximation. This paper proposes an efficient way to directly compute the full-resolution frequency estimates of speech and noise using coupled dictionaries: an input dictionary containing atoms from the desired exemplar space to obtain the decomposition and a coupled output dictionary containing exemplars from the full-resolution frequency domain. We also introduce modulation spectrogram features for the exemplar-based tasks using this approach. The proposed system was evaluated for various choices of input exemplars and yielded improved speech enhancement performances on the AURORA-2 and AURORA-4 databases. We further show that the proposed approach also results in improved word error rates (WERs) for the speech recognition tasks using HMM-GMM and deep-neural network (DNN) based systems.",
    doi = "10.1109/TASLP.2015.2450491",
    issn = "2329-9290",
    journal = "IEEE-ACM Transactions on Audio Speech and Language Processing",
    keywords = "Exemplar-based;Modulation envelope;Noise robust automatic speech recognition;Non-negative sparse coding",
    month = "11",
    number = "11",
    pages = "1788--1799",
    publisher = "IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC",
    title = "Coupled dictionaries for exemplar-based speech enhancement and automatic speech recognition",
    volume = "23",
    year = "2015",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/dbaby\_aslp2015.pdf"
}

@mastersthesis{Rosti1999_master,
    author = "Rosti, Antti-Veikko",
    abstract = "The interest in modulation classification has recently emerged in the research of communication systems. This has been due to the advances in reconfigurable signal processing systems, especially in the study of software radio. Published methods can be divided into two groups: maximum likelihood and pattern recognition approaches. In the maximum likelihood approach, decision rules are often simple but the test statistics are complicated and assume prior knowledge about the signals. In the pattern recognition approach, decision rules are often complicated whereas the features are simple and fast to calculate. Communication signals contain a vast amount of uncertainty due to the unknown modulating signal, modulation type, and noise. Therefore the modulation classification problem has to be approached by using statistical methods. The features and the test statistics may be derived from the known statistical characteristics of the modulated signals. Either implicit or explicit use of higher-order statistics has been studied previously in many communication applications. The higher-order statistics are often more preferable because second-order statistics suppress the phase information of the signal. Nevertheless, the estimation of the higher-order statistics requires long sample sets and has a high computational complexity. To overcome these problems, second-order cyclostationary statistics have been studied and the results seem promising. In this thesis, the feature extraction problem of the modulation classification is discussed. Useful characteristics and representations of the communication signals are presented as well as the relevant knowledge of statistical signal processing. The previous methods are presented in a literature review of the modulation classification. The first and second-order statistics including the cyclostationary statistics of digital modulated signals are studied, and a novel feature is proposed. Some previous methods and this novel feature are compared by investigating their discrimination performance in Matlab simulations.",
    title = "{S}tatistical {M}ethods in {M}odulation {C}lassification",
    year = "1999",
    school = "Tampere University of Technology"
}

@ARTICLE{Eronen2006_TASLP,
    author = {Eronen, Antti and Peltonen, Vesa and Tuomi, Juha and Klapuri, Anssi and Fagerlund, Seppo and Sorsa, Timo and Lorho, Ga{\"e}tan and Huopaniemi, Jyri},
    journal = "IEEE Transactions on Audio, Speech, and Language Processing",
    title = "Audio-based context recognition",
    year = "2006",
    volume = "14",
    number = "1",
    pages = "321-329",
    keywords = "Humans;System testing;Hidden Markov models;Acoustic devices;Context awareness;Mobile handsets;Acoustic signal processing;Computational complexity;Vectors;Feature extraction;Audio classification;context awareness;feature extraction;hidden Markov models (HMMs)",
    doi = "10.1109/TSA.2005.854103"
}

@mastersthesis{Saarelainen1999_master,
    author = "Saarelainen, Teemu",
    title = "{B}lind {MIMO} {D}econvolution in {A}coustic {A}pplications",
    year = "1999",
    school = "Tampere University of Technology"
}

@INPROCEEDINGS{Paulus2006_ICASSP,
    author = "Paulus, J.",
    booktitle = "2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings",
    title = "Acoustic Modelling of Drum Sounds with Hidden Markov Models for Music Transcription",
    year = "2006",
    volume = "5",
    number = "",
    pages = "V-V",
    keywords = "Hidden Markov models;Multiple signal classification;Music;Instruments;Pattern recognition;Source separation;Signal analysis;Acoustic signal processing;Acoustic signal detection;Taxonomy",
    doi = "10.1109/ICASSP.2006.1661257"
}

@mastersthesis{Pasanen2002_master,
    author = "Pasanen, Antti",
    abstract = "In this thesis, Voice Activity Detection (VAD) algorithms are integrated in an ASR system. VAD is assumed to give additional information to the ASR system about the presence of speech, thus increasing the robustness of the ASR system. Two standard VAD algorithms (G.729b and GSM) are described and a statistical Gaussian Mixture Model (GMM) based VAD is introduced. For the GMM based VAD, different adaptation techniques are employed to track the changing background noise statistics. The VAD algorithms are integrated with the ASR system, with explicit and implicit approaches. The explicit approach means that the VAD is a separate module in the front end of the ASR system, while in the implicit approach the VAD decision is included in the decoding stage of the speech recognition unit. The performance of the VAD algorithms are compared directly using frame classification rates and indirectly using recognition rates. Recognition is performed as a small vocabulary isolated word recognition task with a Hidden Model based ASR system using normalized Mel-frequency cepstral coefficients. According to our simulations, ideal information about the word boundaries increases significantly the recognition accuracy of the ASR system. However, the described VAD algorithms were not able to increase the recognition accuracy significantly.",
    title = "{V}oice {A}ctivity {D}etection in {N}oise {R}obust {S}peech {R}ecognition",
    year = "2002",
    school = "Tampere University of Technology"
}

@mastersthesis{Pirinen2002_master,
    author = "Pirinen, Tuomo",
    title = "{R}eliability evaluation of time delay based direction of arrival estimate",
    year = "2002",
    school = "Tampere University of Technology"
}

@inproceedings{Mesaros2013_ICASSP,
    author = {Mesaros, Annamaria and Heittola, Toni and Palom{\"a}ki, Kalle},
    abstract = "A common problem of freely annotated or user contributed audio databases is the high variability of the labels, related to homonyms, synonyms, plurals, etc. Automatically re-labeling audio data based on audio similarity could offer a solution to this problem. This paper studies the relationship between audio and labels in a sound event database, by evaluating semantic similarity of labels of acoustically similar sound event instances. The assumption behind the study is that acoustically similar events are annotated with semantically similar labels. Indeed, for 43\% of the tested data, there was at least one in ten acoustically nearest neighbors having a synonym as label, while the closest related term is on average one level higher or lower in the semantic hierarchy.",
    booktitle = "Proceedings of the 2013 IEEE International Conference on Acoustics, Speech and Signal Processing",
    doi = "http://dx.doi.org/10.1109/ICASSP.2013.6637761",
    isbn = "978-1-4799-0356-6",
    keywords = "audio similarity;semantic similarity;sound events",
    pages = "813-817",
    publisher = "IEEE Computer Society",
    title = "Analysis of acoustic-semantic relationship for diversely annotated real-world audio data",
    year = "2013"
}

@article{Popa2011_JSIP,
    author = "Popa, Victor and Nurminen, Jani and Gabbouj, Moncef",
    doi = "10.4236/jsip.2011.22017",
    issn = "2159-4465",
    journal = "Journal of Signal and Information Processing",
    number = "2",
    pages = "125--139",
    publisher = "Scientific Research Publishing",
    title = "{A} {S}tudy of {B}ilinear {M}odels in {V}oice {C}onversion",
    volume = "2",
    year = "2011"
}

@inproceedings{Niemistö2003_IWAENC,
    author = {Niemist{\"o}, Riitta and M{\"a}kel{\"a}, Tuomo},
    editor = "Makino, S. and Miyoshi, M.",
    booktitle = "Proceedings of the Eight International Workshop on Acoustic Echo and Noise Control, IWAENC 2003, Kyoto, Japan, 8-11 September 2003",
    pages = "79--82",
    title = "{O}n performance of linear adaptive filtering algorithms in acoustic echo control in presence of distorting loudspeakers",
    year = "2003"
}

@inproceedings{Herrera-Boyer2006_SPMMT,
    author = "Herrera-Boyer, P. and Klapuri, Anssi and Davy, Manuel",
    editor = "Klapuri, A. and Davy, M.",
    booktitle = "Signal Processing Methods for Music Transcription",
    doi = "10.1007/0-387-32845-9\_6",
    isbn = "978-0-387-30667-4",
    pages = "163--200",
    publisher = "Springer",
    title = "{A}utomatic classification of pitched musical instrument sounds",
    year = "2006"
}

@mastersthesis{Kuja-Halkola2002_master,
    author = "Kuja-Halkola, Sami",
    abstract = "This thesis concerns the problem of automatic recognition of a person based on his or her voice. The main objective is on the text-independent speaker identification task. A few different feature extraction algorithms are presented and evaluated using the KING and POLYCOST speech corpora. The principal feature used are the mel-frequency cepstral coefficients, but also other feature sets are considered including the spectral slope and the fundamental frequency.The work concentrates mostly on different classification algorithms used in speaker recognition. The biggest attention is on the Gaussian mixture speaker models (GMM) which have been widely used in many text-independent speaker recognition studies. Conventionally, the GMM parameters are trained with the well-known expectation maximization (EM) algorithm. An identification accuracy of 95.8\% is achieved using this method for a population of 25 speakers with 60 seconds of training speech and a test sequence of 5 seconds.A drawback of the conventional EM training is the need for selecting the number of the mixture components, i.e. the model order, before the actual training procedure. Moreover, the number of components is usually the same for each speaker. The most important part of this thesis concerns simultaneous order selection and parameter training of GMMs. We evaluate a couple of recently proposed algorithms capable of selecting the order of each speaker model individually during a single training procedure. The algorithms provide a straightforward way of adjusting the number of components of each GMM to the acoustic characteristics and amount of available training data for each speaker. The methods are based on integrating some model complexity criterion, such as the minimum descripition length, into the EM training process. It is observed that when the amount of available training data varies between speakers, a relative reduction of 13\% in error rate is obtained using an algorithm proposed by Figueiredo and Jain [Figueiredo02]. If the speech samples are recorded over a telephone line, a reduction of 11\% in error rate is observed using the agglomerative EM algorithm [Figueiredo99].",
    title = "{T}ext-independent speaker identification",
    year = "2002",
    school = "Tampere University of Technology"
}

@mastersthesis{Viitaniemi2003_master,
    author = "Viitaniemi, Timo",
    abstract = "The thesis proposed a method for the automatic transcription of single-voice melodies from an acoustic waveform into a symbolic musical notation. The system consisted of a signal processing front-end which calculated a continuous pitch track and of a probabilistic model which converted the pitch track into a discrete musical notation. The proposed probabilistic model consisted of three parts operating in parallel: a pitch trajectory model, a musicological model, and a duration model. The first handled imperfections in the performed/estimated pitch values using a hidden Markov model, the second estimated the musical key signature to improve the transcription accuracy ant the last models the duration of the notes. The thesis covered a literature review on human voice production and on the theory of pitch estimation. In addition, an inspection of an acoustic database recorded for the training and the testing of the proposed model was introduced. The most important part of the thesis is the chapter with the three probabilistic models.",
    title = "{P}robabilistic {M}odels for the {T}ranscription of {S}ingle-{V}oice {M}elodies",
    year = "2003",
    school = "Tampere University of Technology"
}

@inproceedings{Pirinen2004_SAM,
    author = "Pirinen, Tuomo and Yli-Hietanen, Jari",
    booktitle = "Proceedings of 2004 IEEE Sensor Array and Multichannel Signal Processing Workshop, SAM 2004, Barcelona, Spain, 18-21 July 2004",
    pages = "5 p",
    title = "{T}ime delay based failure-robust direction of arrival estimation",
    year = "2004"
}

@inproceedings{Cakir2018_AES,
    author = "Cakir, Emre and Virtanen, Tuomas",
    abstract = "In this work we propose a deep learning based method—namely, variational, convolutional recurrent autoencoders (VCRAE)—for musical instrument synthesis. This method utilizes the higher level time-frequency representations extracted by the convolutional and recurrent layers to learn a Gaussian distribution in the training stage, which will be later used to infer unique samples through interpolation of multiple instruments in the usage stage. The reconstruction performance of VCRAE is evaluated by proxy through an instrument classifier and provides significantly better accuracy than two other baseline autoencoder methods. The synthesized samples for the combinations of 15 different instruments are available on the companion website.",
    booktitle = "Proceedings of the Audio Engineering Society 145th Convention",
    publisher = "AES Audio Engineering Society",
    title = "Musical Instrument Synthesis and Morphing in Multidimensional Latent Space Using Variational, Convolutional Recurrent Autoencoders",
    year = "2018",
    url = "https://trepo.tuni.fi/bitstream/handle/10024/129505/AES\_2018\_Musical\_Instrument\_Synthesis\_and\_Morphing\_in\_Multidimensional\_Latent\_Space\_Using\_Variational\_Convolutional\_Recurrent\_Autoencoders.pdf?sequence=1\&isAllowed=y"
}

@mastersthesis{Heittola2004_master,
    author = "Heittola, Toni",
    abstract = "Collections of digital music have become increasingly common over the recent years. As the amount of data increases, digital content management is becoming more important. In this thesis, we are studying content-based classification of acoustic musical signals according to their musical genre (e.g., classical, rock) and the instruments used. A listening experiment is conducted to study human abilities to recognise musical genres. This thesis covers a literature review on human musical genre recognition, state-of-the-art musical genre recognition systems, and related fields of research. In addition, a general-purpose music database consisting of recordings and their manual annotations is introduced. The theory behind the used features and classifiers is reviewed and the results from the simulations are presented. The developed musical genre recognition system uses mel-frequency cepstral coefficients to represent the time-varying magnitude spectrum of a music signal. The class-conditional feature densities are modelled with hidden Markov models. Musical instrument detection for a few pitched instruments from music signals is also studied using the same structure. Furthermore, this thesis proposes a method for the detection of drum instruments. The presence of drums is determined based on the periodicity of the amplitude envelopes of the signal at subbands. The conducted listening experiment shows that the recognition of musical genres is not a trivial task even for humans. On the average, humans are able to recognise the correct genre in 75 \% of cases (given five-second samples). Results also indicate that humans can do rather accurate musical genre recognition without long-term temporal features, such as rhythm. For the developed automatic recognition system, the obtained recognition accuracy for six musical genres was around 60 \%, which is comparable to the state-of-the-art systems. Detection accuracy of 81 \% was obtained with the proposed drum instrument detection method.",
    keywords = "Musical genre classification",
    title = "{A}utomatic {C}lassification of {M}usic {S}ignals",
    year = "2004",
    school = "Tampere University of Technology"
}

@inproceedings{Heittola2009_ISMIR 2009,
    author = "Heittola, Toni and Klapuri, Anssi and Virtanen, Tuomas",
    abstract = "This paper proposes a novel approach to musical instrument recognition in polyphonic audio signals by using a source-filter model and an augmented non-negative matrix factorization algorithm for sound separation. The mixture signal is decomposed into a sum of spectral bases modeled as a product of excitations and filters. The excitations are restricted to harmonic spectra and their fundamental frequencies are estimated in advance using a multipitch estimator, whereas the filters are restricted to have smooth frequency responses by modeling them as a sum of elementary functions on the Mel-frequency scale. The pitch and timbre information are used in organizing individual notes into sound sources. In the recognition, Mel-frequency cepstral coefficients are used to represent the coarse shape of the power spectrum of sound sources and Gaussian mixture models are used to model instrument-conditional densities of the extracted features. The method is evaluated with polyphonic signals, randomly generated from 19 instrument classes. The recognition rate for signals having six note polyphony reaches 59\%.",
    address = "Kobe, Japan",
    booktitle = "in Proc. 10th Int. Society for Music Information Retrieval Conf. (ISMIR 2009)",
    keywords = "instruments;separation;source-filter model",
    organization = "International Society for Music Information Retrieval (ISMIR)",
    pages = "327-332",
    title = "Musical Instrument Recognition in Polyphonic Audio Using Source-Filter Model for Sound Separation",
    year = "2009",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/ismir09-heittola.pdf"
}

@article{Heittola2013_JASM,
    author = "Heittola, Toni and Mesaros, Annamaria and Eronen, Antti and Virtanen, Tuomas",
    abstract = "The work presented in this article studies how the context information can be used in the automatic sound event detection process, and how the detection system can benefit from such information. Humans are using context information to make more accurate predictions about the sound events and ruling out unlikely events given the context. We propose a similar utilization of context information in the automatic sound event detection process. The proposed approach is composed of two stages: automatic context recognition stage and sound event detection stage. Contexts are modeled using Gaussian mixture models and sound events are modeled using three-state left-to-right hidden Markov models. In the first stage, audio context of the tested signal is recognized. Based on the recognized context, a context-specific set of sound event classes is selected for the sound event detection stage. The event detection stage also uses context-dependent acoustic models and count-based event priors. Two alternative event detection approaches are studied. In the first one, a monophonic event sequence is outputted by detecting the most prominent sound event at each time instance using Viterbi decoding. The second approach introduces a new method for producing polyphonic event sequence by detecting multiple overlapping sound events using multiple restricted Viterbi passes. A new metric is introduced to evaluate the sound event detection performance with various level of polyphony. This combines the detection accuracy and coarse time-resolution error into one metric, making the comparison of the performance of detection algorithms simpler. The two-step approach was found to improve the results substantially compared to the context-independent baseline system. In the block-level, the detection accuracy can be almost doubled by using the proposed context-dependent event detection.",
    journal = "EURASIP Journal on Audio, Speech and Music Processing",
    keywords = "CASA;Sound event detection",
    title = "Context-Dependent Sound Event Detection",
    doi = "10.1186/1687-4722-2013-1",
    year = "2013"
}

@inproceedings{Heittola2013_ICASSP,
    author = "Heittola, Toni and Mesaros, Annamaria and Virtanen, Tuomas and Gabbouj, Moncef",
    abstract = "Sound event detection is addressed in the presence of overlapping sounds. Unsupervised sound source separation into streams is used as a preprocessing step to minimize the interference of overlapping events. This poses a problem in supervised model training, since there is no knowledge about which separated stream contains the targeted sound source. We propose two iterative approaches based on EM algorithm to select the most likely stream to contain the target sound: one by selecting always the most likely stream and another one by gradually eliminating the most unlikely streams from the training. The approaches were evaluated with a database containing recordings from various contexts, against the baseline system trained without applying stream selection. Both proposed approaches were found to give a reasonable increase of 8 percentage units in the detection accuracy.",
    address = "Vancouver, Canada",
    booktitle = "2013 IEEE International Conference on Acoustics, Speech and Signal Processing",
    pages = "8677-8681",
    keywords = "sound event detection;casa",
    publisher = "IEEE Computer Society",
    title = "Supervised Model Training for Overlapping Sound Events Based on Unsupervised Source Separation",
    year = "2013",
    doi = "10.1109/ICASSP.2013.6639360",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/icassp2013\_heittola.pdf"
}

@article{Heittola2014_JASM,
    author = "Heittola, Toni and Mesaros, Annamaria and Korpi, Dani and Eronen, Antti and Virtanen, Tuomas",
    abstract = "An approach is proposed for creating location-specific audio textures for virtual location-exploration services. The presented approach creates audio textures by processing a small amount of audio recorded at a given location, providing a cost-effective way to produce a versatile audio signal that characterizes the location. The resulting texture is non-repetitive and conserves the location-specific characteristics of the audio scene, without the need of collecting large amount of audio from each location. The method consists of two stages: analysis and synthesis. In the analysis stage, the source audio recording is segmented into homogeneous segments. In the synthesis stage, the audio texture is created by randomly drawing segments from the source audio so that the consecutive segments will have timbral similarity near the segment boundaries. Results obtained in listening experiments show that there is no statistically significant difference in the audio quality or location-specificity of audio when the created audio textures are compared to excerpts of the original recordings. Therefore, the proposed audio textures could be utilized in virtual location-exploration services. Examples of source signals and audio textures created from them are available at www.cs.tut.fi/\textasciitilde heittolt/audiotexture.",
    journal = "EURASIP Journal on Audio, Speech and Music Processing",
    number = "9",
    title = "Method for creating location-specific audio textures",
    volume = "2014",
    year = "2014",
    doi = "10.1186/1687-4722-2014-9"
}

@mastersthesis{Tervo2006_master,
    author = "Tervo, Sakari",
    title = "{A}ikaeron estimointimenetelmien suorituskyky reaalitilanteessa",
    year = "2006",
    school = "Tampere University of Technology"
}

@inproceedings{Diment2017_WASPAA,
    author = "Diment, Aleksandr and Virtanen, Tuomas",
    booktitle = "IEEE Workshop on Applications of Signal Processing to Audio and Acoustics",
    doi = "10.1109/WASPAA.2017.8169984",
    isbn = "978-1-5386-1631-4",
    pages = "6--10",
    series = "IEEE Workshop on Applications of Signal Processing to Audio and Acoustics",
    title = "Transfer Learning of Weakly Labelled Audio",
    year = "2017",
    url = "http://diment.kapsi.fi/papers/Diment17\_TL.pdf"
}

@phdthesis{Niemistö2003_phd,
    author = {Niemist{\"o}, Riitta},
    abstract = "This thesis proposes algorithms to be utilized for iterative design of linear filters and for adaptive polynomial filtering. The goal is to improve existing algorithms in terms of filtering performance, time and complexity requirements. First, a family of fast algorithms suitable for adaptive polynomial filtering, namely LS-LMS algorithm and its square root variants, is introduced and its connection with the family of RLS algorithms is analyzed. The algorithms are derived by combining the triangularization and backsolving steps of QR-RLS algorithm. In the derivation we allow a modification of the cost function in order to decrease the complexity of the algorithm from O(M^2) to O(M), where M is the number of filter parameters. Second, adaptive filtering algorithms are proposed for the problem of acoustic echo control in the case of strong acoustic distortion in the loudspeaker that decreases the echo attenuation obtained using adaptive linear filtering algorithms in echo cancelers. Several existing algorithms were tested using both linear and homogeneous Volterra filters, but their performance was not found satisfactory either in terms of performance or complexity. A polynomial preprocessor followed by a linear filter was found to perform well when the preprocessor was adapted by the robust, but computationally demanding, QR-RLS algorithm and the linear part was adapted by the normalized LMS algorithm. Third, the problem of optimum energy compaction for FIR and IIR filters is addressed. The analytical method for optimum compaction FIR filter design is analyzed and shown to provide a very fast algorithm under certain conditions. For the design of optimum compaction IIR filters two new methods are proposed. The first design method operates with the numerator and the denominator in the causal part of the associated product filter, which are in turn optimized via iterative relaxations. The second method is similar, except that the angles of the poles are set to fixed values, situated at the transitions in the frequency representation of the corresponding ideal brickwall filter. Thus, the second method is faster than the first, and, moreover, it was observed that when the ideal brickwall filter does not have too many transitions, the second method provides filters with better performance. Both methods make use of semidefinite programming optimization using an appropriate parameterization of positive real polynomials. The last part of the thesis provides contributions to the frequency selective IIR filter design. The frequency response of the filter is fitted to an ideal frequency response by optimizing either a weighted sum of magnitude squares (least squares) criterion or a minimax (Chebyshev) criterion. For the least squares criterion a new convex stability domain is used and experimental evidence is given that the best designs are usually obtained with a multistage algorithm where three methods, Steiglitz-McBride, Gauss-Newton and classical descent, are used consecutively in that order. The result of multistage least squares IIR design is used to initialize the optimization procedure based on a Chebyshev criterion, for which a simplified iterative procedure is proposed. In the simplified procedure only the numerator is updated after initialization. The simplified procedure is compared experimentally with the complete procedure and the results show only very small differences in the criterion.",
    title = "{F}ast {A}lgorithms for {I}terative {D}esign of {L}inear {F}ilters and {A}daptive {P}olynomial {F}iltering",
    year = "2003",
    school = "Tampere University of Technology"
}

@mastersthesis{Tuomi2004_master,
    author = "Tuomi, Juha",
    title = "{A}udio-{B}ased {C}ontext {T}racking",
    year = "2004",
    school = "Tampere University of Technology"
}

@inproceedings{Mahkonen2013_DAFx,
    author = {Mahkonen, Katariina and Eronen, Antti and Virtanen, Tuomas and Helander, Elina and Popa, Victor and Lepp{\"a}nen, Jussi and Curcio, Igor},
    booktitle = "16th International Conference on Digital Audio Effects, Ireland, 2-5.9,2013",
    publisher = "International Conference on Digital Audio Effects",
    series = "International Conference on Digital Audio Effects",
    title = "Music Dereverberation by Spectral Linear Prediction in Live Recordings",
    year = "2013",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/Mahkonen\_DAFX2013.pdf"
}

@inbook{Nikunen2017,
    author = "Nikunen, Joonas and Virtanen, Tuomas",
    abstract = "This chapter introduces methods for factorizing the spectrogram of multichannel audio into repetitive spectral objects and apply the introduced models to the analysis of spatial audio and modification of spatial sound through source separation. The purpose of decomposing an audio spectrogram using spectral templates is to learn the underlying structures (audio objects) from the observed data. The chapter discusses two main scenarios such as parameterization of multichannel surround sound and parameterization of microphone array signals. It explains the principles of source separation by time-frequency filtering using separation masks constructed from the spectrogram models. The chapter introduces a spatial covariance matrix model based on the directions of arrival of sound events and spectral templates, and discusses its relationship to conventional spatial audio signal processing. Source separation using spectrogram factorization models is achieved via time- frequency filtering of the original observation short-time Fourier transform (STFT) by a generalized Wiener filter obtained from the spectrogram model parameters.",
    booktitle = "Parametric time-frequency-domain spatial audio",
    doi = "10.1002/9781119252634.ch9",
    editor2 = "Ville Pulkki and Symeon Delikaris-Manias and Archontis Politis",
    isbn = "978-1-119-25259-7",
    month = "10",
    pages = "215--250",
    publisher = "John Wiley {\\&} Sons",
    title = "Source Separation and Reconstruction of Spatial Audio Using Spectrogram Factorization",
    year = "2017"
}

@inproceedings{Mesaros2017_WASPAA,
    author = "Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas",
    abstract = "Human and machine performance in acoustic scene classification is examined through a parallel experiment using TUT Acoustic Scenes 2016 dataset. The machine learning perspective is presented based on the systems submitted for the 2016 challenge on Detection and Classification of Acoustic Scenes and Events. The human performance, assessed through a listening experiment, was found to be significantly lower than machine performance. Test subjects exhibited different behavior throughout the experiment, leading to significant differences in performance between groups of subjects. An expert listener trained for the task obtained similar accuracy to the average of submitted systems, comparable also to previous studies of human abilities in recognizing everyday acoustic scenes.",
    address = "United States",
    booktitle = "2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)",
    doi = "10.1109/WASPAA.2017.8170047",
    isbn = "978-1-5386-1631-4",
    keywords = "acoustic scene classification; machine learning; human performance; listening experiment",
    pages = "319–323",
    publisher = "IEEE Computer Society",
    title = "Assessment of human and machine performance in acoustic scene classification: {DCASE} 2016 case study",
    year = "2017",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/mesaros-waspaa2017-humans-vs-machines-asc.pdf"
}

@mastersthesis{Tsoumanis2002_master,
    author = "Tsoumanis, Andreas",
    title = "{F}ace {F}eature {T}racking {U}sing {G}abor {W}avelets",
    year = "2002",
    school = "Tampere University of Technology"
}

@inproceedings{Lipping2019_DCASE2019,
    author = "Lipping, Samuel and Drossos, Konstantinos and Virtanen, Tuomas",
    abstract = {Audio captioning is a novel field of multi-modal translation and it is the task of creating a textual description of the content of an audio signal (e.g. {"}people talking in a big room{"}). The creation of a dataset for this task requires a considerable amount of work, rendering the crowdsourcing a very attractive option. In this paper we present a three steps based framework for crowdsourcing an audio captioning dataset, based on concepts and practises followed for the creation of widely used image captioning and machine translations datasets. During the first step initial captions are gathered. A grammatically corrected and/or rephrased version of each initial caption is obtained in second step. Finally, the initial and edited captions are rated, keeping the top ones for the produced dataset. We objectively evaluate the impact of our framework during the process of creating an audio captioning dataset, in terms of diversity and amount of typographical errors in the obtained captions. The obtained results show that the resulting dataset has less typographical errors than the initial captions, and on average each sound in the produced dataset has captions with a Jaccard similarity of 0.24, roughly equivalent to two ten-word captions having in common four words with the same root, indicating that the captions are dissimilar while they still contain some of the same information.},
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019)",
    day = "26",
    keywords = "audio captioning; captioning; amt; crowdsourcing; Amazon Mechanical Turk",
    month = "10",
    title = "Crowdsourcing a Dataset of Audio Captions",
    year = "2019",
    url = "https://arxiv.org/abs/1907.09238"
}

@inproceedings{Hurmalainen2011,
    author = "Hurmalainen, Antti and Virtanen, Tuomas and Gemmeke, Jort and Mahkonen, Katariina",
    booktitle = {Akustiikkap{\"a}iv{\"a}t 2011, Tampere, 11.-12.5.2011, Akustinen Seura ry},
    pages = "1--5",
    publisher = "Akustinen seura",
    series = {Akustiikkap{\"a}iv{\"a}t},
    title = "{E}simerkkipohjainen meluisan puheen automaattinen tunnistus",
    year = "2011"
}

@article{Cakir2017_TASLP,
    author = "Cakir, Emre and Parascandolo, Giambattista and Heittola, Toni and Huttunen, Heikki and Virtanen, Tuomas",
    abstract = "Sound events often occur in unstructured environments where they exhibit wide variations in their frequency content and temporal structure. Convolutional neural networks (CNNs) are able to extract higher level features that are invariant to local spectral and temporal variations. Recurrent neural networks (RNNs) are powerful in learning the longer term temporal context in the audio signals. CNNs and RNNs as classifiers have recently shown improved performances over established methods in various sound recognition tasks. We combine these two approaches in a convolutional recurrent neural network (CRNN) and apply it on a polyphonic sound event detection task. We compare the performance of the proposed CRNN method with CNN, RNN, and other established methods, and observe a considerable improvement for four different datasets consisting of everyday sound events.",
    doi = "10.1109/TASLP.2017.2690575",
    issn = "2329-9290",
    journal = "IEEE-ACM Transactions on Audio Speech and Language Processing",
    keywords = "deep neural networks;sound event detection",
    month = "6",
    number = "6",
    pages = "1291--1303",
    publisher = "IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC",
    title = "Convolutional Recurrent Neural Networks for Polyphonic Sound Event Detection",
    volume = "25",
    year = "2017",
    url = "https://arxiv.org/abs/1702.06286"
}

@inproceedings{Gemmeke2013_in conjuction with ICASSP,
    author = "Gemmeke, Jort and Hurmalainen, Antti and Virtanen, Tuomas",
    booktitle = "The 2nd International Workshop on Machine Listening in Multisource Environments CHiME Workshop, 1st June 2013, Vancouver, Canada (in conjuction with ICASSP)",
    pages = "47--52",
    keywords = "speech enhancement;exemplar-based;noise robustness;Non-Negative Matrix Factorization;Hidden Markov Models",
    publisher = "International Workshop on Machine Listening in Multisource Environments",
    series = "International Workshop on Machine Listening in Multisource Environments",
    title = "{HMM}-regularization for {NMF}-based noise robust {ASR}",
    year = "2013",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/pP5\_gemmeke.pdf"
}

@mastersthesis{Mäkelä2006_master,
    author = {M{\"a}kel{\"a}, Tuomo},
    abstract = "Acoustic echo is an annoying, nevertheless common, phenomenon in the modern communication systems such as cellular telephones or video conference systems.This thesis introduces methods for polynomial-based acoustic echo cancellation which are used in telecommunication systems, how acoustic echo is formed and how to model and reduce it with the means of signal proeessing. The author describes the principles of adaptive filtering, presents the best known adaptive filtering algorithms and introduces some more sophisticated algorithms based on these. The aim is to develop non-linear models for modelling echo path alongside traditional linear models. These non-linear models try to model the non-linear distortion, which is inflicted into the sound by the small and inexpensive audio equipment. The aim is also to consider side-effects of these non-linear models and how these side-effeets eould be avoided.Since these non-linear models are quite sensitive to the input signals, the mechanisms for controlling adaptive filtering algorithms are studied.",
    title = "Polynomial-based acoustic echo cancellation",
    year = "2006",
    school = "Tampere University of Technology"
}

@inproceedings{Shuyang2017_ICASSP,
    author = "Shuyang, Zhao and Heittola, Toni and Virtanen, Tuomas",
    keywords = "active learning;sound event classification;K-medoids clustering",
    title = "Active Learning for Sound Event Classification by Clustering Unlabeled Data",
    url = "https://trepo.tuni.fi/handle/10024/129132",
    booktitle = "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    pages = "751--755",
    year = "2017",
    organization = "IEEE"
}

@inproceedings{Paulus2002_ISMIR,
    author = "Paulus, Jouni and Klapuri, Anssi",
    editor = "Fingerhut, Michael",
    address = "Paris, France",
    booktitle = "Proc. of the Third International Conference on Music Information Retrieval",
    month = "Oct",
    pages = "150--156",
    title = "Measuring the Similarity of Rhythmic Patterns",
    year = "2002"
}

@inproceedings{Paulus2003_ICME,
    author = "Paulus, Jouni and Klapuri, Anssi",
    address = "Baltimore, Maryland, USA",
    booktitle = "Proc. of the IEEE International Conference on Multimedia and Expo",
    month = "Jul",
    pages = "737--740",
    title = "Conventional and Periodic {N}-grams in the Transcription of Drum Sequences",
    volume = "2",
    year = "2003"
}

@inproceedings{Paulus2003_DAFx,
    author = "Paulus, Jouni and Klapuri, Anssi",
    editor = "Davies, Mike",
    address = "London, UK",
    booktitle = "Proc. of the 6th International Conference on Digital Audio Effects",
    month = "Sep",
    pages = "73--77",
    title = "Model-based Event Labeling in the Transcription of Percussive Audio Signals",
    year = "2003"
}

@inproceedings{Paulus2005_EUSIPCO,
    author = "Paulus, Jouni and Virtanen, Tuomas",
    address = "Antalya, Turkey",
    booktitle = "Proc. of the 13th European Signal Processing Conference",
    keywords = "drums; NMF",
    month = "Sep",
    title = "Drum Transcription with Non-negative Spectrogram Factorisation",
    year = "2005",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/eusipco05\_paulus.pdf"
}

@inproceedings{Paulus2005_MIREX,
    author = "Paulus, Jouni",
    address = "London, UK",
    booktitle = "Proc. of the First Annual Music Information Retrieval Evaluation eXchange",
    keywords = "HMM",
    month = "Sep",
    title = "{D}rum {T}ranscription from {P}olyphonic {M}usic with {I}nstrument-wise {H}idden {M}arkov {M}odels",
    year = "2005"
}

@inproceedings{Paulus2006,
    author = "Paulus, Jouni and Klapuri, Anssi",
    address = "Santa Barbara, CA, USA",
    booktitle = "Proc. of the 1st ACM Audio and Music Computing Multimedia Workshop",
    month = "Oct",
    pages = "59--68",
    title = "Music Structure Analysis by Finding Repeated Parts",
    year = "2006"
}

@inproceedings{Paulus2007_ISMIR,
    author = "Paulus, Jouni and Klapuri, Anssi",
    address = "Vienna, Austria",
    booktitle = "Proc. of the 8th International Conference on Music Information Retrieval",
    keywords = "drums; HMM",
    month = "Sep",
    pages = "225--228",
    title = "{C}ombining temporal and spectral features in {HMM}-based drum transcription",
    year = "2007"
}

@inproceedings{Paulus2008_CMMR,
    author = "Paulus, Jouni and Klapuri, Anssi",
    editor = "Jensen, Kristoffer",
    address = "Copenhagen, Denmark",
    booktitle = "Proc. of the 2008 Computers in Music Modeling and Retrieval Conference",
    keywords = "Music structure analysis",
    month = "May",
    pages = "137--147",
    title = "{L}abelling the {S}tructural {P}arts of a {M}usic {P}iece with {M}arkov {M}odels",
    year = "2008"
}

@inproceedings{Paulus2008_DAFx,
    author = "Paulus, Jouni and Klapuri, Anssi",
    address = "Espoo, Finland",
    booktitle = "Proc. of the 11th International Conference on Digital Audio Effects",
    keywords = "Music structure analysis",
    month = "Sep",
    pages = "309--312",
    title = "{A}coustic {F}eatures for {M}usic {P}iece {S}tructure {A}nalysis",
    year = "2008"
}

@inproceedings{Paulus2008_ISMIR,
    author = "Paulus, Jouni and Klapuri, Anssi",
    address = "Philadelphia, PA, USA",
    booktitle = "Proc. of the Ninth International Conference on Music Information Retrieval",
    keywords = "Music structure analysis",
    month = "Sep",
    number = "369-374",
    series = {""},
    title = "{M}usic {S}tructure {A}nalysis {U}sing a {P}robabilistic {F}itness {M}easure {A}nd an {I}ntegrated {M}usicological {M}odel",
    year = "2008"
}

@article{Paulus2009_TASLP,
    author = "Paulus, Jouni and Klapuri, Anssi",
    journal = "IEEE Transactions on Audio, Speech, and Language Processing",
    keywords = "Music structure analysis",
    month = "Aug",
    number = "6",
    pages = "1159--1170",
    title = "{M}usic {S}tructure {A}nalysis {U}sing a {P}robabilistic {F}itness {M}easure and a {G}reedy {S}earch {A}lgorithm",
    volume = "17",
    year = "2009"
}

@incollection{Paulus2009,
    author = "Paulus, Jouni and Klapuri, Anssi",
    editor = "Ystad, S{\o}lvi and Kronland-Martinet, Richard and Jensen, Kristoffer",
    booktitle = "Computer Music Modeling and Retrieval: Genesis of Meaning in Sound and Music - 5th International Symposium, CMMR 2008 Copenhagen, Denmark, May 19-23, 2008, Revised Papers",
    keywords = "Music structure analysis",
    pages = "166--176",
    publisher = "Springer Berlin / Heidelberg",
    title = "{L}abelling the {S}tructural {P}arts of a {M}usic {P}iece with {M}arkov {M}odels",
    year = "2009"
}

@inproceedings{Paulus2009_MIREX,
    author = "Paulus, Jouni and Klapuri, Anssi",
    address = "Kobe, Japan",
    booktitle = "Proc. of the Fifth Annual Music Information Retrieval Evaluation eXchange",
    keywords = "Music structure analysis",
    month = "Oct",
    title = "{M}usic structure analysis with a probabilistic fitness function in {MIREX}2009",
    year = "2009"
}

@phdthesis{Paulus2010_phd,
    author = "Paulus, Jouni",
    abstract = "This thesis proposes signal processing methods for the analysis of musical audio on two time scales: drum transcription on a ﬁner time scale and music structure analysis on the time scale of entire pieces. The former refers to the process of locating drum sounds in the input and recognising the instruments that were used to produce the sounds. The latter refers to the temporal segmentation of a musical piece into parts, such as chorus and verse. For drum transcription, both low-level acoustic recognition and high-level musicological modelling methods are presented. A baseline acoustic recognition method with a large number of features using Gaussian mixture models for the recognition of drum combinations is presented. Since drums occur in structured patterns, modelling of the sequential dependencies with N-grams is proposed. In addition to the conventional N-grams, periodic N-grams are proposed to model the dependencies between events that occur one pattern length apart. The evaluations show that incorporating musicological modelling improves the performance considerably. As some drums are more probable to occur at certain points in a pattern, this dependency is utilised for producing transcriptions of signals produced with arbitrary sounds, such as beatboxing. A supervised source separation method using non-negative matrix factorisation is proposed for transcribing mixtures of drum sounds. Despite the simple signal model, a high performance is obtained for signals without other instruments. Most of the drum transcription methods operate only on single-channel inputs, but multichannel signals are available in recording studios. A multichannel extension of the source separation method is proposed, and an increase in performance is observed in evaluations. Many of the drum transcription methods rely on detecting sound onsets for the segmentation of the signal. Detection errors will then decrease the overall performance of the system. To overcome this problem, a method utilising a network of connected hidden Markov models is proposed to perform the event segmentation and recognition jointly. The system is shown to be able to perform the transcription even from polyphonic music. The second main topic of this thesis is music structure analysis. Two methods are proposed for this purpose. The ﬁrst relies on deﬁning a cost function for a description of the repeated parts. The second method deﬁnes a ﬁtness function for descriptions covering the entire piece. The abstract cost (and ﬁtness) functions are formulated in terms that can be determined from the input signal algorithmically, and optimisation problems are formulated. In both cases, an algorithm is proposed for solving the optimisation problems. The ﬁrst method is evaluated on a small data set, and the relevance of the cost function terms is shown. The latter method is evaluated on three large data sets with a total of 831 (557+174+100) songs. This is to date the largest evaluation of a structure analysis method. The evaluations show that the proposed method outperforms a reference system on two of the data sets. Music structure analysis methods rarely provide musically meaningful names for the parts in the result. A method is proposed to label the parts in descriptions based on a statistical model of the sequential dependencies between musical parts. The method is shown to label the main parts relatively reliably without any additional information. The labelling model is further integrated into the ﬁtness function based structure analysis method.",
    address = "Tampere, Finland",
    month = "Dec",
    school = "Tampere University of Technology",
    title = "{S}ignal {P}rocessing {M}ethods for {D}rum {T}ranscription and {M}usic {S}tructure {A}nalysis",
    year = "2010"
}

@mastersthesis{Rehtonen2002_master,
    author = "Rehtonen, Petri",
    abstract = "This thesis concerns the development and evaluation of signal processing methods that compensate for the hearing disorders caused by ageing. The global ageing increases the number of elderly people with hearing impairment. In most cases this impairment is irrecoverable and cannot be medically or surgically treated. The only option is to process the sound going to the ear, so that it would be as clear and intelligible as possible. The multichannel dynamic compression is such a processing. With the multichannel compression the need for amplified sound and the problems of redused dynamic range can be solved. The compression reduces the loudness fluctuations in the sound so that when sound is amplified, loud sounds do not sound too loud. As a degree of the hearing impairment is more severe in high frequency bands and by processing the bands individually. In this work, I have implemented a three- channel dynamic compression system controllable in real-time. The system was used to conduct listening tests for hearing impaired: The goal was to found a relationship between the audiogram (which represents the hearing threshold level and uncomfortable listening level measured by using sinusoids as stimuli) and the parameters of the multichannel dynamic compression system. In the first listening test, only a week relationship between the two was found. A second test was conducted in order to find out if the adjusted parameter were actually the most intelligible. This was realised using a paired comparison test. In this tests, four different processing were compared. The results shows that the spectral shaping was judged to be the processing which made speech the most intelligible. The compression was disliked and the possible reason for this is that the stimuli were very intelligible and power-normalised already before processing. There was no room for further improvement in intelligibility and thus the distortions caused by compression were perceived unpleasant",
    title = "{C}ompensation of {H}earing {D}efects {U}sing {M}ultichannel {D}ynamic {C}ompression",
    year = "2002",
    school = "Tampere University of Technology"
}

@inproceedings{Silen2013_Interspeech 2013,
    author = "Silen, Hanna and Nurminen, Jani and Helander, Elina and Gabbouj, Moncef",
    booktitle = "Proceedings of the 14th Annual Conference of the International Speech Communication Association (Interspeech 2013), 25-29 August, Lyon, France",
    pages = "373--377",
    publisher = "International Speech Communication Association",
    series = "Interspeech",
    title = "{V}oice {C}onversion for {N}on-{P}arallel {D}atasets {U}sing {D}ynamic {K}ernel {P}artial {L}east {S}quares {R}egression",
    year = "2013"
}

@phdthesis{Nikunen2015_phd,
    author = "Nikunen, Joonas",
    abstract = "This thesis studies several data decomposition algorithms for obtaining an object-based representation of an audio signal. The estimation of the representation parameters are coupled with audio-specific criteria, such as the spectral redundancy, sparsity, perceptual relevance and spatial position of sounds. The objective is to obtain an audio signal representation that is composed of meaningful entities called audio objects that reflect the properties of real-world sound objects and events. The estimation of the object-based model is based on magnitude spectrogram redundancy using non-negative matrix factorization with extensions to multichannel and complex-valued data. The benefits of working with object-based audio representations over the conventional time-frequency bin-wise processing are studied. The two main applications of the object-based audio representations proposed in this thesis are spatial audio coding and sound source separation from multichannel microphone array recordings. In the proposed spatial audio coding algorithm, the audio objects are estimated from the multichannel magnitude spectrogram. The audio objects are used for recovering the content of each original channel from a single downmixed signal, using time-frequency filtering. The perceptual relevance of modeling the audio signal is considered in the estimation of the parameters of the object-based model, and the sparsity of the model is utilized in encoding its parameters. Additionally, a quantization of the model parameters is proposed that reflects the perceptual relevance of each quantized element. The proposed object-based spatial audio coding algorithm is evaluated via listening tests and comparing the overall perceptual quality to conventional time-frequency block-wise methods at the same bitrates. The proposed approach is found to produce comparable coding efficiency while providing additional functionality via the object-based coding domain representation, such as the blind separation of the mixture of sound sources in the encoded channels. For the sound source separation from multichannel audio recorded by a microphone array, a method combining an object-based magnitude model and spatial covariance matrix estimation is considered. A direction of arrival-based model for the spatial covariance matrices of the sound sources is proposed. Unlike the conventional approaches, the estimation of the parameters of the proposed spatial covariance matrix model ensures a spatially coherent solution for the spatial parameterization of the sound sources. The separation quality is measured with objective criteria and the proposed method is shown to improve over the state-of-the-art sound source separation methods, with recordings done using a small microphone array.",
    isbn = "978-952-15-3438-6",
    month = "1",
    publisher = "Tampere University of Technology",
    series = "Tampere University of Technology. Publication",
    title = "{O}bject-based {M}odeling of {A}udio for {C}oding and {S}ource {S}eparation",
    url = "https://tutcris.tut.fi/portal/files/2460357/nikunen\_1276.pdf",
    year = "2015",
    school = "Tampere University of Technology"
}

@inproceedings{Mesaros2017_DCASE2017,
    author = "Mesaros, Annamaria and Heittola, Toni and Diment, Aleksandr and Elizalde, {Benjamin Martinez} and Shah, Ankit and Vincent, Emmanuel and Raj, Bhiksha and Virtanen, Tuomas",
    abstract = "DCASE 2017 Challenge consists of four tasks: acoustic scene classification, detection of rare sound events, sound event detection in real-life audio, and large-scale weakly supervised sound event detection for smart cars. This paper presents the setup of these tasks: task definition, dataset, experimental setup, and baseline system results on the development dataset. The baseline systems for all tasks rely on the same implementation using multilayer perceptron and log mel-energies, but differ in the structure of the output layer and the decision making process, as well as the evaluation of system output using task specific metrics.",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    keywords = "Sound scene analysis; Acoustic scene classification; Sound event detection; Audio tagging; Rare sound events",
    pages = "85--92",
    publisher = "Tampere University of Technology. Laboratory of Signal Processing",
    title = "{DCASE} 2017 challenge setup: tasks, datasets and baseline system",
    year = "2017",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/dcase-2017-challenge-paper.pdf"
}

@inproceedings{Mimilakis2018_ICASSP,
    author = "Mimilakis, Stylianos Ioannis and Drossos, Konstantinos and Santos, Jo{\\textasciitilde a}o F. and Schuller, Gerald and Virtanen, Tuomas and Bengio, Yoshua",
    abstract = "Singing voice separation based on deep learning relies on the usage of time-frequency masking. In many cases the masking process is not a learnable function or is not encapsulated into the deep learning optimization. Consequently, most of the existing methods rely on a post processing step using the generalized Wiener filtering. This work proposes a method that learns and optimizes (during training) a source-dependent mask and does not need the aforementioned post processing step. We introduce a recurrent inference algorithm, a sparse transformation step to improve the mask generation process, and a learned denoising filter. Obtained results show an increase of 0.49 dB for the signal to distortion ratio and 0.30 dB for the signal to interference ratio, compared to previous state-of-the-art approaches for monaural singing voice separation.",
    title = "Monaural Singing Voice Separation with Skip-Filtering Connections and Recurrent Inference of Time-Frequency Mask",
    booktitle = "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    pages = "721--725",
    year = "2018",
    organization = "IEEE",
    url = "https://arxiv.org/abs/1711.01437"
}

@inproceedings{Naithani2018_IWAENC,
    author = "Naithani, Gaurav and Nikunen, Joonas and Bramslow, Lars and Virtanen, Tuomas",
    abstract = "Mean square error (MSE) has been the preferred choice as loss function in the current deep neural network (DNN) based speech separation techniques. In this paper, we propose a new cost function with the aim of optimizing the extended short time objective intelligibility (ESTOI) measure. We focus on applications where low algorithmic latency (≤ 10 ms) is important. We use long short-term memory networks (LSTM) and evaluate our proposed approach on four sets of two-speaker mixtures from extended Danish hearing in noise (HINT) dataset. We show that the proposed loss function can offer improved or at par objective intelligibility (in terms of ESTOI) compared to an MSE optimized baseline while resulting in lower objective separation performance (in terms of the source to distortion ratio (SDR)). We then proceed to propose an approach where the network is first initialized with weights optimized for MSE criterion and then trained with the proposed ESTOI loss criterion. This approach mitigates some of the losses in objective separation performance while preserving the gains in objective intelligibility.",
    booktitle = "16th International Workshop on Acoustic Signal Enhancement, IWAENC 2018",
    day = "2",
    doi = "10.1109/IWAENC.2018.8521379",
    keywords = "Deep neural networks; Low latency; Speech intelligibility; Speech separation",
    month = "11",
    pages = "386--390",
    publisher = "IEEE",
    title = "{D}eep neural network based speech separation optimizing an objective estimator of intelligibility for low latency applications",
    year = "2018",
    url = "https://arxiv.org/pdf/1807.06899.pdf"
}

@mastersthesis{Yli-Hietanen1995_master,
    author = "Yli-Hietanen, Jari",
    title = {{T}ulosuunnan estimointi k{\"a}ytt{\"a}en pient{\"a} kolmiulotteista sensorij{\"a}rjestelm{\"a}{\"a}},
    year = "1995",
    school = "Tampere University of Technology"
}

@inproceedings{Yli-Hietanen1996_NORSIG,
    author = {Yli-Hietanen, Jari and Kallioj{\"a}rvi, Kari and Astola, Jaakko},
    booktitle = "Proceedings of Norsig'96",
    title = "{R}obust {T}ime-{D}elay {B}ased {A}ngle of {A}rrival {E}stimation",
    year = "1996"
}

@inproceedings{Yli-Hietanen1998_ICSPAT,
    author = "Yli-Hietanen, Jari and Koppinen, Konsta and Paajanen, Erkki",
    address = "Toronto, Canada",
    booktitle = "ICSPAT98",
    month = "September",
    pages = "1277-1280",
    title = "{S}iren {S}ound {S}uppression for {S}peech {E}nhancement in {M}obile {C}ommunications",
    year = "1998"
}

@inproceedings{Yli-Hietanen1999_SIP99,
    author = "Yli-Hietanen, Jari and Koppinen, Konsta and Astola, Jaakko",
    address = "Nassau, Bahamas",
    booktitle = "Proceedings of the IASTED Internatioanl Conference Signal and Image Processing (SIP99)",
    month = "October",
    title = "{T}ime-delay {S}election for {R}obust {A}ngle of {A}rrival {E}stimation",
    year = "1999"
}

@inproceedings{Yli-Hietanen2000_NORSIG,
    author = "Yli-Hietanen, Jari and Saarelainen, Teemu and Routakangas, Jussi",
    address = "Kolm{\aa}rden, Sweden",
    booktitle = "In Proceedings of the IEEE Nordic Signal Processing Symposium (NORSIG)",
    month = "June",
    pages = "65-68",
    title = "Robust Angle-of-Arrival Estimation of Transient Signals",
    year = "2000"
}

@inproceedings{Helén2009_ICA,
    author = "Hel{\'e}n, Marko and Lahti, Tommi and Klapuri, Anssi",
    editor = "Niiranen, S.",
    booktitle = "Open Information Management: applications of interconnectivity and collaboration",
    isbn = "978-1-60566-246-6",
    pages = "244--265",
    title = "{T}ools for automatic audio management",
    year = "2009"
}

@inproceedings{Kivimäki2000_EUSIPCO,
    author = {Kivim{\"a}ki, Jukka and Lahti, Tommi and Koppinen, Konsta},
    address = "Tampere, Finland",
    booktitle = "Proceedings of the X European Signal Processing Conference (EUSIPCO)",
    month = "September",
    pages = "1301-1304",
    title = "{A} Phonetic Vocoder for Finnish",
    year = "2000"
}

@article{Bilcu2008,
    author = {Bilcu, Enik{\"o} Beatrice and Astola, Jaakko},
    issn = "0353-3670",
    journal = "Facta Universitatis, Series: Electronics and Energetics",
    number = "1",
    pages = "91--105",
    title = "{A} hybrid approach to bilingual text-to-phoneme mapping",
    volume = "21",
    year = "2008"
}

@inproceedings{Diment2019_IJCNN,
    author = "Diment, Aleksandr and Fagerlund, Eemi and Benfield, Adrian and Virtanen, Tuomas",
    abstract = "A machine learning method for the automatic detection of pronunciation errors made by non-native speakers of English is proposed. It consists of training word-specific binary classifiers on a collected dataset of isolated words with possible pronunciation errors, typical for Finnish native speakers. The classifiers predict whether the typical error is present in the given word utterance. They operate on sequences of acoustic features, extracted from consecutive frames of an audio recording of a word utterance. The proposed architecture includes a convolutional neural network, a recurrent neural network, or a combination of the two. The optimal topology and hyperpa-rameters are obtained in a Bayesian optimisation setting using a tree-structured Parzen estimator. A dataset of 80 words uttered naturally by 120 speakers is collected. The performance of the proposed system, evaluated on a well-represented subset of the dataset, shows that it is capable of detecting pronunciation errors in most of the words (46/49) with high accuracy (mean accuracy gain over the zero rule 12.21 percent points).",
    booktitle = "2019 International Joint Conference on Neural Networks, IJCNN 2019",
    day = "1",
    doi = "10.1109/IJCNN.2019.8851963",
    keywords = "Computer-assisted language learning; computer-assisted pronunciation training CNN; CRNN; GRU; pronunciation learning",
    month = "7",
    publisher = "IEEE",
    title = "Detection of Typical Pronunciation Errors in Non-native English Speech Using Convolutional Recurrent Neural Networks",
    year = "2019",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/Diment19\_PL.pdf"
}

@mastersthesis{Rämö1999_master,
    author = {R{\"a}m{\"o}, Anssi},
    title = "{P}itch {M}odification and {Q}uantization for {O}ffline {S}peech {C}oding",
    year = "1999",
    school = "Tampere University of Technology"
}

@inproceedings{Magron2018_InterSpecch,
    author = "Magron, Paul and Virtanen, Tuomas",
    abstract = "This paper presents novel expectation-maximization (EM) algorithms for estimating the nonnegative matrix factorization model with Itakura-Saito divergence. Indeed, the common EM-based approach exploits the space-alternating generalized EM (SAGE) variant of EM but it usually performs worse than the conventional multiplicative algorithm. We propose to explore more exhaustively those algorithms, in particular the choice of the methodology (standard EM or SAGE variant) and the latent variable set (full or reduced). We then derive four EM-based algorithms, among which three are novel. Speech separation experiments show that one of those novel algorithms using a standard EM methodology and a reduced set of latent variables outperforms its SAGE variants and competes with the conventional multiplicative algorithm.",
    booktitle = "Interspeech 2018",
    keywords = "source separation",
    publisher = "Interspeech",
    series = "Interspeech",
    title = "{E}xpectation-maximization algorithms for {I}takura-{S}aito nonnegative matrix factorization",
    year = "2018",
    url = "https://hal.archives-ouvertes.fr/hal-01632082/document"
}

@article{Mesaros2018_TASLP,
    author = "Mesaros, Annamaria and Heittola, Toni and Benetos, Emmanouil and Foster, Peter and Lagrange, Mathieu and Virtanen, Tuomas and Plumbley, Mark D.",
    abstract = "Public evaluation campaigns and datasets promote active development in target research areas, allowing direct comparison of algorithms. The second edition of the challenge on Detection and Classification of Acoustic Scenes and Events (DCASE 2016) has offered such an opportunity for development of state-of-the-art methods, and succeeded in drawing together a large number of participants from academic and industrial backgrounds. In this paper, we report on the tasks and outcomes of the DCASE 2016 challenge. The challenge comprised four tasks: acoustic scene classification, sound event detection in synthetic audio, sound event detection in real-life audio, and domestic audio tagging. We present in detail each task and analyse the submitted systems in terms of design and performance. We observe the emergence of deep learning as the most popular classification method, replacing the traditional approaches based on Gaussian mixture models and support vector machines. By contrast, feature representations have not changed substantially throughout the years, as mel frequency-based representations predominate in all tasks. The datasets created for and used in DCASE 2016 are publicly available and are a valuable resource for further research.",
    doi = "10.1109/TASLP.2017.2778423",
    issn = "2329-9290",
    journal = "IEEE-ACM Transactions on Audio Speech and Language Processing",
    keywords = "Acoustic scene classification;Acoustics;audio datasets;Event detection;Hidden Markov models;pattern recognition;sound event detection;Speech;Speech processing;Tagging;audio tagging",
    month = "11",
    publisher = "IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC",
    title = "Detection and Classification of Acoustic Scenes and Events: Outcome of the {DCASE} 2016 Challenge",
    year = "2018",
    url = "https://trepo.tuni.fi//bitstream/handle/10024/126402/dcase2016\_taslp.pdf?sequence=1"
}

@ARTICLE{Virtanen2007_TASLP,
    author = "Virtanen, Tuomas",
    journal = "IEEE Transactions on Audio, Speech, and Language Processing",
    title = "Monaural Sound Source Separation by Nonnegative Matrix Factorization With Temporal Continuity and Sparseness Criteria",
    year = "2007",
    volume = "15",
    number = "3",
    pages = "1066-1074",
    keywords = "Source separation;Unsupervised learning;Multiple signal classification;Spectrogram;Machine learning algorithms;Music;Sparse matrices;Humans;Independent component analysis;Costs;Acoustic signal analysis;audio source separation;blind source separation;music;nonnegative matrix factorization;sparse coding;unsupervised learning",
    doi = "10.1109/TASL.2006.885253",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/virtanen\_taslp2007.pdf"
}

@book{Virtanen2017,
    author = "Virtanen, Tuomas and Plumbley, Mark D. and Ellis, Dan",
    abstract = "This book presents computational methods for extracting the useful information from audio signals, collecting the state of the art in the field of sound event and scene analysis. The authors cover the entire procedure for developing such methods, ranging from data acquisition and labeling, through the design of taxonomies used in the systems, to signal processing methods for feature extraction and machine learning methods for sound recognition. The book also covers advanced techniques for dealing with environmental variation and multiple overlapping sound sources, and taking advantage of multiple microphones or other modalities. The book gives examples of usage scenarios in large media databases, acoustic monitoring, bioacoustics, and context-aware devices. Graphical illustrations of sound signals and their spectrographic representations are presented, as well as block diagrams and pseudocode of algorithms.",
    doi = "10.1007/978-3-319-63450-0",
    isbn = "978-3-319-63449-4",
    month = "9",
    publisher = "Springer",
    title = "Computational analysis of sound scenes and events",
    year = "2017",
    url = "http://www.springer.com/us/book/9783319634494"
}

@inproceedings{Virtanen2000_ICASSP,
    author = "Virtanen, Tuomas and Klapuri, Anssi",
    address = "Istanbul, Turkey",
    booktitle = "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    keywords = "Separation; sinusoidal model",
    title = "{S}eparation of {H}armonic {S}ound {S}ources {U}sing {S}inusoidal {M}odeling",
    year = "2000"
}

@mastersthesis{Virtanen2001_master,
    author = "Virtanen, Tuomas",
    abstract = "In audio signal spectrum modeling, the aim is to transform a signal to a more easily applicable form, removing the information that is irrelevant in signal perception. Sinusoids plus noise model is a spectral model, in which the periodic components of the sound are represented with sinusoids with time-varying frequencies, amplitudes and phases. The remaining non-periodic components are represented with a ﬁltered noise. The sinusoidal model utilizes the physical properties of musical instruments and the noise model the humans’ inability to perceive the exact spectral shape or phase of stochastic signals. In the case of polyphonic music signals, the estimation of the parameters of sinusoids is a difficult task, since the periodic components are usually not stable. A sufficient time and frequency resolution is also difficult to achieve at the same time. A big part of this thesis discusses the detection and parameter estimation of periodic components with several algorithms. In addition to already existing algorithms, a new iterative algorithm is presented, which is based on the fusion of closely spaced sinusoids. The sinusoidal model is applied in the separation of overlapping sounds and manipulation. In the sound separation, a new perceptual distance measure between sinusoids is used. The perceptual distance measure is based on the humans’ way to associate spectral components into sound sources. Also a new separation method based on the multipitch estimation is explained. The modification of the pitch and time scale of sounds with the sinusoid plus noise model without affecting the quality of the sound is explained shortly, too.",
    title = "{A}udio {S}ignal {M}odeling with {S}inusoids {P}lus {N}oise",
    year = "2001",
    school = "Tampere University of Technology"
}

@inproceedings{Virtanen2003_ICMC,
    author = "Virtanen, Tuomas",
    booktitle = "International Computer Music Conference",
    keywords = "NMF;sparseness",
    title = "Sound Source Separation Using Sparse Coding with Temporal Continuity Objective",
    year = "2003",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/icmc2003.pdf"
}

@phdthesis{Virtanen2006_phd,
    author = "Virtanen, Tuomas",
    abstract = "Sound source separation refers to the task of estimating the signals produced by individual sound sources from a complex acoustic mixture. It has several applications, since monophonic signals can be processed more eﬃciently and ﬂexibly than polyphonic mixtures. This thesis deals with the separation of monaural, or, one-channel music recordings. We concentrate on separation methods, where the sources to be separated are not known beforehand. Instead, the separation is enabled by utilizing the common properties of real-world sound sources, which are their continuity, sparseness, and repetition in time and frequency, and their harmonic spectral structures. One of the separation approaches taken here use unsupervised learning and the other uses model-based inference based on sinusoidal modeling. Most of the existing unsupervised separation algorithms are based on a linear instantaneous signal model, where each frame of the input mixture signal is modeled as a weighted sum of basis functions. We review the existing algorithms which use independent component analysis, sparse coding, and non-negative matrix factorization to estimate the basis functions from an input mixture signal. Our proposed unsupervised separation algorithm based on the instantaneous model combines non-negative matrix factorization with sparseness and temporal continuity objectives. The algorithm is based on minimizing the reconstruction error between the magnitude spectrogram of the observed signal and the model, while restricting the basis functions and their gains to non-negative values, and the gains to be sparse and continuous in time. In the minimization, we consider iterative algorithms which are initialized with random values and updated so that the value of the total objective cost function decreases at each iteration. Both multiplicative update rules and a steepest descent algorithm are proposed for this task. To improve the convergence of the projected steepest descent algorithm, we propose an augmented divergence to measure the reconstruction error. Simulation experiments on generated mixtures of pitched instruments and drums were run to monitor the behavior of the proposed method. The proposed method enables average signal-to-distortion ratio (SDR) of 7.3 dB, which is higher than the SDRs obtained with the other tested methods based on the instantaneous signal model. To enable separating entities which correspond better to real-world sound objects, we propose two convolutive signal models which can be used to represent time-varying spectra and fundamental frequencies. We propose unsupervised learning algorithms extended from non-negative matrix factorization for estimating the model parameters from a mixture signal. The objective in them is to minimize the reconstruction error between the magnitude spectrogram of the observed signal and the model while restricting the parameters to non-negative values. Simulation experiments show that time-varying spectra enable better separation quality of drum sounds, and time-varying frequencies representing diﬀerent fundamental frequency values of pitched instruments conveniently. Another class of studied separation algorithms is based on the sinusoidal model, where the periodic components of a signal are represented as the sum of sinusoids with time-varying frequencies, amplitudes, and phases. The model provides a good representation for pitched instrument sounds, and the robustness of the parameter estimation is here increased by restricting the sinusoids of each source to harmonic frequency relationships. Our proposed separation algorithm based on sinusoidal modeling minimizes the reconstruction error between the observed signal and the model. Since the rough shape of spectrum of natural sounds is continuous as a function of frequency, the amplitudes of overlapping overtones can be approximated by interpolating from adjacent overtones, for which we propose several methods. Simulation experiments on generated mixtures of pitched musical instruments show that the proposed methods allow average SDR above 15 dB for two simultaneous sources, and the quality decreases gradually as the number of sources increases.",
    title = "Sound Source Separation in Monaural Music Signals",
    year = "2006",
    school = "Tampere University of Technology"
}

@phdthesis{Helen2009_phd,
    author = "Helen, Marko",
    abstract = "Personal multimedia databases contain thousands of items and other databases on the Internet may contain even billions of items. Finding a particular item manually from such databases becomes overwhelming and thus automatic search engines are required to lighten the job. Query by example refers to automatically finding multimedia items from a database, which are similar to the example provided by the user. This is an important task in modern multimedia databases. This thesis deals with automatic query by example of audio samples. The emphasis is on representation and distance measures between two audio signals, which are used to estimate the similarity between these two signals. The thesis also covers computational issues, which are highly important when it comes to practical implementation of the algorithms. Two different audio signal representations are proposed. These representations are interconnected, since the first separates drums from a polyphonic music signal although the same approach could be used to separate other parts of the original signal as well, for example, harmonic instruments. The second representation models the harmonic sound using only a few parameters. The proposed method is based on Mel frequency cepstral coefficients, which are further modeled using attack-decay-sustain-release curves with temporal evolution of harmonic instruments. Most distance measures, used in audio signal processing, are based on dividing a signal into frames, extracting perceptually motivated features from each frame, and calculating the distance between the features. Most of the proposed distance measures use Gaussian mixture models to estimate the probability density functions of the framewise features and calculate the distance between the Gaussian mixture models. However, the thesis also introduces a parameter free distance measurement. This is based on compression ratios of audio signals and i hence it removes the user influence on the results, since no features or other parameters need to be set. In a query by example application, the similarity between the example provided by the user and each database item need to be calculated in order to obtain a ranked list of database samples. However, in practical applications this operation is very time-consuming if the database contains millions of items. The proposed method applies keysample transformation to reduce the series of feature vectors, used to represent each signal, into a single feature vector. The database is then clustered and the search is restricted to only a few clusters, thus saving retrieval time with some loss of accuracy.",
    isbn = "978-952-15-2168-3",
    month = "6",
    publisher = "Tampere University of Technology",
    series = "Tampere University of Technology. Publication",
    title = "{S}imilarity {M}easures for {C}ontent-{B}ased {A}udio {R}etrieval",
    url = "https://tutcris.tut.fi/portal/files/2309319/helen.pdf",
    year = "2009",
    school = "Tampere University of Technology"
}

@ARTICLE{Klapuri2008_TASLP,
    author = "Klapuri, Anssi",
    journal = "IEEE Transactions on Audio, Speech, and Language Processing",
    title = "Multipitch Analysis of Polyphonic Music and Speech Signals Using an Auditory Model",
    year = "2008",
    volume = "16",
    number = "2",
    pages = "255-266",
    keywords = "Speech analysis;Signal analysis;Multiple signal classification;Frequency estimation;Computational modeling;Music;Humans;Robustness;Signal processing;Interference;Acoustic signal analysis;fundamental frequency estimation;music information retrieval;pitch perception;Acoustic signal analysis;fundamental frequency estimation;music information retrieval;pitch perception",
    doi = "10.1109/TASL.2007.908129"
}

@inproceedings{Bear2019_WASPAA,
    author = "Bear, Helen L. and Heittola, Toni and Mesaros, Annamaria and Benetos, Emmanouil and Virtanen, Tuomas",
    abstract = "The majority of sound scene analysis work focuses on one of two clearly defined tasks: acoustic scene classification or sound event detection. Whilst this separation of tasks is useful for problem definition, they inherently ignore some subtleties of the real-world, in particular how humans vary in how they describe a scene. Some will describe the weather and features within it, others will use a holistic descriptor like `park', and others still will use unique identifiers such as cities or names. In this paper, we undertake the task of automatic city classification to ask whether we can recognize a city from a set of sound scenes? In this problem each city has recordings from multiple scenes. We test a series of methods for this novel task and show that a simple convolutional neural network (CNN) can achieve accuracy of 50\%. This is less than the acoustic scene classification task baseline in the DCASE 2018 ASC challenge on the same data. A simple adaptation to the class labels of pairing city labels with grouped scenes, accuracy increases to 52\%, closer to the simpler scene classification task. Finally we also formulate the problem in a multi-task learning framework and achieve an accuracy of 56\%, outperforming the aforementioned approaches.",
    booktitle = "2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)",
    doi = "10.1109/WASPAA.2019.8937271",
    isbn = "978-1-7281-1124-7",
    keywords = "Acoustic scene classification; location identification; city classification; computational sound scene analysis",
    month = "10",
    pages = "11--15",
    publisher = "IEEE",
    series = "IEEE Workshop on Applications of Signal Processing to Audio and Acoustics",
    title = "City Classification from Multiple Real-World Sound Scenes",
    year = "2019",
    url = "https://arxiv.org/abs/1905.00979"
}

@INPROCEEDINGS{Virtanen2007_WASPAA,
    author = "Virtanen, Tuomas and Helen, Marko",
    booktitle = "2007 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics",
    title = "Probabilistic Model Based Similarity Measures for Audio Query-by-Example",
    year = "2007",
    volume = "",
    number = "",
    pages = "82-85",
    keywords = "Hidden Markov models;Testing;Acoustic measurements;Distortion measurement;Signal generators;Speech;Databases;Humans;Conferences;Acoustic signal processing",
    doi = "10.1109/ASPAA.2007.4393031",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/virtanen\_waspaa07.pdf"
}

@INPROCEEDINGS{Virtanen2008_ICASSP,
    author = "Virtanen, Tuomas and Taylan Cemgil, A. and Godsill, Simon",
    booktitle = "2008 IEEE International Conference on Acoustics, Speech and Signal Processing",
    title = "Bayesian extensions to non-negative matrix factorisation for audio signal modelling",
    year = "2008",
    volume = "",
    number = "",
    pages = "1825-1828",
    keywords = "Bayesian methods;Instruments;Signal processing;Time frequency analysis;Signal generators;Source separation;Spectrogram;Multiple signal classification;Music;Laboratories;acoustic signal processing;matrix decomposition;MAP estimation;source separation",
    doi = "10.1109/ICASSP.2008.4517987",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/icassp08.pdf"
}

@inproceedings{Alves2009_EUSIPCO,
    author = "Alves, David Dos Santos and Paulus, Jouni and Fonseca, Jose",
    address = "Glasgow, Scotland, UK",
    booktitle = "Proc. of the 17th European Signal Processing Conference",
    keywords = "drums; transcription; NMF",
    month = "Aug",
    pages = "894--898",
    title = "{D}rum transcription from multichannel recordings with non-negative matrix factorization",
    year = "2009"
}

@article{Nikunen2017_TASLP,
    author = "Nikunen, Joonas and Diment, Aleksandr and Virtanen, Tuomas",
    abstract = "In this paper we propose a method for separation of moving sound sources. The method is based on first tracking the sources and then estimation of source spectrograms using multichannel non-negative matrix factorization (NMF) and extracting the sources from the mixture by single-channel Wiener filtering. We propose a novel multichannel NMF model with time-varying mixing of the sources denoted by spatial covariance matrices (SCM) and provide update equations for optimizing model parameters minimizing squared Frobenius norm. The SCMs of the model are obtained based on estimated directions of arrival of tracked sources at each time frame. The evaluation is based on established objective separation criteria and using real recordings of two and three simultaneous moving sound sources. The compared methods include conventional beamforming and ideal ratio mask separation. The proposed method is shown to exceed the separation quality of other evaluated blind approaches according to all measured quantities. Additionally, we evaluate the method's susceptibility towards tracking errors by comparing the separation quality achieved using annotated ground truth source trajectories.",
    doi = "10.1109/TASLP.2017.2774925",
    issn = "2329-9290",
    journal = "IEEE-ACM Transactions on Audio Speech and Language Processing",
    keywords = "acoustic source tracking;Acoustics;Array signal processing;Direction-of-arrival estimation;Estimation;Mathematical model;microphone arrays;Microphones;moving sound sources;Sound source separation;Spectrogram;time-varying mixing model",
    month = "11",
    publisher = "IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC",
    title = "Separation of Moving Sound Sources Using Multichannel {NMF} and Acoustic Tracking",
    year = "2017",
    url = "https://arxiv.org/pdf/1710.10005.pdf"
}

@inproceedings{Nikunen2018_ICASSP,
    author = "Nikunen, Joonas and Virtanen, Tuomas",
    abstract = "This paper proposes a method for online estimation of time-varying room impulse responses (RIR) between multiple isolated sound sources and a far-field mixture. The algorithm is formulated as adaptive convolutive filtering in short-time Fourier transform (STFT) domain. We use the recursive least squares (RLS) algorithm for estimating the filter parameters due to its fast convergence rate, which is required for modeling rapidly changing RIRs of moving sound sources. The proposed method allows separation of reverberated sources from the far-field mixture given that their close-field signals are available. The evaluation is based on measuring unmixing performance (removal of reverberated source) using objective separation criteria calculated between the ground truth recording of the preserved sources and the unmixing result obtained with the proposed algorithm. We compare online and offline formulations for the RIR estimation and also provide evaluation with blind source separation algorithm only operating on the mixture signal.",
    address = "United States",
    booktitle = "2018 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2018 - Proceedings",
    day = "10",
    doi = "10.1109/ICASSP.2018.8462535",
    isbn = "9781538646588",
    keywords = "Adaptive filtering; Informed source separation; Online room impulse response estimation; Source unmixing",
    month = "9",
    pages = "421--425",
    publisher = "Institute of Electrical and Electronics Engineers Inc.",
    series = "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing",
    title = "{E}stimation of time-varying room impulse responses of multiple sound sources from observed mixture and isolated source signals",
    volume = "2018-April",
    year = "2018",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/Nikunen\_ICASSP2018\_rev"
}

@phdthesis{Popa2012_phd,
    author = "Popa, Victor",
    abstract = "Voice conversion is a speech technology encompassing transformations applied to the speech signal with the purpose of changing the perceived speaker identity from a source voice to a desired target voice. The principal use of voice conversion is to enable synthesis systems to generate speech with customized voices without need for exhaustive recordings and processing. Voice conversion can be realized as a stand-alone task or alternatively, using adaptation techniques with HMM-based synthesis. Although there are many speaker-dependent voice characteristics, voice conversion deals mainly with those acoustic in nature such as the spectral characteristics and the fundamental frequency. In spite of the remarkable results achieved by the state of the art techniques, further challenges remain to be solved in all sub-areas of voice conversion in order to provide excellent quality and highly successful identity conversion. The objective of this thesis is to develop a stand-alone voice conversion system for coded speech and to propose solutions leading to better quality, versatility or efficiency compared to current techniques. The thesis is focused on the conversion of spectral envelopes but other sub-areas of voice conversion such as speech parameterization or alignment are also treated. The analysis-modification-synthesis system adopted in this thesis is based on a parametric speech model used in a real speech codec but also for the internal speech representation of a concatenative TTS system. This allows an easy integration of voice conversion with communications related and embedded applications developed for coded speech. The validity of this parameterization has been confirmed by using it in an actual voice conversion scheme. In addition, a voicing level control scheme, a speech enhancement technique and a method for automatic speech data collection have been proposed, all of them taking advantage of this parametric framework. The versatility of voice conversion systems depends on the characteristics imposed on the training data in order to properly estimate a conversion function. Depending on the characteristics of the training data different alignment strategies can be adopted. Although dynamic time warping (DTW) has become almost a standard for the alignment of parallel training data, a new soft alignment technique is proposed for the same purpose. This concept allows probabilistic one-to-many frame mapping and is proven valid in an experiment with an artificial example. For practical reasons the non-parallel case has been attracting increased interest lately. In this thesis, two techniques for text independent alignment are proposed. The first one is based on phonetic segmentation and temporal decomposition and was successfully used in a voice conversion application. The second method uses a TTS to break the non-parallel problem into two parallel conversions which concatenated realize the desired voice conversion. GMM-based techniques have been the most popular approach for the conversion of spectral envelopes in spite of their problems related to over-smoothing, over-fitting and the lack of temporal modeling. In order to address these issues and improve the GMM-based voice conversion, this thesis introduces several techniques. First, a new measure of the conversion accuracy is proposed, which can be easily computed from the GMM parameters, and is found to be in line with perceptual and objective metrics. The next technique combines a clustering and mode selection scheme with cluster-wise GMM modeling and is based on the observation that the mapping accuracy improves with the reduction of target data variance. The proposed technique is shown to outperform a comparable approach that uses voicing based clustering. A third technique can be used to adapt an existing well-trained conversion model to a new target speaker using a small amount of data. In a practical evaluation the adapted model outperformed an equivalent model trained exclusively on the reduced data. The continuity issue is addressed in a final idea proposed for future research. In general the spectral conversion techniques proposed in the literature have been subject to a tradeoff between speech quality and identity conversion and the challenge remains to provide optimal results for both criteria simultaneously. A new spectral conversion technique introduced in this thesis uses bilinear models to decompose the line spectral frequencies (LSF) representation of the spectral envelope into two factors corresponding to speaker identity and phonetic content respectively. In an extensive evaluation on different types and for different sizes of training data this approach was found to perform similarly to a GMM-based conversion even though, in contrast to the GMM, it does not require any tuning. The concepts of contextual and local modeling are also introduced arguing that a more accurate mapping can be achieved if multiple models are fit on relatively small subsets of the full training data rather than using one global model. The validity of the concepts has been verified in practical experiments. Furthermore, a local linear transformation technique is shown to effectively reduce the over-smoothing relatively to a globally trained GMM-based conversion function. For spectral conversion, several of the proposed techniques can be considered extensions of a baseline vector quantization framework, opening the way for further developments in this direction. In addition to the local linear transformation method which can be easily integrated in this framework, a memory efficient conversion scheme based on multi-stage vector quantization (MSVQ) was also proposed. The experiments indicated substantial memory savings and even accuracy improvement compared to some conventional codebook conversions. A dynamic programming approach to optimizing the frame to frame continuity in a vector quantization framework is also proposed as a future direction. A final contribution is brought to the existing hybrid technique that combines GMM and frequency warping. The approach is adapted to work with the proposed speech representation and a procedure for automatic formant alignment and warping function calculation is presented.",
    isbn = "978-952-15-2835-4",
    month = "6",
    publisher = "Tampere University of Technology",
    series = "Tampere University of Technology. Publication",
    title = "{T}echniques for {S}pectral {V}oice {C}onversion",
    url = "https://tutcris.tut.fi/portal/files/5114514/popa.pdf",
    year = "2012",
    school = "Tampere University of Technology"
}

@article{Mahkonen2018_JIV,
    author = {Mahkonen, Katariina and Virtanen, Tuomas and K{\"a}m{\"a}r{\"a}inen, Joni},
    abstract = "This paper considers a scenario when we have multiple pre-trained detectors for detecting an event and a small dataset for training a combined detection system. We build the combined detector as a Boolean function of thresholded detector scores and implement it as a binary classification cascade. The cascade structure is computationally efficient by providing the possibility to early termination. For the proposed Boolean combination function, the computational load of classification is reduced whenever the function becomes determinate before all the component detectors have been utilized. We also propose an algorithm, which selects all the needed thresholds for the component detectors within the proposed Boolean combination. We present results on two audio-visual datasets, which prove the efficiency of the proposed combination framework. We achieve state-of-the-art accuracy with substantially reduced computation time in laughter detection task, and our algorithm finds better thresholds for the component detectors within the Boolean combination than the other algorithms found in the literature.",
    keywords = "Binary classification;Boolean combination;Classification cascade",
    title = "{C}ascade of {B}oolean detector combinations",
    journal = "Eurasip Journal on Image and Video Processing",
    volume = "2018",
    pages = "1--22",
    year = "2018",
    publisher = "Springer"
}

@ARTICLE{Eronen2010_TASLP,
    author = "Eronen, Antti J. and Klapuri, Anssi P.",
    journal = "IEEE Transactions on Audio, Speech, and Language Processing",
    title = "Music Tempo Estimation With k-NN Regression",
    year = "2010",
    volume = "18",
    number = "1",
    pages = "50-57",
    keywords = "Performance analysis;Signal processing;Feature extraction;Acoustic signal detection;Harmonic analysis;Visual effects;Software libraries;Mood;Signal analysis;Acoustic measurements;Chroma features;$k$-nearest neighbor ($k$-NN) regression;music tempo estimation",
    doi = "10.1109/TASL.2009.2023165"
}

@INPROCEEDINGS{Virtanen2009_ICASSP,
    author = "Virtanen, Tuomas and Heittola, Toni",
    booktitle = "2009 IEEE International Conference on Acoustics, Speech and Signal Processing",
    title = "Interpolating hidden Markov model and its application to automatic instrument recognition",
    year = "2009",
    volume = "",
    number = "",
    pages = "49-52",
    keywords = "Hidden Markov models;Instruments;Interpolation;Signal processing algorithms;Piecewise linear techniques;Parameter estimation;Acoustic signal processing;Pattern classification;Signal synthesis;Speech synthesis;Hidden Markov models;acoustic signal processing;musical instruments;pattern classification",
    doi = "10.1109/ICASSP.2009.4959517",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/ihmm\_icassp09.pdf"
}

@inproceedings{Drossos2018_IWAENC,
    author = "Drossos, Konstantinos and Magron, Paul and Mimilakis, Stylianos Ioannis and Virtanen, Tuomas",
    abstract = "Harmonic/percussive source separation (HPSS) consists in separating the pitched instruments from the percussive parts in a music mixture. In this paper, we propose to apply the recently introduced Masker-Denoiser with twin networks (MaD TwinNet) system to this task. MaD TwinNet is a deep learning architecture that has reached state-of-the-art results in monaural singing voice separation. Herein, we propose to apply it to HPSS by using it to estimate the magnitude spectrogram of the percussive source. Then, we retrieve the complex-valued short-term Fourier transform of the sources by means of a phase recovery algorithm, which minimizes the reconstruction error and enforces the phase of the harmonic part to follow a sinusoidal phase model. Experiments conducted on realistic music mixtures show that this novel separation system outperforms the previous state-of-the art kernel additive model approach.",
    booktitle = "16th International Workshop on Acoustic Signal Enhancement, IWAENC 2018",
    doi = "10.1109/IWAENC.2018.8521371",
    month = "11",
    pages = "421--425",
    publisher = "IEEE",
    title = "Harmonic-Percussive Source Separation with Deep Neural Networks and Phase Recovery",
    year = "2018",
    url = "https://arxiv.org/pdf/1807.11298.pdf"
}

@book{Klapuri2004,
    author = "Klapuri, Anssi and Eronen, Antti and Astola, Jaakko",
    isbn = "952-15-1149-4",
    number = "1/2004",
    publisher = "TTY-Paino",
    series = "Tampere University of Technology, Institute of Signal Processing, Report",
    title = "{A}utomatic estimation of the meter of acoustic musical signals",
    year = "2004"
}

@inproceedings{Gharib2018_MLSP,
    author = "Gharib, Shayan and Derrar, Honain and Niizumi, Daisuke and Senttula, Tuukka and Tommola, Janne and Heittola, Toni and Virtanen, Tuomas and Huttunen, Heikki",
    abstract = "In this paper we study the problem of acoustic scene classification, i.e., categorization of audio sequences into mutually exclusive classes based on their spectral content. We describe the methods and results discovered during a competition organized in the context of a graduate machine learning course; both by the students and external participants. We identify the most suitable methods and study the impact of each by performing an ablation study of the mixture of approaches. We also compare the results with a neural network baseline, and show the improvement over that. Finally, we discuss the impact of using a competition as a part of a university course, and justify its importance in the curriculum based on student feedback.",
    booktitle = "2018 IEEE International Workshop on Machine Learning for Signal Processing, MLSP 2018",
    doi = "10.1109/MLSP.2018.8517000",
    month = "9",
    publisher = "IEEE",
    title = "Acoustic Scene Classification: A Competition Review",
    year = "2018",
    url = "https://arxiv.org/pdf/1808.02357.pdf"
}

@inproceedings{Shuyang2017_WASPAA,
    author = "Shuyang, Zhao and Heittola, Toni and Virtanen, Tuomas",
    abstract = "This paper targets on a generalized vocal mode classifier (speech/singing) that works on audio data from an arbitrary data source. However, previous studies on sound classification are commonly based on cross-validation using a single dataset, without considering the cases that training and testing data are recorded in mismatched condition. Experiments revealed a big difference between homogeneous recognition scenario and heterogeneous recognition scenario, using a new dataset TUT-vocal-2016. In the homogeneous recognition scenario, the classification accuracy using cross-validation on TUT-vocal-2016 was 95.5\%. In heterogeneous recognition scenario, seven existing datasets were used as training material and TUT-vocal-2016 was used for testing, the classification accuracy was only 69.6\%. Several feature normalization methods were tested to improve the performance in heterogeneous recognition scenario. The best performance (96.8\%) was obtained using the proposed subdataset-wise normalization.",
    address = "United States",
    booktitle = "2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)",
    doi = "10.1109/WASPAA.2017.8169986",
    isbn = "978-1-5386-1631-4",
    keywords = "sound classification; vocal mode; heterogeneous data sources; feature normalization",
    pages = "16–20",
    publisher = "IEEE Computer Society",
    title = "Learning vocal mode classifiers from heterogeneous data sources",
    year = "2017",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/vocal\_mode.pdf"
}

@article{Mimilakis2019_TASLP,
    author = "Mimilakis, Stylianos Ioannis and Drossos, Konstantinos and Cano, Estefania and Schuller, Gerald",
    abstract = "The goal of this article is to investigate what singing voice separation approaches based on neural networks learn from the data. We examine the mapping functions of neural networks based on the denoising autoencoder (DAE) model that are conditioned on the mixture magnitude spectra. To approximate the mapping functions, we propose an algorithm inspired by the knowledge distillation, denoted the neural couplings algorithm (NCA). The NCA yields a matrix that expresses the mapping of the mixture to the target source magnitude information. Using the NCA, we examine the mapping functions of three fundamental DAE-based models in music source separation; one with single-layer encoder and decoder, one with multi-layer encoder and single-layer decoder, and one using skip-filtering connections (SF) with a single-layer encoding and decoding. We first train these models with realistic data to estimate the singing voice magnitude spectra from the corresponding mixture. We then use the optimized models and test spectral data as input to the NCA. Our experimental findings show that approaches based on the DAE model learn scalar filtering operators, exhibiting a predominant diagonal structure in their corresponding mapping functions, limiting the exploitation of inter-frequency structure of music data. In contrast, skip-filtering connections are shown to assist the DAE model in learning filtering operators that exploit richer inter-frequency structures.",
    doi = "10.1109/TASLP.2019.2952013",
    issn = "2329-9290",
    journal = "IEEE-ACM Transactions on Audio Speech and Language Processing",
    keywords = "Music source separation;singing voice;denoising autoencoder;DAE;skip connections;neural couplings algorithm;NCA",
    pages = "266--278",
    publisher = "IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC",
    title = "Examining the Mapping Functions of Denoising Autoencoders in Singing Voice Separation",
    volume = "28",
    year = "2019"
}

@inproceedings{Adavanne2018_EUSIPCO,
    author = "Adavanne, Sharath and Politis, Archontis and Virtanen, Tuomas",
    abstract = "This paper proposes a deep neural network for estimating the directions of arrival (DOA) of multiple sound sources. The proposed stacked convolutional and recurrent neural network (DOAnet) generates a spatial pseudo-spectrum (SPS) along with the DOA estimates in both azimuth and elevation. We avoid any explicit feature extraction step by using the magnitudes and phases of the spectrograms of all the channels as input to the network. The proposed DOAnet is evaluated by estimating the DOAs of multiple concurrently present sources in anechoic, matched and unmatched reverberant conditions. The results show that the proposed DOAnet is capable of estimating the number of sources and their respective DOAs with good precision and generate SPS with high signal-to-noise ratio.",
    booktitle = "2018 26th European Signal Processing Conference (EUSIPCO)",
    doi = "10.23919/EUSIPCO.2018.8553182",
    isbn = "978-1-5386-3736-4",
    keywords = "array signal processing; direction-of-arrival estimation; feature extraction; feedforward neural nets; recurrent neural nets; signal classification; spatial pseudospectrum; SPS; DOA estimates; explicit feature extraction step; DOAnet; multiple concurrently",
    month = "9",
    pages = "1462--1466",
    title = "Direction of Arrival Estimation for Multiple Sound Sources Using Convolutional Recurrent Neural Network",
    year = "2018",
    url = "https://arxiv.org/abs/1710.10059"
}

@mastersthesis{Myllymäki2008_master,
    author = {Myllym{\"a}ki, Mikko},
    abstract = "Voice activity detection stands for the process of recognizing speech segments from an input signal consisting of speech, pauses in the speech, silence, breathing and acoustic interference. Voice activity detection algorithm is an important part of many communication devices, such as mobile phones, because they can be used for example to reduce battery consumption and bandwidth usage. However, the communication devices and also the circumstances in which they are used vary greatly, and thus there does not exist one such voice activity detection algorithm that could be used in every case effectively but the algorithm has to be developed specifically for the problem at hand. In the thesis a voice activity detection algorithm was developed to be used in circumstances, where a very high-level breathing sound is present in the signal. Because the property is unique when compared to previous studies, previously developed voice activity detection algorithms could not be used. Instead, a new voice activity detection algorithm that constitutes of framewise feature extraction, classification of the features and postprocessing was developed. This was done by testing many different options for the parts of the voice activity detection algorithm, evaluating systematically their contribution to the results of the detection and selecting the best combination of parts as the final voice activity detection algorithm. The final voice activity detection algorithm constitutes of Mel-frequency band energies as the features, neural network as the classifier and hidden Markov model as the postprocessing method. All the different options of the algorithm parts and the results obtained with different algorithms were presented in the thesis.",
    title = "{V}oice activity detection in the presence of breathing noise",
    year = "2008",
    school = "Tampere University of Technology"
}

@inproceedings{Adavanne2017_EUSIPCO,
    author = "Adavanne, Sharath and Drossos, Konstantinos and Cakir, Emre and Virtanen, Tuomas",
    booktitle = "2017 25th European Signal Processing Conference (EUSIPCO)",
    doi = "10.23919/EUSIPCO.2017.8081505",
    isbn = "978-0-9928626-7-1",
    pages = "1729--1733",
    publisher = "IEEE",
    title = "Stacked convolutional and recurrent neural networks for bird audio detection",
    year = "2017",
    url = "https://arxiv.org/abs/1706.02047"
}

@mastersthesis{Pertilä2003_master,
    author = {Pertil{\"a}, Pasi},
    title = "{P}assive {L}ocalization of a {S}ound {S}ource",
    year = "2003",
    school = "Tampere University of Technology"
}

@phdthesis{Pertilä2009_phd,
    author = {Pertil{\"a}, Pasi},
    abstract = "The pressure changes of an acoustic wavefront are sensed with a microphone that acts as a transducer, converting sound pressure into voltage. The voltage is then converted into digital form with an analog to digital (AD) -converter to provide a discrete time quantized digital signal. This thesis discusses methods to estimate the location of a sound source from the signals of multiple microphones. Acoustic source localization (ASL) can be used to locate talkers, which is useful for speech communication systems such as teleconferencing and hearing aids. Active localization methods receive and send energy, whereas passive methods only receive energy. The discussed ASL methods are passive which makes them attractive for surveillance applications, such as localization of vehicles and monitoring of areas. This thesis focuses on ASL in a room environment and at moderate distances that are often present in outdoor applications. The frequency range of many commonly occurring sounds such as speech, vehicles, and jet aircraft is large. Time delay estimation (TDE) methods are suitable for estimating properties from such wideband signals. Since TDE methods have been extensively studied, the theory is attractive to apply in localization. Time difference of arrival (TDOA) -based methods estimate the source location from measured TDOA values between microphones. These methods are computationally attractive but deteriorate rapidly when the TDOA estimates are no longer directly related to the source position. In a room environment such conditions could be faced when reverberation or noise starts to dominate TDOA estimation. The combination of microphone pairwise TDE measurements is studied as a more robust localization solution. TDE measurements are combined into a spatial likelihood function (SLF) of source position. A sequential Bayesian method known as particle filtering (PF) is used to estimate the source position. The PF based localization accuracy increases when the variance of SLF decreases. Results from simulations and real-data show that multiplication (intersection operation) results in a SLF with smaller variance than the typically applied summation (union operation). The above localization methods assume that the source is located in the near-field of the microphone array, i.e., the source emitted wavefront curvature is observable. In the far-field, the source wavefront is assumed planar and localization is considered by using spatially separated direction observations. The direction of arrival (DOA) of a source emitted wavefront impinging on a microphone array is traditionally estimated by steering the array to a direction that maximizes the steered response power. Such estimates can be deteriorated by noise and reverberation. Therefore, talker localization is considered using DOA discrimination. The sound propagation delay from the source to the microphone array becomes significant at moderate distances. As a result, the directional observations from a moving sound source point behind the true source position. Omitting the propagation delay results in a biased location estimate of a moving or discontinuously emitting source. To solve this problem the propagation delay is proposed to be modeled in the estimation process. Motivated by the robustness of localization using the combination of TDE measurements, source localization by directly combining the TDE-based array steered responses is considered. This extends the near-field talker localization methods to far-field source localization. The presented propagation delay modeling is then proposed for the steered response localization. The improvement in localization accuracy by including the propagation delay is studied using a simulated moving sound source in the atmosphere. The presented indoor localization methods have been evaluated in the Classification of Events, Activities and Relationships (CLEAR) 2006 and CLEAR'07 technology evaluations. In the evaluations, the performance of the proposed ASL methods was evaluated by a third party from several hours of annotated data. The data was gathered from meetings held in multiple smart rooms. According to the obtained results from CLEAR'07 development dataset (166 min) presented in this thesis, 92 \% of speech activity in a meeting situation was located within 17 cm accuracy.",
    keywords = "Speaker tracking; speaker localization; monte carlo method",
    month = "Jan",
    school = "Tampere University of Technology",
    title = "{A}coustic {S}ource {L}ocalization in a {R}oom {E}nvironment and at {M}oderate {D}istances",
    type = "PhD Thesis",
    year = "2009"
}

@article{Pertilä2013,
    author = {Pertil{\"a}, Pasi},
    abstract = "Separating speech signals of multiple simultaneous talkers in a reverberant enclosure is known as the cocktail party problem. In real-time applications online solutions capable of separating the signals as they are observed are required in contrast to separating the signals offline after observation. Often a talker may move, which should also be considered by the separation system. This work proposes an online method for speaker detection, speaker direction tracking, and speech separation. The separation is based on multiple acoustic source tracking (MAST) using Bayesian filtering and time–frequency masking. Measurements from three room environments with varying amounts of reverberation using two different designs of microphone arrays are used to evaluate the capability of the method to separate up to four simultaneously active speakers. Separation of moving talkers is also considered. Results are compared to two reference methods: ideal binary masking (IBM) and oracle tracking (O-T). Simulations are used to evaluate the effect of number of microphones and their spacing.",
    journal = "Computer Speech {\\&} Language",
    keywords = "Blind source separation;Acoustic source tracking;Particle filtering;Time-frequency masking;Microphone arrays;Spatial Sound Source Separation",
    month = "May",
    doi = "10.1016/j.csl.2012.08.003",
    number = "3",
    pages = "683–702",
    title = "Online Blind Speech Separation using Multiple Acoustic Speaker Tracking and Time-Frequency Masking",
    volume = "27",
    year = "2013"
}

@article{Pertilä2013_TASLP,
    author = {Pertil{\"a}, Pasi and H{\"a}m{\"a}l{\"a}inen, Matti S. and Mieskolainen, Mikael},
    abstract = "In recent years ad-hoc microphone arrays have become ubiquitous, and the capture hardware and quality is increasingly more sophisticated. Ad-hoc arrays hold a vast potential for audio applications, but they are inherently asynchronous, i.e., temporal offset exists in each channel, and furthermore the device locations are generally unknown. Therefore, the data is not directly suitable for traditional microphone array applications such as source localization and beamforming. This work presents a least squares method for temporal offset estimation of a static ad-hoc microphone array. The method utilizes the captured audio content without the need to emit calibration signals, provided that during the recording a sufficient amount of sound sources surround the array. The Cramer-Rao lower bound of the estimator is given and the effect of limited number of surrounding sources on the solution accuracy is investigated. A practical implementation is then presented using non-linear filtering with automatic parameter adjustment. Simulations over a range of reverberation and noise levels demonstrate the algorithm's robustness. Using smartphones an average RMS error of 3.5 samples (at 48 kHz) was reached when the algorithm's assumptions were met.",
    doi = "10.1109/TASLP.2013.2286921",
    issn = "1558-7916",
    journal = "IEEE Transactions on Audio, Speech, and Language Processing",
    keywords = "Acoustic measurement;Ad hoc networks;Calibration;Microphone arrays;Synchronization;self localization",
    month = "Nov.",
    number = "11",
    pages = "2393-2402",
    title = "Passive temporal offset estimation of multichannel recordings of an ad-hoc microphone array",
    volume = "21",
    year = "2013"
}

@inproceedings{Barker2014_ICASSP,
    author = "Barker, Tom and Virtanen, Tuomas and Delhomme, Olivier",
    booktitle = "2014 IEEE International Conference on Acoustic, Speech and Signal Processing (ICASSP), Florence, Italy, May 4-9.2014",
    doi = "10.1109/ICASSP.2014.6853975",
    isbn = "978-1-4799-2893-4",
    pages = "2148--2152",
    publisher = "Institute of Electrical and Electronics Engineers IEEE",
    title = "Ultrasound-Coupled Semi-Supervised Nonnegative Matrix Factorisation for Speech Enhancement",
    year = "2014",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/Barker\_ICASSP2014.pdf"
}

@inproceedings{Adavanne2017_DCASE2017,
    author = "Adavanne, Sharath and Virtanen, Tuomas",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    pages = "12--16",
    publisher = "Tampere University of Technology. Laboratory of Signal Processing",
    title = "Sound event detection using weakly labeled dataset with stacked convolutional and recurrent neural network",
    year = "2017",
    url = "https://arxiv.org/abs/1710.02998"
}

@ARTICLE{Klapuri2010_TASLP,
    author = "Klapuri, Anssi and Virtanen, Tuomas",
    journal = "IEEE Transactions on Audio, Speech, and Language Processing",
    title = "Representing Musical Sounds With an Interpolating State Model",
    year = "2010",
    volume = "18",
    number = "3",
    pages = "613-624",
    keywords = "Hidden Markov models;Signal processing algorithms;Vector quantization;Clustering algorithms;Audio coding;Signal processing;Multidimensional systems;Instruments;Computational modeling;Computational complexity;Acoustic signal processing;audio coding;discrete cosine transforms (DCTs);interpolation;vector quantization",
    doi = "10.1109/TASL.2010.2040781",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/interpolating.pdf"
}

@article{{Carabias Orti}2018_TASLP,
    author = "{Carabias Orti}, {Julio Jose} and Nikunen, Joonas and Virtanen, Tuomas and Vera-Candeas, Pedro",
    abstract = "This paper presents an algorithm for multichannel sound source separation using explicit modeling of level and time differences in source spatial covariance matrices (SCM). We propose a novel SCM model in which the spatial properties are modeled by the weighted sum of direction of arrival (DOA) kernels. DOA kernels are obtained as the combination of phase and level difference covariance matrices representing both time and level differences between microphones for a grid of predefined source directions. The proposed SCM model is combined with the NMF model for the magnitude spectrograms. Opposite to other SCM models in the literature, in this work, source localization is implicitly defined in the model and estimated during the signal factorization. Therefore, no localization pre-processing is required. Parameters are estimated using complex-valued non-negative matrix factorization (CNMF) with both Euclidean distance and Itakura Saito divergence. Separation performance of the proposed system is evaluated using the two-channel SiSEC development dataset and four channels signals recorded in a regular room with moderate reverberation. Finally, a comparison to other state-of-the-art methods is performed, showing better achieved separation performance in terms of SIR and perceptual measures.",
    day = "26",
    doi = "10.1109/TASLP.2018.2830105",
    issn = "2329-9290",
    journal = "IEEE-ACM Transactions on Audio Speech and Language Processing",
    keywords = "Covariance matrices;direction of arrival estimation;Direction-of-arrival estimation;interaural level difference;interaural time difference;Kernel;Microphones;multichannel source separation;non-negative matrix factorization;Source separation;spati;spatial\_s",
    month = "4",
    publisher = "IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC",
    title = "Multichannel Blind Sound Source Separation using Spatial Covariance Model with Level and Time Differences and Non-Negative Matrix Factorization",
    year = "2018",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/level-time\_SCM2018.pdf"
}

@INPROCEEDINGS{Pertilä2010_ICASSP,
    author = "Pertilä, Pasi and Hämäläinen, Matti S.",
    booktitle = "2010 IEEE International Conference on Acoustics, Speech and Signal Processing",
    title = "A track before detect approach for sequential Bayesian tracking of multiple speech sources",
    year = "2010",
    volume = "",
    number = "",
    pages = "4974-4977",
    keywords = "Bayesian methods;Speech;Acoustic signal detection;Particle filters;Target tracking;Particle tracking;Acoustic measurements;Signal to noise ratio;Filtering;Particle measurements;Acoustic Tracking;Multiple Sources;Particle Filters;Likelihood Ratio;Track Management",
    doi = "10.1109/ICASSP.2010.5495092"
}

@INPROCEEDINGS{Klapuri2010_ICASSP,
    author = "Klapuri, Anssi and Virtanen, Tuomas and Heittola, Toni",
    booktitle = "2010 IEEE International Conference on Acoustics, Speech and Signal Processing",
    title = "Sound source separation in monaural music signals using excitation-filter model and {EM} algorithm",
    year = "2010",
    volume = "",
    number = "",
    pages = "5510-5513",
    keywords = "Source separation;Multiple signal classification;Music;Instruments;Frequency estimation;Power harmonic filters;Maximum likelihood estimation;Image analysis;Humans;Layout;Sound source separation;excitation-filter model;maximum likelihood estimation;expectation maximization",
    doi = "10.1109/ICASSP.2010.5495216",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/em-nmf.pdf"
}

@INPROCEEDINGS{Gemmeke2010_ICASSP,
    author = "Gemmeke, Jort F. and Virtanen, Tuomas",
    booktitle = "2010 IEEE International Conference on Acoustics, Speech and Signal Processing",
    title = "Noise robust exemplar-based connected digit recognition",
    year = "2010",
    volume = "",
    number = "",
    pages = "4546-4549",
    keywords = "Noise robustness;Speech enhancement;Speech recognition;Automatic speech recognition;Hidden Markov models;Decoding;Signal to noise ratio;Background noise;Speech processing;Signal processing;Speech recognition;exemplar-based;noise robustness;non-negative matrix factorization;sparsity",
    doi = "10.1109/ICASSP.2010.5495580",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/ICASSP2010\_final.pdf"
}

@INPROCEEDINGS{Mesaros2010_ICASSP,
    author = "Mesaros, Annamaria and Virtanen, Tuomas",
    booktitle = "2010 IEEE International Conference on Acoustics, Speech and Signal Processing",
    title = "Recognition of phonemes and words in singing",
    year = "2010",
    volume = "",
    number = "",
    pages = "2146-2149",
    keywords = "Speech recognition;Hidden Markov models;Natural languages;Databases;Music information retrieval;Automatic speech recognition;Text recognition;Signal processing;System testing;Information analysis;singing recognition;speech recognition;query-by-singing",
    doi = "10.1109/ICASSP.2010.5495585",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/singrec.pdf"
}

@INPROCEEDINGS{Nikunen2010_ICASSP,
    author = "Nikunen, Joonas and Virtanen, Tuomas",
    booktitle = "2010 IEEE International Conference on Acoustics, Speech and Signal Processing",
    title = "Noise-to-mask ratio minimization by weighted non-negative matrix factorization",
    year = "2010",
    volume = "",
    number = "",
    pages = "25-28",
    keywords = "Signal to noise ratio;Acoustic noise;Signal processing algorithms;Audio coding;Signal processing;Psychoacoustic models;Masking threshold;Noise measurement;Nuclear magnetic resonance;Filter bank;Non-negative matrix factorization;Noise-to-mask ratio;Audio coding;Signal representations",
    doi = "10.1109/ICASSP.2010.5496264",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/nikunen.pdf"
}

@inproceedings{Mesaros2019_WASPAA,
    author = "Mesaros, Annamaria and Adavanne, Sharath and Politis, Archontis and Heittola, Toni and Virtanen, Tuomas",
    abstract = "Sound event detection and sound localization or tracking have historically been two separate areas of research. Recent development of sound event detection methods approach also the localization side, but lack a consistent way of measuring the joint performance of the system; instead, they measure the separate abilities for detection and for localization. This paper proposes augmentation of the localization metrics with a condition related to the detection, and conversely, use of location information in calculating the true positives for detection. An extensive evaluation example is provided to illustrate the behavior of such joint metrics. The comparison to the detection only and localization only performance shows that the proposed joint metrics operate in a consistent and logical manner, and characterize adequately both aspects.",
    booktitle = "2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)",
    doi = "10.1109/WASPAA.2019.8937220",
    isbn = "978-1-7281-1124-7",
    keywords = "Sound event detection and localization; performance evaluation",
    month = "10",
    pages = "333--337",
    publisher = "IEEE",
    series = "IEEE Workshop on Applications of Signal Processing to Audio and Acoustics",
    title = "Joint Measurement of Localization and Detection of Sound Events",
    year = "2019",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/mesaros\_Joint\_localization\_and\_detection\_WASPAA2019.pdf"
}

@inproceedings{Drossos2019_WASPAA,
    author = "Drossos, Konstantinos and Magron, Paul and Virtanen, Tuomas",
    abstract = "A challenging problem in deep learning-based machine listening field is the degradation of the performance when using data from unseen conditions. In this paper we focus on the acoustic scene classification (ASC) task and propose an adversarial deep learning method to allow adapting an acoustic scene classification system to deal with a new acoustic channel resulting from data captured with a different recording device. We build upon the theoretical model of HΔH-distance and previous adversarial discriminative deep learning method for ASC unsupervised domain adaptation, and we present an adversarial training based method using the Wasserstein distance. We improve the state-of-the-art mean accuracy on the data from the unseen conditions from 32{\\%} to 45{\\%}, using the TUT Acoustic Scenes dataset.",
    booktitle = "2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)",
    day = "22",
    doi = "10.1109/WASPAA.2019.8937231",
    isbn = "978-1-7281-1124-7",
    keywords = "Acoustic scene classification; unsupervised domain adaptation; Wasserstein distance; adversarial training",
    month = "10",
    publisher = "IEEE",
    series = "IEEE Workshop on Applications of Signal Processing to Audio and Acoustics",
    title = "Unsupervised Adversarial Domain Adaptation Based On The Wasserstein Distance For Acoustic Scene Classification",
    year = "2019",
    url = "https://arxiv.org/abs/1904.10678"
}

@inproceedings{Adavanne2019_DCASE2019,
    author = "Adavanne, Sharath and Politis, Archontis and Virtanen, Tuomas",
    abstract = "This paper investigates the joint localization, detection, and tracking of sound events using a convolutional recurrent neural network (CRNN). We use a CRNN previously proposed for the localization and detection of stationary sources, and show that the recurrent layers enable the spatial tracking of moving sources when trained with dynamic scenes. The tracking performance of the CRNN is compared with a stand-alone tracking method that combines a multi-source (DOA) estimator and a particle filter. Their respective performance is evaluated in various acoustic conditions such as anechoic and reverberant scenarios, stationary and moving sources at several angular velocities, and with a varying number of overlapping sources. The results show that the CRNN manages to track multiple sources more consistently than the parametric method across acoustic scenarios, but at the cost of higher localization error.",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019)",
    month = "10",
    pages = "20--24",
    title = "Localization, Detection and Tracking of Multiple Moving Sound Sources with a Convolutional Recurrent Neural Network",
    year = "2019",
    url = "https://arxiv.org/abs/1904.12769"
}

@inproceedings{Parviainen2009_ISCE,
    author = "Parviainen, M.",
    booktitle = "Proceedings of the 13th IEEE International Symposium on consumer Electronics, ISCE 2009, Kyoto, Japan, 25-28 May 2009",
    doi = "10.1109/ISCE.2009.5156957",
    isbn = "978-1-4244-2976-9",
    pages = "237--240",
    title = "Robust self-localization solutions for meeting room environments",
    year = "2009"
}

@article{Koppinen2004_SP,
    author = "Koppinen, Konsta",
    issn = "0165-1684",
    journal = "Signal Processing",
    number = "3",
    pages = "549--560",
    publisher = "Elsevier",
    title = "{A}nalysis of the asymptotic impulse and frequency responses of polynomial predictors",
    volume = "84",
    year = "2004"
}

@article{Hurmalainen2013,
    author = "Hurmalainen, Antti and Gemmeke, Jort and Virtanen, Tuomas",
    abstract = "Speech recognition systems intended for everyday use must be able to cope with a large variety of noise types and levels, including highly non-stationary multi-source mixtures. This study applies spectral factorisation algorithms and long temporal context for separating speech and noise from mixed signals. To adapt the system to varying environments, noise models are acquired from the context, or learnt from the mixture itself without prior information. We also propose methods for reducing the size of the bases used for speech and noise modelling by 20-40 times for better practical applicability. We evaluate the performance of the methods both as a standalone classifier and as a signal-enhancing front-end for external recognisers. For the CHiME noisy speech corpus containing non-stationary multi-source household noises at signal-to-noise ratios ranging from +9 to -6 dB, we report average keyword recognition rates up to 87.8\% using a single-stream sparse classification algorithm.",
    journal = "Computer Speech {\\&} Language",
    keywords = "automatic speech recognition;noise robustness;non-stationary noise;non-negative spectral factorisation;exemplar-based",
    month = "May",
    number = "3",
    pages = "763-779",
    title = "Modelling Non-stationary Noise with Spectral Factorisation in Automatic Speech Recognition",
    doi = "10.1016/j.csl.2012.07.008",
    volume = "27",
    year = "2013",
    rul = "http://www.sciencedirect.com/science/article/pii/S0885230812000563"
}

@INPROCEEDINGS{Virtanen2002_ICASSP,
    author = "Virtanen, Tuomas and Klapuri, Anssi",
    booktitle = "2002 IEEE International Conference on Acoustics, Speech, and Signal Processing",
    title = "Separation of harmonic sounds using linear models for the overtone series",
    year = "2002",
    volume = "2",
    number = "",
    pages = "II-1757-II-1760",
    keywords = "Laboratories;Transforms;Polynomials;Smoothing methods;Harmonic analysis",
    doi = "10.1109/ICASSP.2002.5744962",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/icassp2002.pdf"
}

@INPROCEEDINGS{Peltonen2002_ICASSP,
    author = "Peltonen, Vesa and Tuomi, Juha and Klapuri, Anssi and Huopaniemi, Jyri and Sorsa, Timo",
    booktitle = "2002 IEEE International Conference on Acoustics, Speech, and Signal Processing",
    title = "Computational auditory scene recognition",
    year = "2002",
    volume = "2",
    number = "",
    pages = "II-1941-II-1944",
    keywords = "Roads;Libraries;Artificial neural networks;Mel frequency cepstral coefficient;Vehicles;Rail transportation",
    doi = "10.1109/ICASSP.2002.5745009"
}

@ARTICLE{Orti2011_JSTSP,
    author = "Orti, Julio Jos{\'e} Carabias and Virtanen, Tuomas and Vera-Candeas, P. and Ruiz-Reyes, N. and Canadas-Quesada, F.J.",
    journal = "IEEE Journal of Selected Topics in Signal Processing",
    title = "Musical Instrument Sound Multi-Excitation Model for Non-Negative Spectrogram Factorization",
    year = "2011",
    volume = "5",
    number = "6",
    pages = "1144-1158",
    keywords = "Instruments;Harmonic analysis;Adaptation model;Music;Computational modeling;Reliability;Training;Automatic music transcription;excitation-filter model;excitation modeling;non-negative matrix factorization (NMF);source-filter model;spectral analysis",
    doi = "10.1109/JSTSP.2011.2159700",
    url = "http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=\&arnumber=5887381"
}

@inproceedings{Yli-Hietanen2002_DSP,
    author = "Yli-Hietanen, Jari and Saarelainen, Teemu",
    editor = "Skodras, A.N. and Constantinides, A.G.",
    booktitle = "DSP2002, 14th International Conference on Digital Signal Processing Proceedings, July 1-3, 2002, Santorini, Greece",
    pages = "239--242",
    title = "Analysis of robust time-delay based angle-of-arrival estimation methods",
    year = "2002"
}

@INPROCEEDINGS{Mökinen2011_EAIS,
    author = "Mökinen, Toni and Kiranyaz, Serkan and Gabbouj, Moncef",
    booktitle = "2011 IEEE Workshop on Evolving and Adaptive Intelligent Systems (EAIS)",
    title = "Content-based audio classification using collective network of binary classifiers",
    year = "2011",
    volume = "",
    number = "",
    pages = "116-123",
    keywords = "Training;Feature extraction;Neurons;Hidden Markov models;Databases;Accuracy;Computer architecture;audio content - based classification;evolutionary neural networks;particle swarm optimization;multilayer perceptron",
    doi = "10.1109/EAIS.2011.5945911"
}

@INPROCEEDINGS{Hurmalainen2011_ICASSP,
    author = "Hurmalainen, Antti and Gemmeke, Jort and Virtanen, Tuomas",
    booktitle = "2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    title = "Non-negative matrix deconvolution in noise robust speech recognition",
    year = "2011",
    volume = "",
    number = "",
    pages = "4588-4591",
    keywords = "Speech;Noise;Speech recognition;Dictionaries;Deconvolution;Noise measurement;Hidden Markov models;Automatic speech recognition;noise robustness;deconvolution;sparsity;exemplar-based",
    doi = "10.1109/ICASSP.2011.5947376",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/NMD\_icassp2011.pdf"
}

@ARTICLE{Helander2012_TASLP,
    author = "Helander, Elina and Silen, Hanna and Virtanen, Tuomas and Gabbouj, Moncef",
    journal = "IEEE Transactions on Audio, Speech, and Language Processing",
    title = "Voice Conversion Using Dynamic Kernel Partial Least Squares Regression",
    year = "2012",
    volume = "20",
    number = "3",
    pages = "806-817",
    keywords = "Kernel;Speech;Hidden Markov models;Data models;Training data;Statistical analysis;Training;Kernel methods;partial least squares regression;voice conversion",
    doi = "10.1109/TASL.2011.2165944",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/voice-conversion-TASLP.pdf"
}

@inproceedings{Stolcke2004_ICASSP,
    author = "Stolcke, A. and Wooters, C. and Mirghafori, N. and Pirinen, Tuomo and Bulyko, I. and Gelbart, D. and Graciarena, M. and Otterson, S. and Peskin, B. and Ostendorf, M.",
    booktitle = "Proceedings of NIST ICASSP 2004 Meeting Recognition Workshop, Montreal, Canada, 17 May 2004",
    pages = "7 p",
    title = "{P}rogress in meeting recognition: {T}he {ICSI}-{SRI}-{UW} spring 2004 evaluation system",
    year = "2004"
}

@article{Richard2017_TASLP,
    author = "Richard, G. and Virtanen, T. and Bello, J. P. and Ono, N. and Glotin, H.",
    abstract = "The papers in this special section are devoted to the growing field of acoustic scene classification and acoustic event recognition. Machine listening systems still have difficulties to reach the ability of human listeners in the analysis of realistic acoustic scenes. If sustained research efforts have been made for decades in speech recognition, speaker identification and to a lesser extent in music information retrieval, the analysis of other types of sounds, such as environmental sounds, is the subject of growing interest from the community and is targeting an ever increasing set of audio categories. This problem appears to be particularly challenging due to the large variety of potential sound sources in the scene, which may in addition have highly different acoustic characteristics, especially in bioacoustics. Furthermore, in realistic environments, multiple sources are often present simultaneously, and in reverberant conditions.",
    issn = "2329-9290",
    journal = "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
    month = "6",
    number = "6",
    pages = "1169--1171",
    publisher = "IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC",
    title = "Introduction to the Special Section on Sound Scene and Event Analysis",
    volume = "25",
    year = "2017",
    doi = "10.1109/TASLP.2017.2699334"
}

@inproceedings{Popa2009_InterSpecch,
    author = "Popa, Victor and Nurminen, Jani and Gabbouj, Moncef",
    booktitle = "Proceedings of the 10th Annual Conference of the International Speech Communication Associationa, Interspeech 2009, Brighton, UK, 6-10 September 2009",
    pages = "2655--2658",
    title = "A novel technique for voice conversion based on style and content decomposition with bilinear models",
    year = "2009"
}

@article{Drgas2017_TASLP,
    author = {Drgas, Szymon and Virtanen, Tuomas and L{\"u}cke, J{\"o}rg and Hurmalainen, Antti},
    abstract = "In this study, we propose an unsupervised method for dictionary learning in audio signals. The new method, called binary nonnegative matrix deconvolution (BNMD), is developed and used to discover patterns from magnitude-scale spectrograms. The BNMD models an audio spectrogram as a sum of delayed patterns having binary gains (activations). Only small subsets of patterns can be active for a given spectrogram excerpt. The proposed method was applied to speaker identification and separation tasks. The experimental results show that dictionaries obtained by the BNMD bring much higher speaker identification accuracies averaged over a range of SNRs from -6 dB to 9 dB (91.3\%) than the NMD-based dictionaries (37.8-75.4\%). The BNMD also gives a benefit over dictionaries obtained using vector quantization (87.8\%). For bigger dictionaries the difference between the BNMD and the vector quantization (VQ) is getting smaller. For the speech separation task the BNMD dictionary gave a slight improvement over the VQ.",
    doi = "10.1109/TASLP.2017.2709909",
    issn = "2329-9290",
    journal = "IEEE-ACM Transactions on Audio Speech and Language Processing",
    keywords = "Sparse coding;speaker recognition;speech separation",
    month = "8",
    number = "8",
    pages = "1644--1656",
    publisher = "IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC",
    title = "Binary Non-Negative Matrix Deconvolution for Audio Dictionary Learning",
    volume = "25",
    year = "2017",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/BNMD.pdf"
}

@inproceedings{Klapuri2008_SP,
    author = "Klapuri, Anssi and Virtanen, Tuomas",
    editor = "Havelock, D.",
    booktitle = ", Handbook of Signal Processing in Acoustics",
    isbn = "978-0-387-77698-9",
    pages = "277--303",
    publisher = "Springer",
    title = "Automatic music transcription",
    year = "2008"
}

@phdthesis{Mesaros2012_phd,
    author = "Mesaros, Annamaria",
    abstract = "This thesis proposes signal processing methods for analysis of singing voice audio signals, with the objectives of obtaining information about the identity and lyrics content of the singing. Two main topics are presented, singer identification in monophonic and polyphonic music, and lyrics transcription and alignment. The information automatically extracted from the singing voice is meant to be used for applications such as music classification, sorting and organizing music databases, music information retrieval, etc. For singer identification, the thesis introduces methods from general audio classification and specific methods for dealing with the presence of accompaniment. The emphasis is on singer identification in polyphonic audio, where the singing voice is present along withmusical accompaniment. The presence of instruments is detrimental to voice identification performance, and eliminating the effect of instrumental accompaniment is an important aspect of the problem. The study of singer identification is centered around the degradation of classification performance in presence of instruments, and separation of the vocal line for improving performance. For the study, monophonic singing was mixed with instrumental accompaniment at different signalto- noise (singing-to-accompaniment) ratios and the classification process was performed on the polyphonic mixture and on the vocal line separated from the polyphonic mixture. The method for classification including the step for separating the vocals is improving significantly the performance compared to classification of the polyphonic mixtures, but not close to the performance in classifying the monophonic singing itself. Nevertheless, the results show that classification of singing voices can be done robustly in polyphonic music when using source separation. In the problem of lyrics transcription, the thesis introduces the general speech recognition framework and various adjustments that can be done before applying themethods on singing voice. The variability of phonation in singing poses a significant challenge to the speech recognition approach. The thesis proposes using phoneme models trained on speech data and adapted to singing voice characteristics for the recognition of phonemes and words from a singing voice signal. Language models and adaptation techniques are an important aspect of the recognition process. There are two different ways of recognizing the phonemes in the audio: one is alignment, when the true transcription is known and the phonemes have to be located, other one is recognition, when both transcription and location of phonemes have to be found. The alignment is, obviously, a simplified form of the recognition task. Alignment of textual lyrics to music audio is performed by aligning the phonetic transcription of the lyrics with the vocal line separated from the polyphonic mixture, using a collection of commercial songs. The word recognition is tested for transcription of lyrics from monophonic singing. The performance of the proposed system for automatic alignment of lyrics and audio is sufficient for facilitating applications such as automatic karaoke annotation or song browsing. The word recognition accuracy of the lyrics transcription from singing is quite low, but it is shown to be useful in a query-by-singing application, for performing a textual search based on the words recognized from the query. When some key words in the query are recognized, the song can be reliably identified.",
    title = "{S}inging {V}oice {R}ecognition for {M}usic {I}nformation {R}etrieval",
    url = "https://tutcris.tut.fi/portal/files/5346343/mesaros.pdf",
    year = "2012",
    school = "Tampere University of Technology"
}

@inproceedings{Mesaros2013_WIA2MIS,
    author = {Mesaros, Annamaria and Heittola, Toni and Palom{\"a}ki, Kalle},
    abstract = "This paper presents a method for combining audio similarity and semantic similarity into a single similarity measure for query-by-example retrieval. The integrated similarity measure is used to retrieve sound events that are similar in content to the given query and have labels containing similar words. Through the semantic component, the method is able to handle variability in labels of sound events. Through the acoustic component, the method retrieves acoustically similar examples. On a test database of over 3000 sound event examples, the proposed method obtains a better retrieval performance than audio-based retrieval, and returns results closer acoustically to the query than a label-based retrieval.",
    booktitle = "14th International Workshop on Image and Audio Analysis for Multimedia Interactive Services (WIA2MIS)",
    pages = "1-4",
    title = "Query-by-example retrieval of sound events using an integrated similarity measure of content and label",
    year = "2013"
}

@inproceedings{Magron2018_ICASSP,
    author = "Magron, Paul and Virtanen, Tuomas",
    abstract = "In audio source separation applications, it is common to model the sources as circular-symmetric Gaussian random variables, which is equivalent to assuming that the phase of each source is uniformly distributed. In this paper, we introduce an anisotropic Gaussian source model in which both the magnitude and phase parameters are modeled as random variables. In such a model, it becomes possible to promote a phase value that originates from a signal model and to adjust the relative importance of this underlying model-based phase constraint. We conduct Bayesian inference of the model through the derivation of an expectation-maximization algorithm for estimating the parameters. Experiments conducted on realistic music songs for a monaural source separation task, in an scenario where the variance parameters are assumed known, show that the proposed approach outperforms state-of-the-art techniques.",
    booktitle = "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    doi = "10.1109/ICASSP.2018.8461741",
    keywords = "source separation",
    month = "4",
    publisher = "IEEE",
    title = "Bayesian anisotropic {G}aussian model for audio source separation",
    year = "2018",
    url = "https://hal.archives-ouvertes.fr/hal-01632081/document"
}

@inproceedings{Cakir2017_EUSIPCO,
    author = "Cakir, Emre and Adavanne, Sharath and Parascandolo, Giambattista and Drossos, Konstantinos and Virtanen, Tuomas",
    booktitle = "European Signal Processing Conference",
    doi = "10.23919/EUSIPCO.2017.8081508",
    pages = "1744--1748",
    publisher = "IEEE",
    series = "European Signal Processing Conference",
    title = "Convolutional recurrent neural networks for bird audio detection",
    year = "2017",
    url = "https://arxiv.org/pdf/1703.02317.pdf"
}

@inproceedings{Mesaros2018_IWAENC,
    author = "Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas",
    abstract = "We present an overview of the challenge entries for the Acoustic Scene Classification task of DCASE 2017 Challenge. Being the most popular task of the challenge, acoustic scene classification entries provide a wide variety of approaches for comparison, with a wide performance gap from top to bottom. Analysis of the submissions confirms once more the popularity of deep-learning approaches and mel frequency representations. Statistical analysis indicates that the top ranked system performed significantly better than the others, and that combinations of top systems are capable of reaching close to perfect performance on the given data.",
    booktitle = "16th International Workshop on Acoustic Signal Enhancement, IWAENC 2018",
    day = "2",
    doi = "10.1109/IWAENC.2018.8521242",
    keywords = "Acoustic scene classification; Audio classb ification; DCASE challenge",
    month = "11",
    pages = "411--415",
    publisher = "IEEE",
    title = "Acoustic scene classification: {A}n overview of {DCASE} 2017 challenge entries",
    year = "2018",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/mesaros-iwaenc2018-asc-in-dcase2017.pdf"
}

@mastersthesis{Löytynoja2009_master,
    author = {L{\"o}ytynoja, Antti},
    title = "{R}eal-time {P}article {F}ilter {B}ased {T}alker {L}ocalization {U}sing {P}ure {D}ata",
    year = "2009",
    school = "Tampere University of Technology"
}

@inproceedings{Viitaniemi2003_FINSIG,
    author = "Viitaniemi, Timo and Klapuri, Anssi and Eronen, Antti",
    editor = "Huttunen, H. and Gotchev, A. and Vasilache, A.",
    booktitle = "Proceedings of the 2003 Finnish Signal Processing Symposium, FINSIG'03, Tampere, Finland, 19 May 2003",
    pages = "59--63",
    publisher = "TICSP",
    title = "{A} probabilistic model for the transcription of single-voice melodies",
    year = "2003"
}

@INPROCEEDINGS{Hurmalainen2012_ICASSP,
    author = "Hurmalainen, Antti and Virtanen, Tuomas",
    booktitle = "2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    title = "Modelling spectro-temporal dynamics in factorisation-based noise-robust automatic speech recognition",
    year = "2012",
    volume = "",
    number = "",
    pages = "4113-4116",
    keywords = "Speech;Vectors;Noise;Speech recognition;Spectrogram;Feature extraction;Noise measurement;Automatic speech recognition;exemplar-based;spectral factorisation;noise robustness",
    doi = "10.1109/ICASSP.2012.6288823",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/hurmalainen\_icassp2012.pdf"
}

@inproceedings{Tervo2010_ICASSP,
    author = "Tervo, Sakari and Korhonen, Teemu",
    booktitle = "Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing ICASSP, Dallas, Texas, USA, March 14-19, 2010",
    doi = "10.1109/ICASSP.2010.5496104",
    isbn = "978-1-4244-4296-6",
    pages = "153--156",
    title = "Estimation of reflective surfaces from continuous signals",
    year = "2010"
}

@article{Magron2020_SP,
    author = "Magron, Paul and Virtanen, Tuomas",
    abstract = "Audio source separation is usually achieved by estimating the short-time Fourier transform (STFT) magnitude of each source, and then applying a spectrogram inversion algorithm to retrieve time-domain signals. In particular, the multiple input spectrogram inversion (MISI) algorithm has been exploited successfully in several recent works. However, this algorithm suffers from two drawbacks, which we address in this letter. First, it has originally been introduced in a heuristic fashion: we propose here a rigorous optimization framework in which MISI is derived, thus proving the convergence of this algorithm. Besides, while MISI operates offline, we propose here an online version of MISI called oMISI, which is suitable for low-latency source separation, an important requirement for e.g., hearing aids applications. oMISI also allows one to use alternative phase initialization schemes exploiting the temporal structure of audio signals. Experiments conducted on a speech separation task show that oMISI performs as well as its offline counterpart, thus demonstrating its potential for real-time source separation.",
    doi = "10.1109/LSP.2020.2970310",
    issn = "1070-9908",
    journal = "IEEE Signal Processing Letters",
    keywords = "Audio source separation; low-latency; online spectrogram inversion; phase recovery; sinusoidal modeling",
    pages = "306--310",
    publisher = "Institute of Electrical and Electronics Engineers",
    title = "Online Spectrogram Inversion for Low-Latency Audio Source Separation",
    volume = "27",
    year = "2020",
    url = "https://arxiv.org/abs/1911.03128"
}

@mastersthesis{Parviainen2003_master,
    author = "Parviainen, Mikko",
    title = {{\"{A}}{\"a}nil{\"a}hteiden erottelu luonnollisissa ymp{\"a}rist{\"o}iss{\"a} k{\"a}ytt{\"a}en kahta sensoria},
    type = "Master's Thesis",
    year = "2003",
    school = "Tampere University of Technology"
}

@article{Parviainen2017_AA,
    author = {Parviainen, Mikko and Pertil{\"a}, Pasi},
    abstract = "Abstract The increase of mobile devices and most recently wearables has raised the interest to utilize their sensors for various applications such as indoor localization. We present the first acoustic self-localization scheme that is passive, and is capable of operating when sensors are moving, and possibly unsynchronized. As a result, the relative microphone positions are obtained and therefore an ad hoc microphone array has been established. The proposed system takes advantage of the knowledge that a device is worn by its user e.g. attached to his/her clothing. A user here acts as a sound source and the sensor is the user-worn microphone. Such an entity is referred to as a node. Node-related spatial information is obtained from Time Difference of Arrival (TDOA) estimated from audio captured by the nodes. Kalman filtering is used for node tracking and prediction of spatial information during periods of node silence. Finally, the node positions are recovered using multidimensional scaling (MDS). The only information required by the proposed system is observations of sounds produced by the nodes such as speech to localize the moving nodes. The general framework for acoustic self-localization is presented followed by an implementation to demonstrate the concept. Real data collected by off-the-shelf equipment is used to evaluate the positioning accuracy of nodes in contrast to image based method. The presented system achieves an accuracy of approximately 10 cm in an acoustic laboratory.",
    doi = "http://dx.doi.org/10.1016/j.apacoust.2016.10.019",
    issn = "0003-682X",
    journal = "Applied Acoustics",
    keywords = "Self-localization; Ad hoc networks; Microphone arrays; Acoustic measurements; Kalman filtering; Data association",
    month = "February",
    pages = "76 - 85",
    title = "{S}elf-localization of dynamic user-worn microphones from observed speech",
    volume = "Volume 117, Part A",
    year = "2017"
}

@ARTICLE{Virtanen2013_TASLP,
    author = "Virtanen, Tuomas and Gemmeke, Jort Florent and Raj, Bhiksha",
    journal = "IEEE Transactions on Audio, Speech, and Language Processing",
    title = "Active-Set Newton Algorithm for Overcomplete Non-Negative Representations of Audio",
    year = "2013",
    volume = "21",
    number = "11",
    pages = "2277-2289",
    keywords = "Large scale systems;Pattern recognition;Optimization;Acoustic signal analysis;Source separation;Acoustic signal analysis;audio source separation;convex optimization;Newton algorithm;non-negative matrix factorization;sparse coding;sparse representation;supervised source separation",
    doi = "10.1109/TASL.2013.2263144",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/asna.pdf"
}

@mastersthesis{Silén2007_master,
    author = "Sil{\'e}n, Hanna",
    abstract = "Concatenative text-to-speech (TTS) synthesis is a method for artificial speech generation that utilizes a pre-recorded natural speech inventory. The intelligibility of modern TTS systems is considered relatively high thus the current emphasis is on developing the naturalness of the synthetic speech. Unit selection speech synthesis utilizes a large pre-recorded speech inventory which provides a sufficient phonetic and prosodic coverage for the language. In synthesis, the best sequence of units, typically half phones or diphones, is retrieved from the inventory and the concatenation of the units is carried out. Unlike in traditional concatenative TTS systems based on prosodic modification, no processing of unit waveforms is required. This increases the quality of the speech by avoiding the modification of units. In this thesis, the full process of constructing a TUT\_VOICE unit selection TTS synthesizer for Finnish is described. The work consisted of the construction of a Finnish voice and the implementation of a synthesis engine. The voice construction included inventory design, recording of a speech inventory with a female voice, phonetic labeling of the speech, and feature extraction. The implementation included building a target unit sequence based on the text to be synthesized, selecting the best sequence of speech units in the inventory, and creation of the output speech waveform by concatenating the selected units. Synthesis quality was in accordance with the expectations: intelligibility was good but quality varied from excellent to poor among different sentences.",
    title = "{D}esign and {I}mplementation of a {F}innish {U}nit {S}election {S}peech {S}ynthesizer",
    year = "2007",
    school = "Tampere University of Technology"
}

@INPROCEEDINGS{Nurminen2013_ISCAS,
    author = "Nurminen, Jani and Silén, Hanna and Helander, Elina and Gabbouj, Moncef",
    booktitle = "2013 IEEE International Symposium on Circuits and Systems (ISCAS)",
    title = "Evaluation of detailed modeling of the LP residual in statistical speech synthesis",
    year = "2013",
    volume = "",
    number = "",
    pages = "313-316",
    keywords = "Hidden Markov models;Speech;Speech synthesis;Harmonic analysis;Databases;Cutoff frequency;Training;statistical speech synthesis;linear prediction;residual modeling",
    doi = "10.1109/ISCAS.2013.6571844"
}

@inproceedings{Koppinen1996_NORSIG,
    author = "Koppinen, Konsta and Vainio, O. and Astola, Jaakko",
    address = "Espoo, Finland",
    booktitle = "Proc. IEEE Nordic Signal Processing Symposium",
    month = "Sep",
    pages = "45-48",
    title = "{A}nalysis and {D}esign of {P}olynomial {P}redictors",
    year = "1996"
}

@phdthesis{Koppinen2003_phd,
    author = "Koppinen, Konsta",
    abstract = "New methods for the design, analysis and generalization of polynomial predictive filters (polynomial predictors for short) are developed in this thesis. Polynomial predictors are a subclass of linear mathematical filters, i.e. linear transformations that transform an input sequence of numbers to an output sequence. Filters are in general used to modify or extract useful information from a signal. Examples of filters are echo cancellers in telephone networks, vocal tract filters in speech synthesizers and analysis filters for EEG signals. Polynomial predictors are useful in situations where the signal of interest changes relatively slowly. Examples of slowly varying signals that can be successfully modeled by polynomials include displacement curves of elevators, the received power level in mobile communication systems and angular accelaration in motor drives. Generally, the filtering of a signal causes delay. However, in many applications, particularly control systems, it is desirable to minimize the delay caused by filtering. Furthermore, the signals in any real-world application contain noise, which we also wish to minimize by filtering. For example, calculating the moving average of a signal generally reduces the noise in the signal, but also causes delay. Polynomial predictors are filters that can predict polynomial signals, and in addition reduce the noise in the signal. In this thesis we determine the precise class of IIR filters which are polynomial predictors, which unifies the various previously proposed predictor structures. Design methods are developed for optimal least-squares FIR polynomial predictors, which enable arbitrary prediction steps, error weighting and an arbitrary desired frequency response. The parameters of predictors based on the general IIR structure are optimized, yielding improvements over the best previously known IIR polynomial predictors. An analysis of the asymptotic properties of polynomial predictors as the filter length grows without bound is carried out, yielding insight into the characteristics of predictors in general, as well as proving a conjecture on the asymptotic behaviour of the noise gain. Predictors are generalized for more general signal models than polynomials, and the resulting signal model (corresponding to the solutions of linear homogenous finite difference equations) is shown to be the most general possible. The generalization of the signal model can be interpreted in terms of a z-plane diagram which specifies the complex exponential signals that are predicted. The design methods for FIR polynomial predictors are also carried over to this general signal model. The generalized signal model includes e.g. sinusoidal signals and the design methods are applied to finding improved filters for use in a 50Hz line frequency signal processing application. In addition to the signal model, the filter type is also generalized. This yields design methods for finding the optimal parameters of differentiators, predictive differentiators and integrators, among others. The asymptotic noise gain of generalized predictors is derived and found to depend on the largest modulus of the complex numbers which specify the signal model. Furthermore, an efficient implementation for generalized FIR predictors is derived.",
    title = "Design, Analysis and Generalization of Polynomial Predictive Filters",
    year = "2003",
    school = "Tampere University of Technology"
}

@INPROCEEDINGS{Pertilä2013_DSP,
    author = "Pertilä, Pasi and Tinakari, Aki",
    booktitle = "2013 18th International Conference on Digital Signal Processing (DSP)",
    title = "Time-of-arrival estimation for blind beamforming",
    year = "2013",
    volume = "",
    number = "",
    pages = "1-6",
    keywords = "Vectors;Kalman filters;Sensors;Array signal processing;Microphone arrays;Smart phones;Time of arrival estimation;Beam steering;Kalman filter;Speech enhancement",
    doi = "10.1109/ICDSP.2013.6622689"
}

@INPROCEEDINGS{Gemmeke2013_ICASSP,
    author = "Gemmeke, Jort F. and Virtanen, Tuomas and Demuynck, Kris",
    booktitle = "2013 IEEE International Conference on Acoustics, Speech and Signal Processing",
    title = "Exemplar-based joint channel and noise compensation",
    year = "2013",
    volume = "",
    number = "",
    pages = "868-872",
    keywords = "Speech;Noise;Speech recognition;Dictionaries;Noise robustness;Iron;Hidden Markov models;Speech recognition;source separation;matrix factorization;noise robustness;channel compensation",
    doi = "10.1109/ICASSP.2013.6637772",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/3645\_postprint.pdf"
}

@article{Bramsløw2018_ICA,
    author = "Bramsl{\o}w, Lars and Naithani, Gaurav and Hafez, Atefeh and Barker, Tom and Pontoppidan, Niels Henrik and Virtanen, Tuomas",
    title = "Improving competing voices segregation for hearing impaired listeners using a low-latency deep neural network algorithm",
    abstract = "Hearing aid users are challenged in listening situations with noise and especially speech-on-speech situations with two or more competing voices. Specifically, the task of attending to and segregating two competing voices is particularly hard, unlike for normal-hearing listeners, as shown in a small sub-experiment. In the main experiment, the competing voices benefit of a deep neural network (DNN) based stream segregation enhancement algorithm was tested on hearing-impaired listeners. A mixture of two voices was separated using a DNN and presented to the two ears as individual streams and tested for word score. Compared to the unseparated mixture, there was a 13\\%",
    journal = "The Journal of the Acoustical Society of America",
    volume = "144",
    number = "1",
    pages = "172--185",
    year = "2018",
    publisher = "AIP Publishing",
    doi = "doi.org/10.1121/1.5045322"
}

@INPROCEEDINGS{Briggs2013_MLSP,
    author = "Briggs, Forrest and Huang, Yonghong and Raich, Raviv and Eftaxias, Konstantinos and Lei, Zhong and Cukierski, William and Hadley, Sarah Frey and Hadley, Adam and Betts, Matthew and Fern, Xiaoli Z. and Irvine, Jed and Neal, Lawrence and Thomas, Anil and Fodor, Gábor and Tsoumakas, Grigorios and Ng, Hong Wei and Nguyen, Thi Ngoc Tho and Huttunen, Heikki and Ruusuvuori, Pekka and Manninen, Tapio and Diment, Aleksandr and Virtanen, Tuomas and Marzat, Julien and Defretin, Joseph and Callender, Dave and Hurlburt, Chris and Larrey, Ken and Milakov, Maxim",
    booktitle = "2013 IEEE International Workshop on Machine Learning for Signal Processing (MLSP)",
    title = "The 9th annual MLSP competition: New methods for acoustic classification of multiple simultaneous bird species in a noisy environment",
    year = "2013",
    volume = "",
    number = "",
    pages = "1-8",
    keywords = "Birds;Spectrogram;Vectors;Rain;Histograms;Image segmentation;Noise",
    doi = "10.1109/MLSP.2013.6661934"
}

@INPROCEEDINGS{Kauppinen2013_WASPAA,
    author = "Kauppinen, Joonas and Klapuri, Anssi and Virtanen, Tuomas",
    booktitle = "2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics",
    title = "Music self-similarity modeling using augmented nonnegative matrix factorization of block and stripe patterns",
    year = "2013",
    volume = "",
    number = "",
    pages = "1-4",
    keywords = "Feature extraction;Vectors;Signal processing algorithms;Music;Signal processing;Approximation methods;Music structure analysis;nonnegative matrix factorization;self-similarity",
    doi = "10.1109/WASPAA.2013.6701855",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/Kauppinen-WASPAA2013-final.pdf"
}

@mastersthesis{Matikainen2010_master,
    author = "Matikainen, Juha",
    abstract = "Loudspeaker is a device that converts electric input signal to acoustic output. The most common type of loudspeaker is a moving-coil transducer. The behaviour of a moving-coil transducer can be considered to be linear only when displacement of the coil-diaphragm assembly is small. When input signal level rises, nonlinearities start to cause audible distortion. In this thesis we examine microspeaker, a small loudspeaker used in mobile phones. The electro-mechanical process which converts the electrical signal into sound waves is exaplained. Based on this, we present a continuous-time, linear model of a loudspeaker mounted in a closed box. The model describes the loudspeaker’s small-signal behaviour using only few parameters. We then consider the main sources of nonlinearities and how to model them. Two major sources nonlinearities are added to the continuous-time model. Then transformations from continuous-time models to discrete-time models are considered. The nonlinear model is converted to discrete-time while taking into account the properties of the microspeaker. The main purpose of this thesis is to study performance of a algorithm that ﬁnds the parameter values of the nonlinear loudspeaker model. Performance of the algorithm is compared to performance of an earlier algorithm for the linear loudspeaker model. The parameter values are found and changes in them are tracked using an adaptive signal processing method called system identiﬁcation. The parameter values are updated using LMS algorithm. Since the discrete-time mechanical model of the microspeaker is based on a recursive ﬁlter, LMS algorithm for recursive ﬁlters is presented. We also review previous research related to parameter identiﬁcation in linear and nonlinear loudspeaker models. Based on results from the experiments the studied algorithm is deemed to be yet incomplete. Linear parameters adapt in general quickly whereas the nonlinear parameters adapt too slowly and sometimes erroneously. The diﬀerence between the output predicted by the nonlinear loudspeaker model and the actual output of the loudspeaker (prediction error) is too high, meaning the parameters do not adapt to their true values. The model is also prone to instability. The algorithm requires further development regarding adaptation speed and prevention of instability. Other development considering initial parameter values and operation during silent moments should also be conducted in the future.",
    title = "{P}arameter {A}daptation in {N}onlinear {L}oudspeaker {M}odels",
    year = "2010",
    school = "Tampere University of Technology"
}

@inproceedings{Parascandolo2016_ICASSP,
    author = "Parascandolo, Giambattista and Huttunen, Heikki and Virtanen, Tuomas",
    abstract = "In this paper we present an approach to polyphonic sound event detection in real life recordings based on bi-directional long short term memory (BLSTM) recurrent neural networks (RNNs). A single multilabel BLSTM RNN is trained to map acoustic features of a mixture signal consisting of sounds from multiple classes, to binary activity indicators of each event class. Our method is tested on a large database of real-life recordings, with 61 classes (e.g. music, car, speech) from 10 different everyday contexts. The proposed method outperforms previous approaches by a large margin, and the results are further improved using data augmentation techniques. Overall, our system reports an average F1-score of 65.5\% on 1 second blocks and 64.7\% on single frames, a relative improvement over previous state-of-the-art approach of 6.8\% and 15.1\% respectively.",
    booktitle = "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    doi = "10.1109/ICASSP.2016.7472917",
    isbn = "978-1-4799-9988-0",
    month = "3",
    pages = "6440--6444",
    title = "Recurrent Neural Networks for Polyphonic Sound Event Detection in Real Life Recordings",
    year = "2016",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/parascandolo-icassp2016.pdf"
}

@ARTICLE{Wu2014_TASLP,
    author = "Wu, Zhizheng and Virtanen, Tuomas and Chng, Eng Siong and Li, Haizhou",
    journal = "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
    title = "Exemplar-Based Sparse Representation With Residual Compensation for Voice Conversion",
    year = "2014",
    volume = "22",
    number = "10",
    pages = "1506-1521",
    keywords = "Speech;Speech processing;Spectrogram;Training;Vectors;IEEE transactions;Training data;Exemplar;nonnegative matrix factorization;residual compensation;sparse representation;voice conversion",
    doi = "10.1109/TASLP.2014.2333242",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/taslp\_voco\_exemplar\_2014.pdf"
}

@INPROCEEDINGS{Baby2014_ICASSP,
    author = "Baby, Deepak and Virtanen, Tuomas and Barker, Tom and Van hamme, Hugo",
    booktitle = "2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    title = "Coupled dictionary training for exemplar-based speech enhancement",
    year = "2014",
    volume = "",
    number = "",
    pages = "2883-2887",
    keywords = "Discrete Fourier transforms;Speech;Noise;Dictionaries;Speech enhancement;Noise measurement;Non-negative matrix factorisation;coupled dictionary training;speech enhancement;modulation envelope",
    doi = "10.1109/ICASSP.2014.6854127",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/Baby\_ICASSP2014.pdf"
}

@INPROCEEDINGS{Virtanen2014_ICASSP,
    author = "Virtanen, Tuomas and Raj, Bhiksha and Gemmeke, Jort F. and Van hamme, Hugo",
    booktitle = "2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    title = "Active-set newton algorithm for non-negative sparse coding of audio",
    year = "2014",
    volume = "",
    number = "",
    pages = "3092-3096",
    keywords = "Dictionaries;Vectors;Speech;Source separation;Sparse matrices;Signal processing algorithms;Encoding;sound source separation;non-negative matrix factorization;Newton algorithm;convex optimization;sparse coding",
    doi = "10.1109/ICASSP.2014.6854169",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/ASNA\_L1.pdf"
}

@inproceedings{Cakir2017_DCASE2017,
    author = "Cakir, Emre and Virtanen, Tuomas",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    pages = "27--31",
    publisher = "Tampere University of Technology. Laboratory of Signal Processing",
    title = "Convolutional Recurrent Neural Networks for Rare Sound Event Detection",
    year = "2017",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/DCASE2017Workshop\_Cakir\_105.pdf"
}

@inproceedings{Shuyang2018_IWAENC,
    author = "Shuyang, Zhao and Heittola, Toni and Virtanen, Tuomas",
    booktitle = "2018 16th International Workshop on Acoustic Signal Enhancement (IWAENC)",
    title = "An Active Learning Method Using Clustering and Committee-Based Sample Selection for Sound Event Classification",
    year = "2018",
    volume = "",
    number = "",
    pages = "116-120",
    keywords = "Labeling;Acoustics;Training;Predictive models;Clustering algorithms;Measurement;Process control;active learning;K-medoids clustering;committee-based sample selection;sound event classification",
    doi = "10.1109/IWAENC.2018.8521336"
}

@inproceedings{Niemistö2002_NORSIG,
    author = {Niemist{\"o}, Riitta and M{\"a}kel{\"a}, Tuomo},
    booktitle = "Proceedings of the 5th Nordic Signal Processing Symposium, NORSIG 2002, October 4-7, 2002, on board Hurtigruten, Norway",
    isbn = "82-993158-4-0",
    pages = "5 s",
    title = "Robust adaptive polynomial filters for acoustic echo cancellation",
    year = "2002"
}

@article{Naithani2018_JASM,
    author = {Naithani, Gaurav and Kivinummi, Jaana and Virtanen, Tuomas and Tammela, Outi and Peltola, Mikko J. and Lepp{\"a}nen, Jukka M.},
    abstract = "Automatic extraction of acoustic regions of interest from recordings captured in realistic clinical environments is a necessary preprocessing step in any cry analysis system. In this study, we propose a hidden Markov model (HMM) based audio segmentation method to identify the relevant acoustic parts of the cry signal (i.e., expiratory and inspiratory phases) from recordings made in natural environments with various interfering acoustic sources. We examine and optimize the performance of the system by using different audio features and HMM topologies. In particular, we propose using fundamental frequency and aperiodicity features. We also propose a method for adapting the segmentation system trained on acoustic material captured in a particular acoustic environment to a different acoustic environment by using feature normalization and semi-supervised learning (SSL). The performance of the system was evaluated by analyzing a total of 3 h and 10 min of audio material from 109 infants, captured in a variety of recording conditions in hospital wards and clinics. The proposed system yields frame-based accuracy up to 89.2\%. We conclude that the proposed system offers a solution for automated segmentation of cry signals in cry analysis applications.",
    doi = "10.1186/s13636-018-0124-x",
    issn = "1687-4714",
    journal = "Eurasip Journal on Audio, Speech, and Music Processing",
    keywords = "Acoustic analysis;Audio segmentation;Hidden Markov models;Infant cry analysis;Model adaptation",
    number = "1",
    publisher = "Springer Verlag",
    title = "Automatic segmentation of infant cry signals using hidden {M}arkov models",
    volume = "2018",
    year = "2018",
    url = "https://doi.org/10.1186/s13636-018-0124-x"
}

@inproceedings{Bilcu2007_ISPA,
    author = {Bilcu, Enik{\"o} Beatrice and Astola, Jaakko},
    editor = "Petrou, M.",
    booktitle = "Proceedings of the 5th International Symposium on Image and Signal Processing and Analysis, ISPA 2007, Istanbul, Turkey, 27-29 September 2007",
    isbn = "978-953-184-117-7",
    pages = "190--195",
    title = "{I}mproved hybrid approach for bilingual language recognition from text",
    year = "2007"
}

@ARTICLE{Virtanen2015_SPM,
    author = "Virtanen, Tuomas and Gemmeke, Jort Florent and Raj, Bhiksha and Smaragdis, Paris",
    journal = "IEEE Signal Processing Magazine",
    title = "Compositional Models for Audio Processing: Uncovering the structure of sound mixtures",
    year = "2015",
    volume = "32",
    number = "2",
    pages = "125-144",
    keywords = "Matrix decomposition;Spectrogram;Time-frequency analysis;Principal component analysis;Atomic clocks;Acoustic signal processing;Signal resolution",
    doi = "10.1109/MSP.2013.2288990",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/virtanen-spm-compositional.pdf"
}

@INPROCEEDINGS{Virtanen2009_EUSIPCO,
    author = "Virtanen, Tuomas",
    booktitle = "2009 17th European Signal Processing Conference",
    title = "Spectral covariance in prior distributions of non-negative matrix factorization based speech separation",
    year = "2009",
    volume = "",
    number = "",
    pages = "1933-1937",
    keywords = "Vectors;Speech;Training;Stability analysis;Testing",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/covariance/virtanen\_eusipco2009.pdf"
}

@INPROCEEDINGS{Myllymäki2009_EUSIPCO,
    author = "Myllymäki, Mikko and Virtanen, Tuomas",
    booktitle = "2009 17th European Signal Processing Conference",
    title = "Non-stationary noise model compensation in voice activity detection",
    year = "2009",
    volume = "",
    number = "",
    pages = "2186-2190",
    keywords = "Noise;Estimation;Adaptation models;Robustness;Hidden Markov models",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/myllymaki\_eusipco2009.pdf"
}

@INPROCEEDINGS{Baby2014_SLT,
    author = "Baby, Deepak and Virtanen, Tuomas and Gemmeke, Jort F. and Barker, Tom and Van hamme, Hugo",
    booktitle = "2014 IEEE Spoken Language Technology Workshop (SLT)",
    title = "Exemplar-based noise robust automatic speech recognition using modulation spectrogram features",
    year = "2014",
    volume = "",
    number = "",
    pages = "519-524",
    keywords = "Speech;Noise;Discrete Fourier transforms;Modulation;Dictionaries;Databases;Spectrogram;modulation envelope;coupled dictionaries;non-negative matrix factorisation;automatic speech recognition",
    doi = "10.1109/SLT.2014.7078628"
}

@INPROCEEDINGS{Klapuri1998_EUSIPCO 1998,
    author = "Klapuri, Anssi",
    booktitle = "9th European Signal Processing Conference (EUSIPCO 1998)",
    title = "Number theoretical means of resolving a mixture of several harmonic sounds",
    year = "1998",
    volume = "",
    number = "",
    pages = "1-5",
    keywords = "Harmonic analysis;Signal resolution;Robustness;Mathematical model;Probability;Signal processing algorithms;Hidden Markov models"
}

@INPROCEEDINGS{Yli-Hietanen1998_EUSIPCO 1998,
    author = "Yli-Hietanen, Jari and Koppinen, Konsta and Halonen, Katriina",
    booktitle = "9th European Signal Processing Conference (EUSIPCO 1998)",
    title = "Cluster filter",
    year = "1998",
    volume = "",
    number = "",
    pages = "1905-1907",
    keywords = "Delay effects;Robustness;Estimation;Sensor arrays;Signal processing;Delays"
}

@inproceedings{Parviainen2017_SiPS,
    author = {Parviainen, M. and Pertil{\"a}, P.},
    abstract = "This article presents a method to obtain personalized Head-Related Transfer Functions (HRTFs) for creating virtual soundscapes based on small amount of measurements. The best matching set of HRTFs are selected among the entries from publicly available databases. The proposed method is evaluated using a listening test where subjects assess the audio samples created using the best matching set of HRTFs against a randomly chosen set of HRTFs from the same location. The listening test indicates that subjects prefer the proposed method over random set of HRTFs.",
    booktitle = "2017 IEEE International Workshop on Signal Processing Systems (SiPS)",
    doi = "10.1109/SiPS.2017.8110008",
    keywords = "acoustic signal processing; audio signal processing; hearing; sound reproduction; transfer functions; HRTFs; head-related transfer functions; matching set; optimal set; publicly available databases; randomly chosen set; virtual soundscapes; Ear; Indexes; M",
    month = "10",
    publisher = "IEEE",
    title = "Obtaining an optimal set of head-related transfer functions with a small amount of measurements",
    year = "2017"
}

@INPROCEEDINGS{Barker2015_ICASSP,
    author = "Barker, Tom and Virtanen, Tuomas and Pontoppidan, Niels Henrik",
    booktitle = "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    title = "Low-latency sound-source-separation using non-negative matrix factorisation with coupled analysis and synthesis dictionaries",
    year = "2015",
    volume = "",
    number = "",
    pages = "241-245",
    keywords = "Dictionaries;Welding;Tin;Computational modeling;Analytical models;Discrete Fourier transforms;Mixture models;Non-negative matrix factorisation;NMF;source separation;real-time;low-latency",
    doi = "10.1109/ICASSP.2015.7177968",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/barker\_icassp2015.pdf"
}

@INPROCEEDINGS{Hurmalainen2015_ICASSP,
    author = "Hurmalainen, Antti and Saeidi, Rahim and Virtanen, Tuomas",
    booktitle = "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    title = "Similarity induced group sparsity for non-negative matrix factorisation",
    year = "2015",
    volume = "",
    number = "",
    pages = "4425-4429",
    keywords = "Speech;Atomic measurements;Speech recognition;Noise measurement;Sparse matrices;Noise;Cost function;non-negative matrix factorization;group sparsity;sparse representations;speech recognition;speaker recognition",
    abstract = "Non-negative matrix factorisations are used in several branches of signal processing and data analysis for separation and classification. Sparsity constraints are commonly set on the model to promote discovery of a small number of dominant patterns. In group sparse models, atoms considered to belong to a consistent group are permitted to activate together, while activations across groups are suppressed, reducing the number of simultaneously active sources or other structures. Whereas most group sparse models require explicit division of atoms into separate groups without addressing their mutual relations, we propose a constraint that permits dynamic relationships between atoms or groups, based on any defined distance measure. The resulting solutions promote approximation with components considered similar to each other. Evaluation results are shown for speech enhancement and noise robust speech and speaker recognition.",
    doi = "10.1109/ICASSP.2015.7178807",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/hurmalainen\_icassp2015.pdf"
}

@INPROCEEDINGS{Baby2015_ICASSP,
    author = "Baby, Deepak and Gemmeke, Jort F. and Virtanen, Tuomas and Van Hamme, Hugo",
    booktitle = "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    title = "Exemplar-based speech enhancement for deep neural network based automatic speech recognition",
    year = "2015",
    volume = "",
    number = "",
    pages = "4485-4489",
    keywords = "Training;Speech recognition;Neural networks;Testing;Computational modeling;Speech;deep neural networks;non-negative matrix factorisation;coupled dictionaries;speech enhancement;modulation envelope",
    doi = "10.1109/ICASSP.2015.7178819",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/dbaby\_icassp2015.pdf"
}

@mastersthesis{Mazhar2012_master,
    author = "Mazhar, Fawad",
    abstract = "Automatic guitar chord detection is a process that attempts to detect a guitar chord from a piece of audio. Generally, automatic chord detection is considered to be a part of a large problem termed as automatic transcription. Although there has been a lot of research in the field of automatic transcription, but having a reliable transcription system is still a distant prospect. Chord detection becomes interesting as chords have comparatively stable structure and they completely describe the occurring harmonies in a piece of music. This thesis presents a novel approach for detecting the correctness of musical chords played by guitar. The approach is based on pattern matching technique applied to the database of chords and their typical mistakes. Mistakes are the versions of a chord where typical playing errors are made. Transient of a chord is skipped and its spectrum is whitened. A certain region of whitened spectra is chosen as a feature vector. Cosine distance is computed between the extracted features and the data present in a reference chord database. Finally, the system detects the correctness of a played chord based on k-Nearest Neighbor (k-NN) classifier. The developed system uses two types of spectral whitening techniques: one is based on Linear Predictive Coding (LPC) and the other is based on Phase Transform-beta (PHAT-beta). The average accuracy shown by LPC based system is 72\% while that of PHAT-beta is 82.5\%. The system was also evaluated under different noise conditions.",
    title = "{A}utomatic {G}uitar {C}hord {D}etection",
    url = "http://dspace.cc.tut.fi/dpub/bitstream/handle/123456789/20923/mazhar.pdf?sequence=3",
    year = "2012",
    school = "Tampere University of Technology"
}

@INPROCEEDINGS{Cakir2015_IJCNN,
    author = "Cakir, Emre and Heittola, Toni and Huttunen, Heikki and Virtanen, Tuomas",
    booktitle = "2015 International Joint Conference on Neural Networks (IJCNN)",
    title = "Polyphonic sound event detection using multi label deep neural networks",
    year = "2015",
    volume = "",
    number = "",
    pages = "1-7",
    keywords = "Sound event detection;deep neural networks",
    abstract = "In this paper, the use of multi label neural networks are proposed for detection of temporally overlapping sound events in realistic environments. Real-life sound recordings typically have many overlapping sound events, making it hard to recognize each event with the standard sound event detection methods. Frame-wise spectral-domain features are used as inputs to train a deep neural network for multi label classification in this work. The model is evaluated with recordings from realistic everyday environments and the obtained overall accuracy is 63.8\%. The method is compared against a state-of-the-art method using non-negative matrix factorization as a pre-processing stage and hidden Markov models as a classifier. The proposed method improves the accuracy by 19\% percentage points overall.",
    doi = "10.1109/IJCNN.2015.7280624",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/cakir\_ijcnn2015.pdf"
}

@mastersthesis{Adavanne2011_master,
    author = "Adavanne, Sharath",
    abstract = "Room surface estimation is the process of estimating and characterizing the surfaces of a room from the measurement provided by an array of microphones. In any normal room, wavefront radiated by the source reaches the microphones after reflecting from the surfaces of the room. On performing the post processing on this recorded signal, depending on the type of source signal used; we obtain the unique signature of the room called the room impulse response (RIR). Historically room impulse responses have been used to calculate the acoustical parameters. These are generally used for objective evaluation of the rooms. Here is an effort to extract more information out of the RIR by understanding the physics of it. In this thesis the reflection from the surfaces is employed as useful information and methods to use this information are presented. The reflective information can be used in the determination of the reflection coefficient of the surface. The reflection coefficients of four common room materials are obtained from their respective impulse response at oblique incidence. The obtained reflection coefficients are studied for the classification of materials. The problem of estimating the room surface using the reflection coefficients has been approached in a systematic way, accounting for parasitic reflections and background noise. The room impulse response is obtained using the sine sweep signal transmitted through the speaker attached to one microphone array, and recorded by another microphone array. The recorded signal was used to identify possible surfaces using clustering on the obtained reflection coefficients. The classification so obtained is compared with the ground truth to calculate the performance and practicability",
    keywords = "Room Surface Estimation; Reflection Coecient; Reflection Method; Sine Sweep; Room Impulse Response",
    month = "October",
    school = "Tampere University of Technology",
    title = "Room Surface Estimation Using Reflection Coefficients Measured In-Situ",
    type = "Master's Thesis",
    url = "https://trepo.tuni.fi/handle/123456789/20707",
    year = "2011"
}

@INPROCEEDINGS{Diment2015_WASPAA,
    author = "Diment, Aleksandr and Virtanen, Tuomas",
    booktitle = "2015 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)",
    title = "Archetypal analysis for audio dictionary learning",
    year = "2015",
    volume = "",
    number = "",
    pages = "1-5",
    keywords = "Dictionaries;Speech;Algorithm design and analysis;Signal processing;Acoustics;Analytical models;Signal processing algorithms;archetypes;audio analysis;non-negative matrix factorisation;sparse representation",
    abstract = "This paper proposes dictionary learning with archetypes for audio processing. Archetypes refer to so-called pure types, which are a combination of a few data points and which can be combined to obtain a data point. The concept has been found useful in various problems, but it has not yet been applied for audio analysis. The algorithm performs archetypal analysis that minimises the generalised Kullback-Leibler divergence, shown suitable for audio, between an observation and the model. The methodology is evaluated in a source separation scenario (mixtures of speech) and shows results, which are comparable to the state-of-the-art, with perceptual measures indicating its superiority over all of the competing methods in the case of medium-size dictionaries.",
    doi = "10.1109/WASPAA.2015.7336903",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/Diment15\_AA.pdf"
}

@INPROCEEDINGS{Battaglino2015_EUSIPCO,
    author = "Battaglino, Daniele and Mesaros, Annamaria and Lepauloux, Ludovick and Pilati, Laurent and Evans, Nicholas",
    booktitle = "2015 23rd European Signal Processing Conference (EUSIPCO)",
    title = "Acoustic context recognition for mobile devices using a reduced complexity SVM",
    year = "2015",
    volume = "",
    number = "",
    pages = "534-538",
    keywords = "Context;Support vector machines;Training;Training data;Mobile handsets;Complexity theory;Hidden Markov models;Acoustic Context Recognition;mobile devices contextualization;SVM;k-means;LDA",
    doi = "10.1109/EUSIPCO.2015.7362440"
}

@INPROCEEDINGS{Cakir2015_EUSIPCO,
    author = "Cakir, Emre and Heittola, Toni and Huttunen, Heikki and Virtanen, Tuomas",
    booktitle = "2015 23rd European Signal Processing Conference (EUSIPCO)",
    title = "Multi-label vs. combined single-label sound event detection with deep neural networks",
    year = "2015",
    volume = "",
    number = "",
    pages = "2551-2555",
    keywords = "Training;Feature extraction;Signal processing;Europe;Event detection;Databases;Cost function;Sound event detection;deep neural networks;multi-label classification;binary classification;audio analysis",
    doi = "10.1109/EUSIPCO.2015.7362845",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/multi\_vs\_single\_eusipco-2015.pdf"
}

@inproceedings{Diment2013_CMMR,
    author = "Diment, Aleksandr and Padmanabhan, Rajan and Heittola, Toni and Virtanen, Tuomas",
    abstract = "In this work, the modified group delay feature (MODGDF) is proposed for pitched musical instrument recognition. Conventionally, the spectrum-related features used in instrument recognition take into account merely the magnitude information, whereas the phase is often overlooked due to the complications related to its interpretation. However, there is often additional information concealed in the phase, which could be beneficial for recognition. The MODGDF is a method of incorporating phase information, which lacks of the issues related to phase unwrapping. Having shown its applicability for speech-related problems, it is now explored in terms of musical instrument recognition. The evaluation is performed on separate note recordings in various instrument sets, and combined with the conventional mel frequency cepstral coefficients (MFCCs), MODGDF shows the noteworthy absolute accuracy gains of up to 5.1\% compared to the baseline MFCCs case.",
    booktitle = "10th International Symposium on Computer Music Multidisciplinary Research (CMMR)",
    title = "Modified Group Delay Feature for Musical Instrument Recognition",
    year = "2013",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/Diment13\_MODGDF.pdf"
}

@INPROCEEDINGS{Klapuri1999_ICASSP,
    author = "Klapuri, Anssi",
    booktitle = "1999 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings. ICASSP99",
    title = "Sound onset detection by applying psychoacoustic knowledge",
    year = "1999",
    volume = "6",
    number = "",
    pages = "3089-3092 vol.6",
    keywords = "Psychology;Acoustic signal detection;Signal processing;Robustness;Psychoacoustic models;Frequency;Audio recording;Event detection;Acoustic signal processing;Laboratories",
    doi = "10.1109/ICASSP.1999.757494"
}

@INPROCEEDINGS{Pertilä2016_IWAENC,
    author = "Pertilä, Pasi and Brutti, Alessio",
    booktitle = "2016 IEEE International Workshop on Acoustic Signal Enhancement (IWAENC)",
    title = "Increasing the environment-awareness of rake beamforming for directive acoustic sources",
    year = "2016",
    volume = "",
    number = "",
    pages = "1-5",
    keywords = "Array signal processing;Microphone arrays;Speech;Reverberation;Signal to noise ratio;Mirrors;Microphone arrays;Beamforming;Acoustic reflection;Speech enhancement;Speech intelligibility",
    doi = "10.1109/IWAENC.2016.7602932"
}

@inproceedings{Drossos2017_AES,
    author = "Drossos, Konstantinos and Mimilakis, Stylianos Ioannis and Floros, Andreas and Virtanen, Tuomas and Schuller, Gerald",
    abstract = "Close miking represents a widely employed practice of placing a microphone very near to the sound source in order to capture more direct sound and minimize any pickup of ambient sound, including other, concurrently active sources. It is used by the audio engineering community for decades for audio recording, based on a number of empirical rules that were evolved during the recording practice itself. But can this empirical knowledge and close miking practice be systematically verified? In this work we aim to address this question based on an analytic methodology that employs techniques and metrics originating from the sound source separation evaluation field. In particular, we apply a quantitative analysis of the source separation capabilities of the close miking technique. The analysis is applied on a recording dataset obtained at multiple positions of a typical musical hall, multiple distances between the microphone and the sound source multiple microphone types and multiple level differences between the sound source and the ambient acoustic component. For all the above cases we calculate the Source to Interference Ratio (SIR) metric. The results obtained clearly demonstrate an optimum close-miking performance that matches the current empirical knowledge of professional audio recording.",
    booktitle = "Audio Engineering Society Convention 142",
    publisher = "AES Audio Engineering Society",
    title = "Close Miking Empirical Practice Verification: A Source Separation Approach",
    year = "2017"
}

@INPROCEEDINGS{Mesaros2016_EUSIPCO,
    author = "Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas",
    booktitle = "2016 24th European Signal Processing Conference (EUSIPCO)",
    title = "{TUT} database for acoustic scene classification and sound event detection",
    year = "2016",
    volume = "",
    number = "",
    pages = "1128-1132",
    keywords = "Event detection;Databases;Automobiles;Signal processing;Mel frequency cepstral coefficient;Europe",
    abstract = "We introduce TUT Acoustic Scenes 2016 database for environmental sound research, consisting ofbinaural recordings from 15 different acoustic environments. A subset of this database, called TUT Sound Events 2016, contains annotations for individual sound events, specifically created for sound event detection. TUT Sound Events 2016 consists of residential area and home environments, and is manually annotated to mark onset, offset and label of sound events. In this paper we present the recording and annotation procedure, the database content, a recommended cross-validation setup and performance of supervised acoustic scene classification system and event detection baseline system using mel frequency cepstral coefficients and Gaussian mixture models. The database is publicly released to provide support for algorithm development and common ground for comparison of different techniques.",
    doi = "10.1109/EUSIPCO.2016.7760424",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/mesaros\_eusipco2016-dcase.pdf"
}

@INPROCEEDINGS{Diment2016_EUSIPCO,
    author = "Diment, Aleksandr and Parviainen, Mikko and Virtanen, Tuomas and Zelov, Roman and Glasman, Alex",
    booktitle = "2016 24th European Signal Processing Conference (EUSIPCO)",
    title = "Noise-robust detection of whispering in telephone calls using deep neural networks",
    year = "2016",
    volume = "",
    number = "",
    pages = "2310-2314",
    keywords = "Speech;Feature extraction;Noise measurement;Training;Signal processing;Neural networks;Speech recognition",
    abstract = "Detection of whispered speech in the presence of high levels of background noise has applications in fraudulent behaviour recognition. For instance, it can serve as an indicator of possible insider trading. We propose a deep neural network (DNN)-based whispering detection system, which operates on both magnitude and phase features, including the group delay feature from all-pole models (APGD). We show that the APGD feature outperforms the conventional ones. Trained and evaluated on the collected diverse dataset of whispered and normal speech with emulated phone line distortions and significant amounts of added background noise, the proposed system performs with accuracies as high as 91.8\%.",
    doi = "10.1109/EUSIPCO.2016.7760661",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/Diment16\_WHI.pdf"
}

@article{Drossos2015_TAC,
    author = "Drossos, K. and Floros, Andreas and Giannakoulopoulos, A. and Kanellopoulos, Nikolaos",
    abstract = "Emotion recognition from sound signals represents an emerging field of recent research. Although many existing works focus on emotion recognition from music, there seems to be a relative scarcity of research on emotion recognition from general sounds. One of the key characteristics of sound events is the sound source spatial position, i.e. the location of the source relatively to the acoustic receiver. Existing studies that aim to investigate the relation of the latter source placement and the elicited emotions are limited to distance, front and back spatial localization and/or specific emotional categories. In this paper we analytically investigate the effect of the source angular position on the listener's emotional state, modeled in the well-established valence/arousal affective space. Towards this aim, we have developed an annotated sound events dataset using binaural processed versions of the available International Affective Digitized Sound (IADS) sound events library. All subjective affective annotations were obtained using the Self Assessment Manikin (SAM) approach. Preliminary results obtained by processing these annotation scores are likely to indicate a systematic change in the listener affective state as the sound source angular position changes. This trend is more obvious when the sound source is located outside of the visible field of the listener.",
    doi = "10.1109/TAFFC.2015.2392768",
    issn = "1949-3045",
    journal = "IEEE Transactions on Affective Computing",
    keywords = "acoustic receivers; audio signal processing; cognition; emotion recognition; IADS; SAM; acoustic receiver; elicited emotions; emotional categories; international affective digitized sound sound events library; listener affective state; self assessment Mani",
    month = "1",
    number = "1",
    pages = "27--42",
    publisher = "Institute of Electrical and Electronics Engineers",
    title = "Investigating the Impact of Sound Angular Position on the Listener Affective State",
    volume = "6",
    year = "2015"
}

@inproceedings{Nikunen2014_ICASSP,
    author = "Nikunen, Joonas and Virtanen, Tuomas",
    abstract = "This paper studies multichannel audio separation using non-negative matrix factorization (NMF) combined with a new model for spatial covariance matrices (SCM). The proposed model for SCMs is parameterized by source direction of arrival (DoA) and its parameters can be optimized to yield a spatially coherent solution over frequencies thus avoiding permutation ambiguity and spatial liasing. The model constrains the estimation of SCMs to a set of geometrically possible solutions. Additionally we present a method for using a priori DoA information of the sources extracted blindly from the mixture for the initialization of the parameters of the proposed model. The simulations show that the proposed algorithm exceeds the separation quality of existing spatial separation methods.",
    booktitle = "2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    keywords = "Spatial sound separation; non-negative matrix factorization; spatial covariance models; Complex-Valued NMF",
    pages = "6727--6731",
    title = "Multichannel audio separation by Direction of Arrival Based Spatial Covariance Model and Non-negative Matrix Factorization",
    year = "2014",
    doi = "10.1109/ICASSP.2014.6854892",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/Nikunen\_ICASSP2014.pdf"
}

@INPROCEEDINGS{Naithani2016_GlobalSIP,
    author = "Naithani, Gaurav and Parascandolo, Giambattista and Barker, Tom and Pontoppidan, Niels Henrik and Virtanen, Tuomas",
    booktitle = "2016 IEEE Global Conference on Signal and Information Processing (GlobalSIP)",
    title = "Low-latency sound source separation using deep neural networks",
    year = "2016",
    volume = "",
    number = "",
    pages = "272-276",
    keywords = "Source separation;Training;Context;Measurement;Neural networks;Speech;Acoustics;Source separation;Deep neural networks;Low-latency",
    doi = "10.1109/GlobalSIP.2016.7905846",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/naithani\_globalsip2016.pdf"
}

@INPROCEEDINGS{Pertilä2017_ICASSP,
    author = "Pertilä, Pasi and Cakir, Emre",
    booktitle = "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    title = "Robust direction estimation with convolutional neural networks based steered response power",
    year = "2017",
    pages = "6125-6129",
    keywords = "Speech;Interference;Reverberation;Training;Microphones;Estimation;Convolution;sound source localization;steered response power;convolutional neural networks;time-frequency masking",
    doi = "10.1109/ICASSP.2017.7953333"
}

@article{Garcia-Molla2019_DSP,
    author = "Garcia-Molla, Victor M. and Juan, Pablo San and Virtanen, Tuomas and Vidal, Antonio M. and Alonso, Pedro",
    abstract = "In this paper, we propose, describe, and test a modification of the K-SVD algorithm. Given a set of training data, the proposed algorithm computes an overcomplete dictionary by minimizing the β-divergence (β>=1) between the data and its representation as linear combinations of atoms of the dictionary, under strict sparsity restrictions. For the special case β=2, the proposed algorithm minimizes the Frobenius norm and, therefore, for β=2 the proposed algorithm is equivalent to the original K-SVD algorithm. We describe the modifications needed and discuss the possible shortcomings of the new algorithm. The algorithm is tested with random matrices and with an example based on speech separation.",
    day = "1",
    doi = "10.1016/j.dsp.2019.05.001",
    issn = "1051-2004",
    journal = "Digital Signal Processing",
    keywords = "Beta-divergence;K-SVD;Matching pursuit algorithms;NMF;Nonnegative K-SVD",
    month = "9",
    pages = "47--53",
    publisher = "Elsevier",
    title = "Generalization of the {K}-{SVD} algorithm for minimization of β-divergence",
    volume = "92",
    year = "2019"
}

@inproceedings{Barker2014_IJCNN,
    author = "Barker, Tom and Virtanen, Tuomas",
    abstract = "This paper details the use of a semi-supervised approach to audio source separation. Where only a single source model is available, the model for an unknown source must be estimated. A mixture signal is separated through factorisation of a feature-tensor representation, based on the modulation spectrogram. Harmonically related components tend to modulate in a similar fashion, and this redundancy of patterns can be isolated. This feature representation requires fewer parameters than spectrally based methods and so minimises overfitting. Following the tensor factorisation, the separated signals are reconstructed by learning appropriate Wiener-filter spectral parameters which have been constrained by activation parameters learned in the first stage. Strong results were obtained for two-speaker mixtures where source separation performance exceeded those used as benchmarks. Specifically, the proposed semi-supervised method outperformed both semi-supervised non-negative matrix factorisation and blind non-negative modulation spectrum tensor factorisation.",
    booktitle = "Neural Networks (IJCNN), 2014 International Joint Conference on",
    doi = "10.1109/IJCNN.2014.6889522",
    isbn = "978-1-4799-1484-5",
    keywords = "Wiener filters; audio signal processing; matrix decomposition; signal reconstruction; source separation; speech processing; tensors; Wiener-filter spectral parameters; activation parameters; audio source separation; blind nonnegative modulation spectrum te",
    month = "7",
    pages = "3556--3561",
    title = "Semi-supervised non-negative tensor factorisation of modulation spectrograms for monaural speech separation",
    year = "2014",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/WCCI\_TomBarker\_Preprint.pdf"
}

@article{Mesaros2019_TASLP,
    author = "Mesaros, Annamaria and Diment, Aleksandr and Elizalde, Benjamin and Heittola, Toni and Vincent, Emmanuel and Raj, Bhiksha and Virtanen, Tuomas",
    abstract = "Each edition of the challenge on Detection and Classification of Acoustic Scenes and Events (DCASE) contained several tasks involving sound event detection in different setups. DCASE 2017 presented participants with three such tasks, each having specific datasets and detection requirements: Task 2, in which target sound events were very rare in both training and testing data, Task 3 having overlapping events annotated in real-life audio, and Task 4, in which only weakly labeled data were available for training. In this paper, we present three tasks, including the datasets and baseline systems, and analyze the challenge entries for each task. We observe the popularity of methods using deep neural networks, and the still widely used mel frequency-based representations, with only few approaches standing out as radically different. Analysis of the systems behavior reveals that task-specific optimization has a big role in producing good performance; however, often this optimization closely follows the ranking metric, and its maximization/minimization does not result in universally good performance. We also introduce the calculation of confidence intervals based on a jackknife resampling procedure to perform statistical analysis of the challenge results. The analysis indicates that while the 95{\\%} confidence intervals for many systems overlap, there are significant differences in performance between the top systems and the baseline for all tasks.",
    day = "1",
    doi = "10.1109/TASLP.2019.2907016",
    issn = "2329-9290",
    journal = "IEEE-ACM Transactions on Audio Speech and Language Processing",
    keywords = "confidence intervals;jackknife estimates;pattern recognition;Sound event detection;weak labels",
    month = "6",
    number = "6",
    pages = "992--1006",
    publisher = "IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC",
    title = "Sound Event Detection in the {DCASE} 2017 Challenge",
    volume = "27",
    year = "2019",
    url = "https://hal.inria.fr/hal-02067935/file/mesaros\_TASLP19.pdf"
}

@inproceedings{Barker2013_Interspeech 2013,
    author = "Barker, Tom and Virtanen, Tuomas",
    booktitle = "Proceedings of the 14th Annual Conference of the International Speech Communication Association (Interspeech 2013), 25-29 August, Lyon, France",
    pages = "827 -- 831",
    publisher = "International Speech Communication Association",
    series = "Interspeech",
    title = "Non-negative Tensor Factorisation of Modulation Spectrograms for Monaural Sound Source Separation",
    year = "2013",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/barker\_ntf2013.pdf"
}

@article{Drossos2016_AES,
    author = "Drossos, Konstantinos and Kaliakatsos-Papakostas, Maximos and Floros, Andreas and Virtanen, Tuomas",
    abstract = "Sound events are known to have an influence on the listener’s emotions, but the reason for this influence is less clear. Take for example the sound produced by a gun firing. Does the emotional impact arise from the fact that the listener recognizes that a gun produced the sound (semantic content) or does it arise from the attributes of the sound created by the firing gun? This research explores the relation between the semantic similarity of the sound events and the elicited emotions. Results indicate that the semantic content seems to have a limited role in the conformation of the listener’s affective states. However, when the semantic content is matched to specific areas in the Arousal-Valence space or when the source’s spatial position is considered, the effect of the semantic content is higher, especially for the cases of medium to low valence and medium to high arousal or when the sound source is at the lateral positions of the listener’s head.",
    day = "11",
    doi = "10.17743/jaes.2016.0024",
    issn = "1549-4950",
    journal = "Journal of the Audio Engineering Society",
    month = "8",
    number = "7/8",
    pages = "525--532",
    publisher = "Audio Engineering Society",
    title = "On the Impact of The Semantic Content of Sound Events in Emotion Elicitation",
    volume = "64",
    year = "2016"
}

@inproceedings{Drossos2019_DCASE2019,
    author = "Drossos, Konstantinos and Gharib, Shayan and Magron, Paul and Virtanen, Tuomas",
    abstract = {A sound event detection (SED) method typically takes as an input a sequence of audio frames and predicts the activities of sound events in each frame. In real-life recordings, the sound events exhibit some temporal structure: for instance, a {"}car horn{"} will likely be followed by a {"}car passing by{"}. While this temporal structure is widely exploited in sequence prediction tasks (e.g., in machine translation), where language models (LM) are exploited, it is not satisfactorily modeled in SED. In this work we propose a method which allows a recurrent neural network (RNN) to learn an LM for the SED task. The method conditions the input of the RNN with the activities of classes at the previous time step. We evaluate our method using F1 score and error rate (ER) over three different and publicly available datasets; the TUT-SED Synthetic 2016 and the TUT Sound Events 2016 and 2017 datasets. The obtained results show an increase of 9{\\%} and 2{\\%} at the F1 (higher is better) and a decrease of 7{\\%} and 2{\\%} at ER (lower is better) for the TUT Sound Events 2016 and 2017 datasets, respectively, when using our method. On the contrary, with our method there is a decrease of 4{\\%} at F1 score and an increase of 7{\\%} at ER for the TUT-SED Synthetic 2016 dataset.},
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019)",
    day = "25",
    keywords = "sound event detection; language modelling; sequence modelling; teacher forcing; scheduled sampling",
    month = "10",
    title = "Language Modelling for Sound Event Detection with Teacher Forcing and Scheduled Sampling",
    year = "2019",
    url = "https://arxiv.org/abs/1907.08506"
}

@INPROCEEDINGS{Klapuri1999_Cat. No.99TH8452,
    author = "Klapuri, Anssi",
    booktitle = "Proceedings of the 1999 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics. WASPAA'99 (Cat. No.99TH8452)",
    title = "Pitch estimation using multiple independent time-frequency windows",
    year = "1999",
    volume = "",
    number = "",
    pages = "115-118",
    keywords = "Time frequency analysis;Psychoacoustic models;Signal processing algorithms;Yield estimation;Frequency estimation;Acoustic signal processing;Psychology;Noise robustness;Interference;Laboratories",
    doi = "10.1109/ASPAA.1999.810863"
}

@mastersthesis{Gencoglu2014_master,
    author = "Gencoglu, Oguzhan",
    abstract = "Audio information retrieval has been a popular research subject over the last decades and being a subfield of this area, acoustic event classification has a considerable amount of share in the research. In this thesis, acoustic event classification using deep neural networks is investigated. Neural networks have been used in several pattern recognition (both function approximation and classification) tasks. Due to their stacked, layer-wise structure they have been proved to model highly nonlinear relations between inputs and outputs of a system with high performance. Even though several works imply an advantage of deeper networks over shallow ones in terms of recognition performance, advancements in training deep architectures were encountered only recently. These methods excel conventional methods such as HMMs and GMMs in terms of acoustic event classification performance. In this thesis, effects of several NN classifier parameters such as number of hidden layers, number of units in hidden layers, batch size, learning rate etc. on classification accuracy are examined. Effects of implementation parameters such as types of features, number of adjacent frames, number of most energetic frames etc. are also investigated. A classification accuracy of 61.1\% has been achieved with certain parameter values. In the case of DBNs, An application of greedy, layer-wise, unsupervised training before standard supervised training in order to initialize network weights in a better way, provided a 2-4\% improvement in classification performance. A NN that had randomly initialized weights before supervised training was shown to be considerably powerful in terms of acoustic event classification tasks compared to conventional methods. DBNs have provided even better classification accuracies and justified its significant potential for further research on the topic.",
    school = "Tampere University of Technology",
    keywords = "artificial neural networks;deep belief networks;acoustic event classification;deep learning",
    month = "February",
    title = "{A}coustic {E}vent {C}lassification {U}sing {D}eep {N}eural {N}etworks",
    url = "https://dspace.cc.tut.fi/dpub/bitstream/handle/123456789/22050/Gencoglu.pdf?sequence=1",
    year = "2014"
}

@INPROCEEDINGS{Naithani2017_WASPAA,
    author = "Naithani, Gaurav and Barker, Tom and Parascandolo, Giambattista and Bramsl⊘w, Lars and Pontoppidan, Niels Henrik and Virtanen, Tuomas",
    booktitle = "2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)",
    title = "Low latency sound source separation using convolutional recurrent neural networks",
    year = "2017",
    pages = "71-75",
    keywords = "Convolution;Neural networks;Training data;Training;Source separation;Time-frequency analysis;Source Separation;Low-latency;Deep Neural Networks;Convolutional Recurrent Neural Networks",
    doi = "10.1109/WASPAA.2017.8169997",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/PID4978439.pdf"
}

@mastersthesis{Klapuri1998_master,
    author = "Klapuri, Anssi",
    abstract = "This thesis concerns the transcription of music, where the idea is to pick a musical performance by a microphone or from a musical recording, and convert it into a symbolic representation. According to musical practice, this requires extraction of notes, their pitches, timings, and classiﬁcation of the instruments used. The respective subproblems, pitch tracking, rhythm detection and the analysis of musical instruments, were studied and are reviewed. Emphasis of this thesis is laid on two points: on literature review and on algorithm development. A literature review was conducted on automatic music transcription and several related areas of interest. An appropriate decomposition of the problem and the selection of an approach is ﬁrst considered. Then the state-of-the-art of the research is represented and discussed, and promising directions for further work are indicated. An original and the most important part of this thesis concerns the development of algorithms that can be used to detect and observe harmonic sounds in polyphonic signals. A novel number theoretical method is proposed, which is motivated by the spectral properties of a mixture of harmonic sounds, especially in musical signals. The performance of the method is evaluated by applying it in a piano music transcription program, which was implemented and simulated in Matlab environment. In the last chapter, the role and use of internal models and predictions in music transcription are discussed. This part of the work introduces processing principles that utilize high-level knowledge sources, such as instrument models and perception-based dependencies in music.",
    title = "Automatic transcription of music",
    year = "1998",
    school = "Tampere University of Technology"
}

@inproceedings{Klapuri1999_AES,
    author = "Klapuri, Anssi",
    booktitle = "106th Audio Engineering Society Convention",
    keywords = "Transcription",
    month = "May",
    title = "{W}ide-band {P}itch {E}stimation for {N}atural {S}ound {S}ources with {I}nharmonicities",
    year = "1999"
}

@inproceedings{Klapuri2006_SPMMT,
    author = "Klapuri, Anssi",
    editor = "Klapuri, A. and Davy, M.",
    booktitle = "Signal Processing Methods for Music Transcription",
    doi = "10.1007/0-387-32845-9\_8",
    isbn = "978-0-387-30667-4",
    pages = "229--265",
    publisher = "Springer",
    title = "{A}uditory model-based methods for multiple fundamental frequency estimation",
    year = "2006"
}

@inproceedings{Klapuri2000_EUSIPCO,
    author = "Klapuri, Anssi",
    booktitle = "Proceedings of the European Signal Processing Conference EUSIPCO",
    keywords = "transcription",
    title = "{Q}ualitative and quantitative aspects in the design of periodicity estimation algorithms",
    year = "2000"
}

@inproceedings{Klapuri2000_DAFx,
    author = "Klapuri, Anssi and Virtanen, Tuomas and Holm, Jan-Markus",
    address = "Verona, Italy",
    booktitle = "In Proc. COST-G6 Conference on Digital Audio Effects, DAFx-00",
    keywords = "transcription",
    title = "{R}obust multipitch estimation for the analysis and manipulation of polyphonic musical signals",
    year = "2000"
}

@inproceedings{Xie2019_WASPAA,
    author = "Xie, Huang and Virtanen, Tuomas",
    abstract = "This paper proposes a zero-shot learning approach for audio classification based on the textual information about class labels without any audio samples from target classes. We propose an audio classification system built on the bilinear model, which takes audio feature embeddings and semantic class label embeddings as input, and measures the compatibility between an audio feature embedding and a class label embedding. We use VGGish to extract audio feature embeddings from audio recordings. We treat textual labels as semantic side information of audio classes, and use Word2Vec to generate class label embeddings. Results on the ESC-50 dataset show that the proposed system can perform zero-shot audio classification with small training dataset. It can achieve accuracy (26 {\\%} on average) better than random guess (10 {\\%}) on each audio category. Particularly, it reaches up to 39.7 {\\%} for the category of natural audio classes.",
    booktitle = "2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)",
    doi = "10.1109/WASPAA.2019.8937283",
    isbn = "978-1-7281-1124-7",
    keywords = "zero-shot learning; audio classification; class label embedding",
    month = "10",
    pages = "264--267",
    publisher = "IEEE",
    series = "IEEE Workshop on Applications of Signal Processing to Audio and Acoustics",
    title = "Zero-Shot Audio Classification Based On Class Label Embeddings",
    year = "2019",
    url = "https://arxiv.org/abs/1905.01926"
}

@inbook{Virtanen2017_a,
    author = "Virtanen, Tuomas and Plumbley, Mark D. and Ellis, Dan",
    abstract = "Developing computational systems requires methods for evaluating their performance to guide development and compare alternate approaches. A reliable evaluation procedure for a classification or recognition system will involve a standard dataset of example input data along with the intended target output, and well-defined metrics to compare the systems' outputs with this ground truth. This chapter examines the important factors in the design and construction of evaluation datasets and goes through the metrics commonly used in system evaluation, comparing their properties. We include a survey of currently available datasets for environmental sound scene and event recognition and conclude with advice for designing evaluation protocols.",
    booktitle = "Computational Analysis of Sound Scenes and Events",
    doi = "10.1007/978-3-319-63450-0\_1",
    editor2 = "Tuomas Virtanen and Mark D. Plumbley and Dan Ellis",
    isbn = "978-3-319-63449-4",
    month = "9",
    pages = "3--12",
    publisher = "Springer",
    title = "Introduction to sound scene and event analysis",
    year = "2017"
}

@INPROCEEDINGS{Adavanne2018_IJCNN,
    author = "Adavanne, Sharath and Politis, Archontis and Virtanen, Tuomas",
    booktitle = "2018 International Joint Conference on Neural Networks (IJCNN)",
    title = "Multichannel Sound Event Detection Using 3D Convolutional Neural Networks for Learning Inter-channel Features",
    year = "2018",
    volume = "",
    number = "",
    pages = "1-7",
    keywords = "Feature extraction;Three-dimensional displays;Two dimensional displays;Event detection;Task analysis;Recurrent neural networks",
    abstract = "In this paper, we propose a stacked convolutional and recurrent neural network (CRNN) with a 3D convolutional neural network (CNN) in the first layer for the multichannel sound event detection (SED) task. The 3D CNN enables the network to simultaneously learn the inter-and intra-channel features from the input multichannel audio. In order to evaluate the proposed method, multichannel audio datasets with different number of overlapping sound sources are synthesized. Each of this dataset has a four-channel first-order Ambisonic, binaural, and single-channel versions, on which the performance of SED using the proposed method are compared to study the potential of SED using multichannel audio. A similar study is also done with the binaural and single-channel versions of the real-life recording TUT-SED 2017 development dataset. The proposed method learns to recognize overlapping sound events from multichannel features faster and performs better SED with a fewer number of training epochs. The results show that on using multichannel Ambisonic audio in place of single-channel audio we improve the overall F-score by 7.5\%",
    doi = "10.1109/IJCNN.2018.8489542"
}

@INPROCEEDINGS{Eronen2000_ICASSP,
    author = "Eronen, A. and Klapuri, A.",
    booktitle = "2000 IEEE International Conference on Acoustics, Speech, and Signal Processing",
    title = "Musical instrument recognition using cepstral coefficients and temporal features",
    year = "2000",
    volume = "2",
    number = "",
    pages = "II753-II756 vol.2",
    keywords = "Instruments;Cepstral analysis;Signal processing algorithms;Testing;Laboratories;Data mining;Algorithm design and analysis;Multiple signal classification;Music;Signal analysis",
    doi = "10.1109/ICASSP.2000.859069"
}

@mastersthesis{Cakir2014_master,
    author = "Cakir, Emre",
    abstract = "There are multiple sound events simultaneously occuring in a real-life audio recording collected e.g. at a busy street in rush hour. The events may include traffic noise, sound of rain, people talking etc. The humans are amazingly good at distinguishing these individual events, but as of yet, there is not any machine that can detect these events with (even close to) human accuracy. Polyphonic nature of the environmental audio recordings makes it hard to detect single sound events when many events are overlapping. With the gigantic audio database and state-of-the-art machine learning methods of the digital age, this is bound to change. In this thesis, we use frequency-domain features to represent the audio input and multilabel deep neural networks (DNN) to detect multiple, simultaneous sound events in a real-life recording. We extract frequency-domain features from these recordings in short time frames. DNNs are artificial neural networks (ANN) with two or more hidden layers and they are especially good at modeling highly nonlinear relations and finding intermediate representations between system input and output. This is exactly the case in real-life sound event detection. Every feature extract is used as a training example and we train the neural network with these examples. For the evaluation of this work, we focus on the performance of different topologies of DNNs used in this task. There are a large number of hyper parameters that define the structure of a DNN, such as the number of neurons in a layer, the learning rate used during learning, number of the hidden layers etc. The effects of each of these parameters are investigated in detail. A detection accuracy of 66.5\% is achieved, which outperforms the state-of-the-art method by a large margin.",
    school = "Tampere University of Technology",
    month = "December",
    title = "Multilabel Sound Event Classification with Neural Networks",
    year = "2014"
}

@inproceedings{Pirinen2008_DAFx-08,
    author = "Pirinen, Tuomo",
    editor = "Pakarinen, J.",
    booktitle = "Proceedings of the 11th International Conference on Digital Audio Effects (DAFx-08), Espoo, Finland, 1-4 September 2008",
    isbn = "978-951-22-9516-6",
    pages = "4 p",
    title = "{A}n experimental comparison of time delay weights for direction of arrival estimation",
    year = "2008"
}

@inproceedings{Bilcu2002_EUSIPCO,
    author = {Bilcu, Enik{\"o} Beatrice and Salmela, Petri and Suontausta, Janne and Saarinen, Jukka},
    booktitle = "Proceedings of EUSIPCO 2002 the XI European Signal Processing Conference, September 3-6, 2002, Tolouse, France",
    pages = "97--100",
    title = "Application of the Neural Networks for Text-to-Phoneme Mapping",
    year = "2002"
}

@article{Maijala2017_AA,
    author = "Maijala, Panu and Shuyang, Zhao and Heittola, Toni and Virtanen, Tuomas",
    abstract = "Environmental noise monitoring systems continuously measure sound levels without assigning these measurements to different noise sources in the acoustic scenes, therefore incapable of identifying the main noise source. In this paper a feasibility study is presented on a new monitoring concept in which an acoustic pattern classification algorithm running in a wireless sensor is used to automatically assign the measured sound level to different noise sources. A supervised noise source classifier is learned from a small amount of manually annotated recordings and the learned classifier is used to automatically detect the activity of target noise source in the presence of interfering noise sources. The sensor is based on an inexpensive credit-card-sized single-board computer with a microphone and associated electronics and wireless connectivity. The measurement results and the noise source information are transferred from the sensors scattered around the measurement site to a cloud service and a noise portal is used to visualise the measurements to users. The proposed noise monitoring concept was piloted on a rock crushing site. The system ran reliably over 50 days on site, during which it was able to recognise more than 90\% of the noise sources correctly. The pilot study shows that the proposed noise monitoring system can reduce the amount of required human validation of the sound level measurements when the target noise source is clearly defined.",
    doi = "10.1016/j.apacoust.2017.08.006",
    issn = "0003-682X",
    journal = "Applied Acoustics",
    keywords = "Acoustic pattern classification; Cloud service; Environmental noise monitoring; Wireless sensor network",
    month = "8",
    pages = "258--267",
    publisher = "Elsevier",
    title = "Environmental noise monitoring using source classification in sensors",
    volume = "129",
    year = "2017",
    url = "http://www.sciencedirect.com/science/article/pii/S0003682X17307533"
}

@article{Kiranyaz2012_NN,
    author = {Kiranyaz, Serkan and M{\"a}kinen, Toni and Gabbouj, Moncef},
    doi = "10.1016/j.neunet.2012.07.003",
    issn = "0893-6080",
    journal = "Neural Networks",
    pages = "80--95",
    publisher = "Elsevier",
    title = "{D}ynamic and scalable audio classification by collective network of binary classifiers framework: {A}n evolutionary approach",
    volume = "34",
    year = "2012"
}

@inproceedings{Adavanne2019_DCASE2019_a,
    author = "Adavanne, Sharath and Politis, Archontis and Virtanen, Tuomas",
    abstract = "This paper presents the sound event localization and detection (SELD) task setup for the DCASE 2019 challenge. The goal of the SELD task is to detect the temporal activities of a known set of sound event classes, and further localize them in space when active. As part of the challenge, a synthesized dataset where each sound event associated with a spatial coordinate represented using azimuth and elevation angles is provided. These sound events are spatialized using real-life impulse responses collected at multiple spatial coordinates in five different rooms with varying dimensions and material properties. A baseline SELD method employing a convolutional recurrent neural network is used to generate benchmark scores for this reverberant dataset. The benchmark scores are obtained using the recommended cross-validation setup.",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019)",
    month = "10",
    pages = "10--14",
    title = "A Multi-room Reverberant Dataset for Sound Event Localization and Detection",
    year = "2019",
    url = "https://arxiv.org/abs/1905.08546"
}

@mastersthesis{Mikkonen2000_master,
    author = "Mikkonen, Tomi",
    title = "Spherical Code Quantization in {CELP} Speech Coding",
    year = "2000",
    school = "Tampere University of Technology"
}

@phdthesis{Mikkonen2009_phd,
    author = "Mikkonen, Tomi",
    title = "{T}he {R}ing of {G}raph {I}nvariants",
    year = "2009",
    school = "Tampere University of Technology"
}

@INPROCEEDINGS{Drossos2020_IJCNN,
    author = "Drossos, Konstantinos and Mimilakis, Stylianos I. and Gharib, Shayan and Li, Yanxiong and Virtanen, Tuomas",
    booktitle = "2020 International Joint Conference on Neural Networks (IJCNN)",
    title = "Sound Event Detection with Depthwise Separable and Dilated Convolutions",
    year = "2020",
    pages = "1-7",
    keywords = "Feature extraction;Two dimensional displays;Kernel;Convolutional codes;Context modeling;Event detection;Convolution;sound event detection;depthwise separable convolution;dilated convolution",
    doi = "10.1109/IJCNN48605.2020.9207532",
    url = "https://arxiv.org/abs/2002.00476"
}

@INPROCEEDINGS{Muñoz-Montoro2020_MMSP,
    author = "Muñoz-Montoro, Antonio J. and Politis, Archontis and Drossos, Konstantinos and Carabias-Orti, Julio J.",
    booktitle = "2020 IEEE 22nd International Workshop on Multimedia Signal Processing (MMSP)",
    title = "Multichannel Singing Voice Separation by Deep Neural Network Informed {DOA} Constrained {CMNMF}",
    year = "2020",
    pages = "1-6",
    keywords = "Time-frequency analysis;Direction-of-arrival estimation;Estimation;Channel estimation;Information filters;Task analysis;Spectrogram;Multichannel Source Separation;Singing Voice;Deep Learning;CMNMF;Spatial Audio",
    abstract = "This work addresses the problem of multichannel source separation combining two powerful approaches, multichannel spectral factorization with recent monophonic deep-learning (DL) based spectrum inference. Individual source spectra at different channels are estimated with a Masker-Denoiser Twin Network (MaD TwinNet), able to model long-term temporal patterns of a musical piece. The monophonic source spectrograms are used within a spatial covariance mixing model based on Complex Non-Negative Matrix Factorization (CNMF) that predicts the spatial characteristics of each source. The proposed framework is evaluated on the task of singing voice separation with a large multichannel dataset. Experimental results show that our joint DL+CNMF method outperforms both the individual monophonic DL-based separation and the multichannel CNMF baseline methods.",
    doi = "10.1109/MMSP48831.2020.9287068"
}

@INPROCEEDINGS{Pyykkönen2020_MMSP,
    author = "Pyykkönen, Pyry and Mimilakis, Styliannos I. and Drossos, Konstantinos and Virtanen, Tuomas",
    booktitle = "2020 IEEE 22nd International Workshop on Multimedia Signal Processing (MMSP)",
    title = "Depthwise Separable Convolutions Versus Recurrent Neural Networks for Monaural Singing Voice Separation",
    year = "2020",
    pages = "1-6",
    keywords = "Measurement;Training;Source separation;Recurrent neural networks;Convolution;Task analysis;Standards;Depthwise separable convolutions;recurrent neural networks;mad;madtwinnet;monaural singing voice separation",
    abstract = "Recent approaches for music source separation are almost exclusively based on deep neural networks, mostly employing recurrent neural networks (RNNs). Although RNNs are in many cases superior than other types of deep neural networks for sequence processing, they are known to have specific difficulties in training and parallelization, especially for the typically long sequences encountered in music source separation. In this paper we present a use-case of replacing RNNs with depth-wise separable (DWS) convolutions, which are a lightweight and faster variant of the typical convolutions. We focus on singing voice separation, employing an RNN architecture, and we replace the RNNs with DWS convolutions (DWS-CNNs). We conduct an ablation study and examine the effect of the number of channels and layers of DWS-CNNs on the source separation performance, by utilizing the standard metrics of signal-to-artifacts, signal-to-interference, and signal-to-distortion ratio. Our results show that by replacing RNNs with DWS-CNNs yields an improvement of 1.20, 0.06, 0.37 dB, respectively, while using only 20.57{\\%} of the amount of parameters of the RNN architecture.",
    doi = "10.1109/MMSP48831.2020.9287169",
    url = "https://arxiv.org/abs/2007.02683"
}

@INPROCEEDINGS{Mimilakis2021_EUSIPCO,
    author = "Mimilakis, Stylianos I. and Drossos, Konstantinos and Schuller, Gerald",
    booktitle = "2020 28th European Signal Processing Conference (EUSIPCO)",
    title = "Unsupervised Interpretable Representation Learning for Singing Voice Separation",
    year = "2021",
    pages = "1412-1416",
    keywords = "Time-frequency analysis;Source separation;Fourier transforms;Noise reduction;Multiple signal classification;Signal representation;Task analysis;representation learning;unsupervised learning;denoising auto-encoders;singing voice separation",
    doi = "10.23919/Eusipco47968.2020.9287352"
}

@INPROCEEDINGS{Nicodemo2021_EUSIPCO,
    author = "Nicodemo, Niccolò and Naithani, Gaurav and Drossos, Konstantinos and Virtanen, Tuomas and Saletti, Roberto",
    booktitle = "2020 28th European Signal Processing Conference (EUSIPCO)",
    title = "Memory Requirement Reduction of Deep Neural Networks for Field Programmable Gate Arrays Using Low-Bit Quantization of Parameters",
    year = "2021",
    pages = "466-470",
    keywords = "Quantization (signal);Neural networks;Memory management;Speech enhancement;Logic gates;Table lookup;Field programmable gate arrays;neural network quantization;memory footprint reduction;FPGA;hardware accelerators",
    abstract = "Effective employment of deep neural networks (DNNs) in mobile devices and embedded systems is hampered by requirements for memory and computational power. This paper presents a non-uniform quantization approach which allows for dynamic quantization of DNN parameters for different layers and within the same layer. A virtual bit shift (VBS) scheme is also proposed to improve the accuracy of the proposed scheme. Our method reduces the memory requirements, preserving the performance of the network. The performance of our method is validated in a speech enhancement application, where a fully connected DNN is used to predict the clean speech spectrum from the input noisy speech spectrum. A DNN is optimized and its memory footprint and performance are evaluated using the short-time objective intelligibility, STOI, metric. The application of the low-bit quantization allows a 50{\\%} reduction of the DNN memory footprint while the STOI performance drops only by 2.7{\\%}.",
    doi = "10.23919/Eusipco47968.2020.9287739",
    url = "https://arxiv.org/abs/1911.00527"
}

@INPROCEEDINGS{Klapuri2001_ICASSP,
    author = "Klapuri, Anssi",
    booktitle = "2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings",
    title = "Multipitch estimation and sound separation by the spectral smoothness principle",
    year = "2001",
    volume = "5",
    number = "",
    pages = "3381-3384 vol.5",
    keywords = "Humans;Auditory system;Psychoacoustic models;Frequency;Databases;Instruments;Smoothing methods;Error correction;Error analysis;Computational modeling",
    doi = "10.1109/ICASSP.2001.940384"
}

@inproceedings{Barker2014_InterSpecch,
    author = "Barker, Tom and Hamme, Hugo Van and Virtanen, Tuomas",
    booktitle = "INTERSPEECH2014, 15th Annual Conference of the International Speech Communication Association, 14-18 September 2014, Singapore",
    pages = "1371--1375",
    publisher = "International Speech Communication Association",
    title = "Modelling Primitive Streaming of Simple Tone Sequences Through Factorisation of Modulation Pattern Tensors",
    year = "2014",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/Barker2014b.pdf"
}

@inproceedings{Pirinen2005_ISCAS,
    author = "Pirinen, Tuomo",
    booktitle = "Proceedings of 2005 IEEE International Symposium on Circuits and Systems, ISCAS 2005, Kobe, Japan, 23-26 May 2005",
    pages = "1429--1432",
    title = "{N}ormalized confidence factors for robust direction of arrival estimation",
    year = "2005"
}

@INPROCEEDINGS{Eronen2001_Cat. No.01TH8575,
    author = "Eronen, A.",
    booktitle = "Proceedings of the 2001 IEEE Workshop on the Applications of Signal Processing to Audio and Acoustics (Cat. No.01TH8575)",
    title = "Comparison of features for musical instrument recognition",
    year = "2001",
    volume = "",
    number = "",
    pages = "19-22",
    keywords = "Instruments;Cepstral analysis;Steady-state;Mel frequency cepstral coefficient;Humans;Performance analysis;Brightness;Frequency synchronization;Acoustic testing;Filter bank",
    doi = "10.1109/ASPAA.2001.969532"
}

@INPROCEEDINGS{Virtanen2001_Cat. No.01TH8575,
    author = "Virtanen, Tuomas and Klapuri, Anssi",
    booktitle = "Proceedings of the 2001 IEEE Workshop on the Applications of Signal Processing to Audio and Acoustics (Cat. No.01TH8575)",
    title = "Separation of harmonic sounds using multipitch analysis and iterative parameter estimation",
    year = "2001",
    volume = "",
    number = "",
    pages = "83-86",
    keywords = "Harmonic analysis;Parameter estimation;Frequency estimation;Amplitude estimation;Acoustic signal processing;Iterative methods;Image analysis;Humans;Laboratories;Signal processing algorithms",
    doi = "10.1109/ASPAA.2001.969548",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/waspaa2001.pdf"
}

@inproceedings{Magron2017,
    author = "Magron, Paul and Badeau, Roland and Liutkus, Antoine",
    abstract = "In this paper, we address the problem of robust source separation of nonnegative data. We introduce the PαS distributions, which are a subclass of the stable distributions family, to model the nonnegative latent sources. Since those distributions are heavy-tailed, they are expected to be robust to outliers. Considering the Lévy distribution, the only PαS distribution whose density admits a closed form expression, we propose a mixture model called Lévy Nonnegative Matrix Factorization (Lévy NMF). The model is estimated in a maximum-likelihood sense. We also derive an estimator of the sources which extends the validity of the generalized Wiener filtering to the PαS case. Experiments on musical spectrograms and fluorescence spectra highlight the potential of the Lévy NMF model for decomposing nonnegative data.",
    booktitle = "Actes du XXVI{\`e}me Colloque GRETSI",
    month = "9",
    title = "{L}{\'e}vy {NMF} : un mod{\`e}le robuste de s{\'e}paration de sources non-n{\'e}gatives",
    year = "2017"
}

@inproceedings{Li2020_ICASSP,
    author = "Li, Yanxiong and Liu, Mingle and Drossos, Konstantinos and Virtanen, Tuomas",
    abstract = "Convolutional recurrent neural networks (CRNNs) have achieved state-of-the-art performance for sound event detection (SED). In this paper, we propose to use a dilated CRNN, namely a CRNN with a dilated convolutional kernel, as the classifier for the task of SED. We investigate the effectiveness of dilation operations which provide a CRNN with expanded receptive fields to capture long temporal context without increasing the amount of CRNN's parameters. Compared to the classifier of the baseline CRNN, the classifier of the dilated CRNN obtains a maximum increase of 1.9\%, 6.3\% and 2.5\% at F1 score and a maximum decrease of 1.7\%, 4.1\% and 3.9\% at error rate (ER), on the publicly available audio corpora of the TUT-SED Synthetic 2016, the TUT Sound Event 2016 and the TUT Sound Event 2017, respectively.",
    booktitle = "2020 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)",
    doi = "10.1109/ICASSP40776.2020.9054433",
    isbn = "978-1-5090-6632-2",
    pages = "286--290",
    publisher = "IEEE",
    series = "IEEE International Conference on Acoustics, Speech and Signal Processing",
    title = "Sound Event Detection Via Dilated Convolutional Recurrent Neural Networks",
    year = "2020",
    url = "https://arxiv.org/abs/1911.10888"
}

@inproceedings{Caballero2017_EX-TEL,
    author = "Caballero, Daniela and Araya, Roberto and Kronholm, Hanna and Viiri, Jouni and Mansikkaniemi, Andr{\'e} and Lehesvuori, Sami and Virtanen, Tuomas and Kurimo, Mikko",
    abstract = "Automatic Speech Recognition (ASR) field has improved substantially in the last years. We are in a point never saw before, where we can apply such algorithms in non-ideal conditions such as real classrooms. In these scenarios it is still not possible to reach perfect recognition rates, however we can already take advantage of these improvements. This paper shows preliminary results using ASR in Chilean and Finnish middle and high school to automatically provide teachers a visualization of the structure of concepts present in their discourse in science classrooms. These visualizations are conceptual networks that relate key concepts used by the teacher. This is an interesting tool that gives feedback to the teacher about his/her pedagogical practice in classes. The result of initial comparisons shows great similarity between conceptual networks generated in a manual way with those generated automatically.",
    address = "Germany",
    booktitle = "Data Driven Approaches in Digital Education - 12th European Conference on Technology Enhanced Learning, EC-TEL 2017, Proceedings",
    doi = "10.1007/978-3-319-66610-5\_58",
    isbn = "9783319666099",
    keywords = "Automatic speech recognition; Classroom dialogue; Conceptual network; Teacher discourse",
    pages = "541--544",
    publisher = "Springer Verlag",
    series = "Lecture Notes in Computer Science",
    title = "{ASR} in classroom today: {A}utomatic visualization of conceptual network in science classrooms",
    year = "2017",
    url = "https://jyx.jyu.fi/bitstream/handle/123456789/55458/ectel142\%201.pdf?sequence=1\&isAllowed=y"
}

@phdthesis{Klapuri2004_phd,
    author = "Klapuri, Anssi",
    institution = "Tampere University of Technology",
    title = "{S}ignal {P}rocessing {M}ethods for the {A}utomatic {T}ranscription of {M}usic",
    year = "2004",
    school = "Tampere University of Technology"
}

@inproceedings{Bilcu2004_ISCCSP,
    author = {Bilcu, Enik{\"o} Beatrice and Astola, Jaakko and Saarinen, Jukka},
    address = "Hammamet, Tunisia",
    booktitle = "Proceedings of the 2004 First International Symposium on Control, Communications and Signal Processing, ISCCSP",
    month = "March",
    pages = "599-602",
    title = "Recurrent neural networks with both side input context dependence for text-to-phoneme mapping",
    year = "2004"
}

@inproceedings{Bilcu2005_EUSIPCO,
    author = {Bilcu, Enik{\"o} Beatrice and Astola, Jaakko and Saarinen, Jukka},
    abstract = "Text-to-phoneme mapping is a very important preliminary step in any text-to-speech synthesis system. In this paper, we study the performances of the multilayer perceptron (MLP) neural network for the problem of text-to-phoneme mapping. Specifically, we study the influence of the input letter encoding in the conversion accuracy of such system. We show, that for large network complexities the orthogonal binary codes (as introduced in NetTalk) gives better performance. On the other hand in applications that require very small memory load and computational complexity other compact codes may be more suitable. This study is a first step toward implementation a neural network based text-to-phoneme mapping in mobile devices.",
    address = "Antalya, Turkey",
    booktitle = "Proceedings of 13. European Signal Processing Conference, EUSIPCO",
    month = "September",
    title = "{C}omparative study of letter encoding for text-to-phoneme mapping",
    year = "2005"
}

@inproceedings{Bilcu2006_EUSIPCO,
    author = {Bilcu, Enik{\"o} Beatrice and Astola, Jaakko},
    address = "Florence, Italy",
    booktitle = "Proceedings of the 14th European Signal Processing Conference, EUSIPCO",
    month = "September",
    title = "{N}eural networks with random letter codes for text-to-phoneme mapping and small training dictionary",
    year = "2006"
}

@inproceedings{Bilcu2006_MLSP,
    author = {Bilcu, Enik{\"o} Beatrice and Astola, Jaakko},
    address = "Maynooth, Ireland",
    booktitle = "Proceedings of the 2006 IEEE International Workshop on Machine Learning for Signal Processing",
    month = "September",
    pages = "253-258",
    title = "{A} {H}ybrid neural network for language identification from text",
    year = "2006"
}

@incollection{FitzGerald2006,
    author = "FitzGerald, Derry and Paulus, Jouni",
    editor = "Klapuri, Anssi and Davy, Manuel",
    booktitle = "Signal Processing Methods for Music Transcription",
    pages = "131--162",
    publisher = "Springer-Verlag",
    title = "Unpitched Percussion Transcription",
    url = "http://www.springerlink.com/content/w4175761l68h5t85",
    year = "2006"
}

@incollection{Diment2014,
    author = "Diment, Aleksandr and Rajan, Padmanabhan and Heittola, Toni and Virtanen, Tuomas",
    editor = "Aramaki, Mitsuko and Derrien, Olivier and Kronland-Martinet, Richard and Ystad, S{\o}lvi",
    abstract = "In this work, the feature based on the group delay function from all-pole models (APGD) is proposed for pitched musical instrument recognition. Conventionally, the spectrum-related features take into account merely the magnitude information, whereas the phase is often overlooked due to the complications related to its interpretation. However, there is often additional information concealed in the phase, which could be beneficial for recognition. The APGD is an elegant approach to inferring phase information, which lacks of the issues related to interpreting the phase and does not require extensive parameter adjustment. Having shown applicability for speech-related problems, it is now explored in terms of instrument recognition. The evaluation is performed with various instrument sets and shows noteworthy absolute accuracy gains of up to 7\% compared to the baseline mel-frequency cepstral coefficients (MFCCs) case. Combined with the MFCCs and with feature selection, APGD demonstrates superiority over the baseline with all the evaluated sets.",
    booktitle = "Sound, Music, and Motion",
    doi = "10.1007/978-3-319-12976-1\_37",
    isbn = "978-3-319-12975-4",
    keywords = "Musical instrument recognition; Music information retrieval; All-pole group delay feature; Phase spectrum",
    pages = "606-618",
    publisher = "Springer International Publishing",
    title = "Group Delay Function from All-Pole Models for Musical Instrument Recognition",
    year = "2014"
}

@inproceedings{Diment2015_EUSIPCO,
    author = "Diment, Aleksandr and Cakir, Emre and Heittola, Toni and Virtanen, Tuomas",
    abstract = "A feature based on the group delay function from all-pole models (APGD) is proposed for environmental sound event recognition. The commonly used spectral features take into account merely the magnitude information, whereas the phase is overlooked due to the complications related to its interpretation. Additional information concealed in the phase is hypothesised to be beneficial for sound event recognition. The APGD is an approach to inferring phase information, which has shown applicability for analysis of speech and music signals and is now studied in environmental audio. The evaluation is performed within a multi-label deep neural network (DNN) framework on a diverse real-life dataset of environmental sounds. It shows performance improvement compared to the baseline log mel-band energy case. In combination with the magnitude-based features, APGD demonstrates further improvement.",
    booktitle = "2015 23rd European Signal Processing Conference (EUSIPCO)",
    title = "Automatic recognition of environmental sound events using all-pole group delay features",
    doi = "10.1109/EUSIPCO.2015.7362479",
    pages = "729-733",
    volume = "",
    number = "",
    keywords = "Delays;Discrete cosine transforms;Feature extraction;Computational modeling;Signal processing;Europe;Neural networks;Phase spectrum;sound event recognition;audio classification;neural networks",
    year = "2015",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/Diment15\_APGD4events.pdf"
}

@inproceedings{Diment2013_EUSIPCO 2013,
    author = "Diment, Aleksandr and Heittola, Toni and Virtanen, Tuomas",
    abstract = "In this work, the semi-supervised learning (SSL) techniques are explored in the context of musical instrument recognition. The conventional supervised approaches normally rely on annotated data to train the classifier. This implies performing costly manual annotations of the training data. The SSL methods enable utilising the additional unannotated data, which is significantly easier to obtain, allowing the overall development cost maintained at the same level while notably improving the performance. The implemented classifier incorporates the Gaussian mixture model-based SSL scheme utilising the iterative EM-based algorithm, as well as the extensions facilitating a simpler convergence criteria. The evaluation is performed on a set of nine instruments while training on a dataset, in which the relative size of the labelled data is as little as 15\%. It yields a noteworthy absolute performance gain of 16\% compared to the performance of the initial supervised models.",
    booktitle = "21st European Signal Processing Conference 2013 (EUSIPCO 2013)",
    keywords = "Music information retrieval; musical instrument recognition; semi-supervised learning; instruments",
    month = "Sep",
    title = "Semi-supervised Learning for Musical Instrument Recognition",
    pages = "1-5",
    year = "2013",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/Diment13\_SSL.pdf"
}

@mastersthesis{Diment2013_master,
    author = "Diment, Aleksandr",
    abstract = "The application areas of music information retrieval have been gaining popularity over the last decades. Musical instrument recognition is an example of a specific research topic in the field. In this thesis, semi-supervised learning techniques are explored in the context of musical instrument recognition. The conventional approaches employed for musical instrument recognition rely on annotated data, i.e., example recordings of the target instruments with associated information about the target labels in order to perform training. This implies a highly laborious and tedious work of manually annotating the collected training data. The semi-supervised methods enable incorporating additional unannotated data into training. Such data consists of merely the recordings of the instruments and is therefore significantly easier to acquire. Hence, these methods allow keeping the overall development cost at the same level while notably improving the performance of a system. The implemented musical instrument recognition system utilises the mixture model semi-supervised learning scheme in the form of two EM-based algorithms. Furthermore, upgraded versions, namely, the additional labelled data weighting and class-wise retraining, for the improved performance and convergence criteria in terms of the particular classification scenario are proposed. The evaluation is performed on sets consisting of four and ten instruments and yields the overall average recognition accuracy rates of 95.3 and 68.4\%, respectively. These correspond to the absolute gains of 6.1 and 9.7\% compared to the initial, purely supervised cases. Additional experiments are conducted in terms of the effects of the proposed modifications, as well as the investigation of the optimal relative labelled dataset size. In general, the obtained performance improvement is quite noteworthy, and future research directions suggest to subsequently investigate the behaviour of the implemented algorithms along with the proposed and further extended approaches.",
    address = "Finland",
    keywords = "music information retrieval; musical instrument recognition; pattern recogni- tion; semi-supervised learning",
    school = "Tampere University of Technology",
    title = "{S}emi-supervised musical instrument recognition",
    url = "https://dspace.cc.tut.fi/dpub/bitstream/handle/123456789/21595/diment.pdf?sequence=1{\\&}isAllowed=y",
    year = "2013"
}

@article{Mäkinen2012_JASM,
    author = {M{\"a}kinen, Toni and Kiranyaz, Serkan and Raitoharju, Jenni and Gabbouj, Moncef},
    abstract = "A vast amount of audio features have been proposed in the literature to characterize the content of audio signals. In order to overcome specific problems related to the existing features (such as lack of discriminative power), as well as to reduce the need for manual feature selection, in this article, we propose an evolutionary feature synthesis technique with a built-in feature selection scheme. The proposed synthesis process searches for optimal linear/nonlinear operators and feature weights from a pre-defined multi-dimensional search space to generate a highly discriminative set of new (artificial) features. The evolutionary search process is based on a stochastic optimization approach in which a multi-dimensional particle swarm optimization algorithm, along with fractional global best formation and heterogeneous particle behavior techniques, is applied. Unlike many existing feature generation approaches, the dimensionality of the synthesized feature vector is also searched and optimized within a set range in order to better meet the varying requirements set by many practical applications and classifiers. The new features generated by the proposed synthesis approach are compared with typical low-level audio features in several classification and retrieval tasks. The results demonstrate a clear improvement of up to 15--20\% in average retrieval performance. Moreover, the proposed synthesis technique surpasses the synthesis performance of evolutionary artificial neural networks, exhibiting a considerable capability to accurately distinguish among different audio classes.",
    journal = "EURASIP Journal on Audio, Speech, and Music Processing",
    keywords = "CASA;general audio classification",
    number = "23",
    title = "{A}n evolutionary feature synthesis approach for content-based audio retrieval",
    year = "2012"
}

@inproceedings{Gemmeke2011_EUSIPCO,
    author = "Gemmeke, Jort and Hurmalainen, Antti and Virtanen, Tuomas and Yang, Sun",
    abstract = "In previous work it was shown that, at least in principle, an exemplar-based approach to noise robust ASR is possible. The method, sparse representation based classification (SC), works by modelling noisy speech as a sparse linear combination of speech and noise exemplars. After recovering the sparsest possible linear combination of labelled exemplars, noise robust posterior likelihoods are estimated by using the weights of the exemplars as evidence of the state labels underlying exemplars. Although promising recognition accuracies at low SNRs were obtained, the method was impractical due to its slow execution speed. Moreover, the performance was not as good on noisy speech corrupted by noise types not represented by the noise exemplars. The importance of sparsity was poorly understood, and the influence of the size of the exemplar-dictionary was unclear. In this paper we investigate all these issues, and we show for example that speedups of a factor 28 can be obtained by using modern GPUs, bringing its execution speed within range to practical applications.",
    address = "Barcelona, Spain",
    booktitle = "European Signal Processing Conference (EUSIPCO)",
    keywords = "speech recognition",
    month = "August",
    organization = "EURASIP",
    pages = "1490-1494",
    title = "Toward a Practical Implementation of Exemplar-Based Noise Robust {ASR}",
    year = "2011"
}

@inproceedings{Heittola2010_EUSIPCO,
    author = "Heittola, Toni and Mesaros, Annamaria and Eronen, Antti and Virtanen, Tuomas",
    abstract = "This paper presents a method for audio context recognition, meaning classification between everyday environments. The method is based on representing each audio context using a histogram of audio events which are detected using a supervised classifier. In the training stage, each context is modeled with a histogram estimated from annotated training data. In the testing stage, individual sound events are detected in the unknown recording and a histogram of the sound event occurrences is built. Context recognition is performed by computing the cosine distance between this histogram and event histograms of each context from the training database. Term frequency--inverse document frequency weighting is studied for controlling the importance of different events in the histogram distance calculation. An average classification accuracy of 89\% is obtained in the recognition between ten everyday contexts. Combining the event based context recognition system with more conventional audio based recognition increases the recognition rate to 92\%.",
    booktitle = "In Proc. European Signal Processing Conference",
    keywords = "CASA;Context recognition",
    title = "Audio context recognition using audio event histograms",
    year = "2010",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/eusipco2010\_heittola.pdf"
}

@inproceedings{Mesaros2010_EUSIPCO,
    author = "Mesaros, Annamaria and Heittola, Toni and Eronen, Antti and Virtanen, Tuomas",
    abstract = "This paper presents a system for acoustic event detection in recordings from real life environments. The events are modeled using a network of hidden Markov models; their size and topology is chosen based on a study of isolated events recognition. We also studied the effect of ambient background noise on event classification performance. On real life recordings, we tested recognition of isolated sound events and event detection. For event detection, the system performs recognition and temporal positioning of a sequence of events. An accuracy of 24\% was obtained in classifying isolated sound events into 61 classes. This corresponds to the accuracy of classifying between 61 events when mixed with ambient background noise at 0dB signal-to-noise ratio. In event detection, the system is capable of recognizing almost one third of the events, and the temporal positioning of the events is not correct for 84\% of the time.",
    address = "Aalborg, Denmark",
    booktitle = "In Proc. European Signal Processing Conference",
    keywords = "CASA;sound event detection",
    pages = "1267-1271",
    title = "Acoustic event detection in real life recordings",
    year = "2010",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/acoustic\_event\_detection\_1406.pdf"
}

@techreport{Cakir2017,
    author = "Cakir, Emre and Drossos, Konstantinos and Virtanen, Tuomas",
    abstract = "In this paper, we focus on bird audio detection in short audio segments (namely 10 seconds) using stacked convolutional and recurrent neural networks. The evaluation data for this task was recorded in an acoustic soundscape different from the development data, thus motivating to work on methods that are generic and context independent. Data augmentation and regularization methods are proposed and evaluated in this regard. Area under curve (AUC) measure is used to compare different results. Our best achieved AUC measure on five cross-validations of the development data is 95.3\% and 88.41\% on the unseen evaluation data.",
    keywords = "Bird audio detection; convolutional recurrent neural network",
    title = "{QMUL} bird audio detection challenge 2016",
    url = "http://machine-listening.eecs.qmul.ac.uk/wp-content/uploads/sites/26/2017/01/adavanne.pdf",
    year = "2017",
    institution = "Tampere University of Technology"
}

@inproceedings{Eronen2002_ICASSP,
    author = {Eronen, Antti and Tuomi, Juha and Klapuri, Anssi and Fagerlund, Seppo and Sorsa, Timo and Lorho, Ga{\"e}tan and Huopaniemi, Jyri},
    booktitle = "Proc. IEEE Int. Conf. Acoustics, Speech, Signal Processing",
    keywords = "context recognition",
    month = "May",
    pages = "1941-1944",
    title = "Audio-based context awareness - {A}coustic modeling and perceptual evaluation",
    year = "2002"
}

@inproceedings{Eronen2003_FINSIG,
    author = "Eronen, Antti and Heittola, Toni",
    address = "Tampere, Finland",
    booktitle = "Proceedings of the 2003 Finnish Signal Processing Symposium, FINSIG'03",
    number = "20",
    series = {""},
    pages = "54-58",
    title = "{D}iscriminative training of unsupervised acoustic models for non-speech audio",
    year = "2003"
}

@inproceedings{Eronen2003_ICA,
    author = "Eronen, Antti",
    address = "Paris, France",
    booktitle = "Proceedings of the Seventh International Symposium on Signal Processing and its Applications",
    keywords = "instruments; HMM; ICA",
    month = "July",
    pages = "133-136",
    title = "{M}usical instrument recognition using {ICA}-based transform of features and discriminatively trained {HMM}s",
    volume = "2",
    year = "2003"
}

@article{Räsänen2015_PLOSONE,
    author = {R{\"a}s{\"a}nen, Esa and Pulkkinen, Otto and Virtanen, Tuomas and Zollner, Manfred and Hennig, Holger},
    journal = "PLoS ONE",
    number = "6",
    title = "Fluctuations of Hi-Hat Timing and Dynamics in a Virtuoso Drum Track of a Popular Music Recording",
    url = "http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0127902",
    volume = "10",
    year = "2015"
}

@inproceedings{Geiger2013_CHiME,
    author = {Geiger, J{\"u}rgen and Weninger, Felix and Hurmalainen, Antti and Gemmeke, Jort and W{\"o}llmer, Martin and Schuller, Bj{\"o}rn and Rigoll, Gerhard and Virtanen, Tuomas},
    abstract = "We present our joint contribution to the 2nd CHiME Speech Separation and Recognition Challenge. Our system combines speech enhancement by supervised sparse non-negative matrix factorisation (NMF) with a multi-stream speech recognition system. In addition to a conventional MFCC HMM recogniser, predictions by a bidirectional Long Short-Term Memory recurrent neural network (BLSTM-RNN) and from non-negative sparse classification (NSC) are integrated into a triple-stream recogniser. Experiments are carried out on the small vocabulary and the medium vocabulary recognition tasks of the Challenge. Consistent improvements over the Challenge baselines demonstrate the efficacy of the proposed system, resulting in an average word accuracy of 92.8\% in the small vocabulary task and an average word error rate of 41.42\% in the medium vocabulary task.",
    awards = "Best paper award (the 2nd CHiME workshop, 2013)",
    booktitle = "proceedings of the 2nd CHiME workshop",
    journal = "Proceedings of the 2nd CHiME workshop",
    keywords = "Long Short-Term Memory;recurrent neural networks;non-negative matrix factorisation;dynamic Bayesian networks",
    month = "June",
    pages = "25-30",
    title = "{T}he {TUM}+{TUT}+{KUL} {A}pproach to the {CH}i{ME} {C}hallenge 2013: {M}ulti-{S}tream {ASR} {E}xploiting {BLSTM} {N}etworks and {S}parse {NMF}",
    year = "2013",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/pP1\_geiger.pdf"
}

@inproceedings{Adavanne2016_DCASE,
    author = "Adavanne, Sharath and Parascandolo, Giambattista and Pertila, Pasi and Heittola, Toni and Virtanen, Tuomas",
    abstract = "In this paper, we propose the use of spatial and harmonic features in combination with long short term memory (LSTM) recurrent neural network (RNN) for automatic sound event detection (SED) task. Real life sound recordings typically have many overlapping sound events, making it hard to recognize with just mono channel audio. Human listeners have been successfully recognizing the mixture of overlapping sound events using pitch cues and exploiting the stereo (multichannel) audio signal available at their ears to spatially localize these events. Traditionally SED systems have only been using mono channel audio, motivated by the human listener we propose to extend them to use multichannel audio. The proposed SED system is compared against the state of the art mono channel method on the development subset of TUT sound events detection 2016 database [1]. The usage of spatial and harmonic features are shown to improve the performance of SED.",
    booktitle = "Detection and Classification of Acoustic Scenes and Events",
    keywords = "Sound event detection;multichannel;time difference of arrival;pitch;recurrent neural networks;long short term memory",
    title = "{S}ound event detection in multichannel audio using spatial and harmonic features",
    url = "https://dcase.community/documents/workshop2016/proceedings/Adavanne-DCASE2016workshop.pdf",
    year = "2016"
}

@article{Gómez2003_JNMR,
    author = "G{\'o}mez, Emilia and Klapuri, Anssi and Meudic, Beno{\^i}t",
    journal = "Journal of New Music Research",
    number = "1",
    title = "{M}elody {D}escription and {E}xtraction in the {C}ontext of {M}usic {C}ontent {P}rocessing",
    volume = "32",
    year = "2003"
}

@article{Gouyon2006_TASLP,
    author = "Gouyon, Fabien and Klapuri, Anssi and Dixon, Simon and Alonso, Miguel and Tzanetakis, George and Uhle, Christian and Cano, Pedro",
    journal = "IEEE Trans. Audio, Speech, and Language Processing",
    keywords = "tempo",
    month = "Sept",
    number = "5",
    pages = "1832-1844",
    title = "{A}n experimental comparison of audio tempo induction algorithms",
    url = "http://ieeexplore.ieee.org/stamp/stamp.jsp?tp={{{\\&}}}arnumber=1678001{{{\\&}}}isnumber=35293",
    volume = "14",
    year = "2006"
}

@inproceedings{Helander2007_SPECOM,
    author = "Helander, Elina and Sil{\'e}n, Hanna and Gabbouj, Moncef",
    address = "Moscow, Russia",
    booktitle = "Proceedings of the 12th International conference Speech and Computer, SPECOM",
    keywords = "speech synthesis",
    month = "October",
    pages = "293-298",
    title = "{T}he use of diphone variants in optimal text selection for {F}innish unit selection speech synthesis",
    year = "2007"
}

@inproceedings{Helander2007_ICASSP,
    author = "Helander, Elina and Nurminen, Jani",
    address = "Honolulu, Hawaii, USA",
    booktitle = "IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP",
    keywords = "voice conversion",
    month = "April",
    pages = "509-512",
    title = "{A} novel method for prosody prediction in voice conversion",
    volume = "4",
    year = "2007"
}

@inproceedings{Helander2007_InterSpecch,
    author = "Helander, Elina and Nurminen, Jani",
    address = "Antwerp, Belgium",
    booktitle = "Proceedings of the 8th Annual Conference of the International Speech Communication Association, Interspeech",
    month = "August",
    pages = "2665-2668",
    title = "{O}n the importance of pure prosody in the perception of speaker identity",
    year = "2007"
}

@inproceedings{Helander2007_SPECOM_a,
    author = "Helander, Elina and Nurminen, Jani and Gabbouj, Moncef",
    address = "Moscow, Russia",
    booktitle = "Proceedings of the 12th International conference Speech and Computer, SPECOM",
    month = "October",
    pages = "651-656",
    title = "{A}nalysis of {LSF} frame selection in voice conversion",
    year = "2007"
}

@inproceedings{Helander2008_ICASSP,
    author = "Helander, Elina and Nurminen, Jani and Gabbouj, Moncef",
    address = "Las Vegas, Nevada, USA",
    booktitle = "Proceedings of 2008 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP",
    keywords = "voice conversion",
    month = "March",
    pages = "4669-4672",
    title = "{LSF} mapping for voice conversion with very small training sets",
    year = "2008"
}

@inproceedings{Helander2008_InterSpecch,
    author = "Helander, Elina and Schwarz, Jan and Nurminen, Jani and Sil{\'e}n, Hanna and Gabbouj, Moncef",
    address = "Brisbane, Australia",
    booktitle = "roceedings of the 9th Annual Conference of the International Speech Communication Associationa, Interspeech",
    keywords = "voice conversion",
    month = "September",
    pages = "1453-1456",
    title = "{O}n the impact of alignment on voice conversion performance",
    year = "2008"
}

@inproceedings{Helén2003_DAFx,
    author = "Hel{\'e}n, Marko and Virtanen, Tuomas",
    address = "London, England",
    booktitle = "Proceedings of the 6th International Conference on Digital Audio Effects DAFx-03",
    keywords = "sinusoidal model",
    month = "September",
    pages = "249-253",
    title = "Perceptually motivated parametric representation for harmonic sounds for data compression purposes",
    year = "2003",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/DAFx03\_Helen.pdf"
}

@inproceedings{Helén2006_NORSIG 2006,
    author = "Hel{\'e}n, Marko and Lahti, Tommi",
    address = "Reykjavik, Iceland",
    booktitle = "7th Nordic Signal Processing Symposium (NORSIG 2006)",
    keywords = "query by example",
    month = "June",
    title = "{Q}uery by {E}xample {M}ethods for {A}udio {S}ignals",
    year = "2006"
}

@inproceedings{Helén2007_DAFx-07,
    author = "Hel{\'e}n, Marko and Virtanen, Tuomas",
    booktitle = "proc. 10th International Conference on Digital Audio Effects (DAFx-07)",
    month = "September",
    title = "A Similarity Measure for Audio Query by Example Based on Perceptual Coding and Compression",
    year = "2007",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/dafx\_helen4.pdf"
}

@article{Helén2009_JASM,
    author = "Hel{\'e}n, Marko and Virtanen, Tuomas",
    title = "Audio Query by Example Using Similarity Measures between Probability Density Functions of Features",
    journal = "EURASIP Journal on Audio, Speech, and Music Processing",
    volume = "2010",
    number = "1",
    pages = "1--12",
    year = "2009",
    doi = "10.1155/2010/179303"
}

@inproceedings{Hurmalainen2013_ASRU,
    author = "Hurmalainen, Antti and Virtanen, Tuomas",
    abstract = "Non-negative spectral factorisation with long temporal context has been successfully used for noise robust recognition of speech in multi-source environments. Sparse classification from activations of speech atoms can be employed instead of conventional GMMs to determine speech state likelihoods. For accurate classification, correct linguistic state labels must be assigned to speech atoms. We propose using non-negative matrix deconvolution for learning the labels with algorithms closely matching a framework that separates speech from additive noises. Experiments on the 1st CHiME Challenge corpus show improvement in recognition accuracy over labels acquired from original atom sources or previously used least squares regression. The new approach also circumvents numerical issues encountered in previous learning methods, and opens up possibilities for new speech basis generation algorithms.",
    booktitle = "Proceedings of the Automatic Speech Recognition and Understanding Workshop (ASRU)",
    keywords = "automatic speech recognition;noise robustness;non-negative matrix factorization;sparse classification",
    month = "December",
    organization = "IEEE",
    title = "Learning State Labels for Sparse Classification of Speech with Matrix Deconvolution",
    year = "2013",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/Hurmalainen\_ASRU2013.pdf"
}

@inproceedings{Hurmalainen2013_CHiME,
    author = "Hurmalainen, Antti and Gemmeke, Jort and Virtanen, Tuomas",
    abstract = "In environments containing multiple non-stationary sound sources, it becomes increasingly difficult to recognise speech from its short-time spectra alone. Long-context speech and noise models, where phonetic patterns and noise events may span hundreds of milliseconds, have been found beneficial in such separation tasks. Thus far the majority of work employing non-negative matrix factorisation to long-context spectrogram separation has been conducted on small vocabulary tasks by exploiting large speech and noise dictionaries containing thousands of atoms. In this work we study whether the previously proposed factorisation methods are applicable to more natural speech and limited noise context while keeping the model sizes practically feasible. Results are evaluated on the WSJ0 5k -based 2nd CHiME Challenge Track 2 corpus, where we achieve approximately 4\% absolute improvement in speech recognition rates compared to baseline using the proposed enhancement framework.",
    booktitle = "Proceedings of the 2nd CHiME workshop",
    keywords = "spectral factorisation;speech recognition;noise robustness",
    month = "June",
    pages = "13-18",
    title = "Compact Long Context Spectral Factorisation Models for Noise Robust Recognition of Medium Vocabulary Speech",
    year = "2013",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/pS13\_hurmalainen.pdf"
}

@inproceedings{Hurmalainen2012_EUSIPCO,
    author = "Hurmalainen, Antti and Gemmeke, Jort and Virtanen, Tuomas",
    abstract = "In real world speech processing, the signals are often continuous and consist of momentary segments of speech over non-stationary background noise. It has been demonstrated that spectral factorisation using multi-frame atoms can be successfully employed to separate and recognise speech in adverse conditions. While in previous work full knowledge of utterance endpointing and speaker identity was used for noise modelling and speech recognition, this study proposes spectral factorisation and sparse classification techniques to detect, identify, separate and recognise speech from a continuous noisy input. Speech models are trained beforehand, but noise models are acquired adaptively from the input by using voice activity detection without prior knowledge of noise-only locations. The results are evaluated on the CHiME corpus, containing utterances from 34 speakers over highly non-stationary multi-source noise.",
    address = "Bucharest, Romania",
    booktitle = "20th European Signal Processing Conference (EUSIPCO)",
    keywords = "Spectral factorization;speech recognition;speaker recognition;voice activity detection;speech separation",
    month = "August",
    organization = "European Association for Signal, Speech, and Image Processing (EURASIP)",
    pages = "2649-2653",
    title = "Detection, Separation and Recognition of Speech From Continuous Signals Using Spectral Factorisation",
    year = "2012",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/hurmalainen\_eusipco2012.pdf"
}

@inproceedings{Hurmalainen2013_EUSIPCO,
    author = "Hurmalainen, Antti and Virtanen, Tuomas",
    abstract = "Studies from multiple disciplines show that spectro-temporal units of natural languages and human speech perception are longer than short-time frames commonly employed in automatic speech recognition. Extended temporal context is also beneﬁcial for separation of concurrent sound sources such as speech and noise. However, the length of patterns in speech varies greatly, making it difﬁcult to model with ﬁxed-length units. We propose methods for acquiring variable length speech atom bases for accurate yet compact representation of speech with a large temporal context. Bases are generated from spectral features, from assigned state labels, and as a combination of both. Results for factorisation-based speech recognition in noisy conditions show equal or better separation and recognition quality in comparison to ﬁxed length units, while model sizes are reduced by up to 40\%.",
    booktitle = "Proceedings of the 21st European Signal Processing Conference (EUSIPCO)",
    keywords = "spectral factorization;speech recognition;noise robustness",
    month = "September",
    organization = "The European Association for Signal Processing (EURASIP)",
    title = "Acquiring Variable Length Speech Bases for Factorisation-Based Noise Robust Speech Recognition",
    year = "2013",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/Hurmalainen\_EUSIPCO2013.pdf"
}

@inproceedings{Hurmalainen2012_InterSpecch,
    author = "Hurmalainen, Antti and Saeidi, Rahim and Virtanen, Tuomas",
    abstract = "Spectrogram factorisation using a dictionary of spectro-temporal atoms has been successfully employed to separate a mixed audio signal into its source components. When atoms from multiple sources are included in a combined dictionary, the relative weights of activated atoms reveal likely sources as well as the content of each source. Enforcing sparsity on the activation weights produces solutions, where only a small number of atoms are active at a time. In this paper we propose using group sparsity to restrict simultaneous activation of sources, allowing us to discover the identity of an unknown speaker from multiple candidates, and further to recognise the phonetic content more reliably with a narrowed down subset of atoms belonging to the most likely speakers. An evaluation on the CHiME corpus shows that the use of group sparsity improves the results of noise robust speaker identification and speech recognition using speaker-dependent models.",
    booktitle = "13th Interspeech",
    keywords = "group sparsity; speech recognition; speaker identification; spectrogram factorization",
    organization = "International Speech Communication Association (ISCA)",
    title = "Group Sparsity for Speaker Identity Discrimination in Factorisation-based Speech Recognition",
    year = "2012",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/hurmalainen\_interspeech2012.pdf"
}

@inproceedings{Hurmalainen2015_InterSpecch,
    author = "Hurmalainen, Antti and Saeidi, Rahim and Virtanen, Tuomas",
    abstract = "Recognition and classification of speech content in everyday environments is challenging due to the large diversity of real-world noise sources, which may also include competing speech. At signal-to-noise ratios below 0 dB, a majority of features may become corrupted, severely degrading the performance of classifiers built upon clean observations of a target class. As the energy and complexity of competing sources increase, their explicit modelling becomes integral for successful detection and classification of target speech. We have previously demonstrated how non-negative compositional modelling in a spectrogram space is suitable for robust recognition of speech and speakers even at low SNRs. In this work, the sparse coding approach is extended to cover the whole separation and classification chain to recognise the speaker of short utterances in difficult noise environments. A convolutive matrix factorisation and coding system is evaluated on 2nd CHiME Track 1 data. Over 98\% average speaker recognition accuracy is achieved for shorter than three second utterances at +9 ... -6 dB SNR, illustrating the system's performance in challenging conditions.",
    booktitle = "Proceedings of 16th Interspeech",
    keywords = "speaker recognition; noise robustness; compositional models; sparse coding; non-negative matrix factorization",
    month = "September",
    title = "Noise Robust Speaker Recognition with Convolutive Sparse Coding",
    year = "2015",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/hurmalainen\_interspeech2015.pdf"
}

@phdthesis{Hurmalainen2014_phd,
    author = "Hurmalainen, Antti",
    abstract = "Communication by speech is intrinsic for humans. Since the breakthrough of mobile devices and wireless communication, digital transmission of speech has become ubiquitous. Similarly distribution and storage of audio and video data has increased rapidly. However, despite being technically capable to record and process audio signals, only a fraction of digital systems and services are actually able to work with spoken input, that is, to operate on the lexical content of speech. One persistent obstacle for practical deployment of automatic speech recognition systems is inadequate robustness against noise and other interferences, which regularly corrupt signals recorded in real-world environments. Speech and diverse noises are both complex signals, which are not trivially separable. Despite decades of research and a multitude of different approaches, the problem has not been solved to a sufficient extent. Especially the mathematically ill-posed problem of separating multiple sources from a single-channel input requires advanced models and algorithms to be solvable. One promising path is using a composite model of long-context atoms to represent a mixture of non-stationary sources based on their spectro-temporal behaviour. Algorithms derived from the family of non-negative matrix factorisations have been applied to such problems to separate and recognise individual sources like speech. This thesis describes a set of tools developed for non-negative modelling of audio spectrograms, especially involving speech and real-world noise sources. An overview is provided to the complete framework starting from model and feature definitions, advancing to factorisation algorithms, and finally describing different routes for separation, enhancement, and recognition tasks. Current issues and their potential solutions are discussed both theoretically and from a practical point of view. The included publications describe factorisation-based recognition systems, which have been evaluated on publicly available speech corpora in order to determine the efficiency of various separation and recognition algorithms. Several variants and system combinations that have been proposed in literature are also discussed. The work covers a broad span of factorisation-based system components, which together aim at providing a practically viable solution to robust processing and recognition of speech in everyday situations.",
    booktitle = "Proc. 40th International Conference on Acoustics, Speech, and Signal Processing (ICASSP)",
    keywords = "automatic speech recognition;noise robustness;non-negative matrix factorization;sparse representations;sparse classification",
    month = "October",
    title = "{R}obust {S}peech {R}ecognition with {S}pectrogram {F}actorisation",
    url = "https://tutcris.tut.fi/portal/files/1232264/hurmalainen.pdf",
    year = "2014",
    school = "Tampere University of Technology"
}

@mastersthesis{Sáez2017_master,
    author = "S{\'a}ez, Natalia Ib{\'a}{\\textasciitilde n}ez",
    abstract = "This thesis consists in the extension of the baseline system for Acoustic Scene Classification, developed by the Audio Research Group at Tampere university of Technology for the challenge of Detection and Classification of Acoustic Scenes and Events (DCASE). The baseline is based on a supervised classification approach which is composed by training and testing stages. The training stage is based on the construction of a statistical model capable to describe each of the environmental classes that will be used during the training stage. The innovation part has the goal of clustering the available observations so that each class is divided into some subclasses. The models will be created for each subclass. These models describe acoustic environments in more detail, which allows achieving higher level of accuracy. The system has preserved its previous stages and the method used for the clustering has been k-means. The experiments have been performed firstly with the development dataset and the results obtained have been validated with the challenge dataset aiming to verify that the system is capable to generalize its results. Three different approaches have been tested: First, the number of clusters has been set invariant for all the classes. Values 2, 3, 5 and 10 have been tested. The performance has increased 2\% for 2 clusters. Second, the number of clusters has been selected manually choosing the values that proved to provide better performance for each class during the development stage. The performance has increased 2.3\% with respect to the baseline. Third approach is more sophisticated and includes cluster evaluation based on BD and CH indices. This method allows calculating the number of clusters for each class automatically. It has improved the performance in 2\% with respect to the baseline.",
    institution = "Tampere University of Technology",
    title = "{A}coustic scene classification using clustering-based acoustic models",
    url = "https://dspace.cc.tut.fi/dpub/bitstream/handle/123456789/24930/Ibanez\%20Saez.pdf?sequence=1{\\&}isAllowed=y",
    year = "2017",
    school = "Tampere University of Technology"
}

@mastersthesis{Ryynänen2012_master,
    author = {Ryyn{\"a}nen, Jari-Pekka},
    abstract = {T{\"a}m{\"a} diplomity{\"o} k{\"a}sittelee avainsanojen tunnistuksen testaamista. Avainsanojen tunnistus on er{\"a}s puheentunnistuksen osa-alue, jossa tunnistetaan yksitt{\"a}isi{\"a} sanoja puhevirrasta. Tunnistuksen testaaminen tarkoittaa t{\"a}ss{\"a} ty{\"o}ss{\"a} tunnistustarkkuuden ja laskennallisen tehokkuuden mittaamista. Ty{\"o}ss{\"a} esitell{\"a}{\"a}n avainsanojen tunnistuksen testaamiseen k{\"a}ytett{\"a}v{\"a}n j{\"a}rjestelm{\"a}n rakenne, sek{\"a} j{\"a}rjestelm{\"a}{\"a}n toteutetut yksitt{\"a}iset testit. Puheentunnistuksen ja avainsanojen tunnistuksen teoriaa esitell{\"a}{\"a}n lyhyesti, jonka j{\"a}lkeen k{\"a}sitell{\"a}{\"a}n erilaisia tapoja mitata tunnistustarkkuutta ja laskennallista tehokkuutta. Ty{\"o}h{\"o}n liittyv{\"a}n testausmateriaalin, ja sen k{\"a}sittelyyn liittyvien seikkojen l{\"a}pik{\"a}ynnin j{\"a}lkeen, esitell{\"a}{\"a}n ty{\"o}n toteutuksen kannalta oleelliset ohjelmistotekniikan teoriat. T{\"a}m{\"a}n j{\"a}lkeen esitell{\"a}{\"a}n varsinainen testausj{\"a}rjestelm{\"a}. Esitelty{\"a} testausj{\"a}rjestelm{\"a}{\"a} on onnistuneesti k{\"a}ytetty avainsanojen tunnistimien testauksessa, ja k{\"a}yt{\"o}ss{\"a} tehtyj{\"a} havaintoja ja huomioita esitell{\"a}{\"a}n ty{\"o}n lopuksi.},
    institution = "Tampere University of Technology",
    title = "{A}vainsanojen tunnistusratkaisujen testaus",
    url = "https://dspace.cc.tut.fi/dpub/bitstream/handle/123456789/21057/ryynanen.pdf?sequence=3{\\&}isAllowed=y",
    year = "2012",
    school = "Tampere University of Technology"
}

@article{Nikunen2014_TASLP,
    author = "Nikunen, Joonas and Virtanen, Tuomas",
    abstract = "This paper addresses the problem of sound source separation from a multichannel microphone array capture via estimation of source spatial covariance matrix (SCM) of a short-time Fourier transformed mixture signal. In many conventional audio separation algorithms the source mixing parameter estimation is done separately for each frequency thus making them prone to errors and leading to suboptimal source estimates. In this paper we propose a SCM model which consists of a weighted sum of direction of arrival (DoA) kernels and estimate only the weights dependent on the source directions. In the proposed algorithm, the spatial properties of the sources become jointly optimized over all frequencies, leading to more coherent source estimates and mitigating the effect of spatial aliasing at high frequencies. The proposed SCM model is combined with a linear model for magnitudes and the parameter estimation is formulated in a complex-valued non-negative matrix factorization (CNMF) framework. Simulations consist of recordings done with a hand-held device sized array having multiple microphones embedded inside the device casing. Separation quality of the proposed algorithm is shown to exceed the performance of existing state of the art separation methods with two sources when evaluated by objective separation quality metrics.",
    journal = "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
    keywords = "Complex-valued NMF",
    month = "March",
    number = "3",
    pages = "727--739",
    title = "Direction of Arrival Based Spatial Covariance Model for Blind Sound Source Separation",
    volume = "22",
    year = "2014",
    doi = "10.1109/TASLP.2014.2303576",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/nikunen\_taslp2014.pdf"
}

@inproceedings{Klapuri2001_AES,
    author = "Klapuri, Anssi",
    address = "Amsterdam, Netherlands",
    booktitle = "10th Audio Engineering Society Convention",
    title = "Means of integrating audio content analysis algorithms",
    year = "2001"
}

@inproceedings{Klapuri2001_CRAC,
    author = {Klapuri, Anssi and Virtanen, Tuomas and Eronen, Antti and Sepp{\"a}nen, Jarno},
    address = "Aalborg, Denmark",
    booktitle = "Consistent {\\&} Reliable Acoustic Cues Workshop, CRAC-01",
    keywords = "transcription",
    month = "September",
    title = "Automatic transcription of musical recordings",
    year = "2001"
}

@inproceedings{Klapuri2001,
    author = {Klapuri, Anssi and Eronen, Antti and Sepp{\"a}nen, Jarno and Virtanen, Tuomas},
    address = "Ghent, Belgium",
    booktitle = "Symposium on Stochastic Modeling of Music, 14th Meeting of the FWO Research Society on Foundations of Music Research",
    month = "October",
    title = "Automatic transcription of music",
    year = "2001"
}

@article{Klapuri2003_TASP,
    author = "Klapuri, Anssi",
    journal = "IEEE Trans. Speech and Audio Processing",
    keywords = "fundamental frequency estimation",
    number = "6",
    pages = "804-816",
    title = "{M}ultiple fundamental frequency estimation by harmonicity and spectral smoothness",
    volume = "11",
    year = "2003"
}

@article{Klapuri2004_JNMR,
    author = "Klapuri, Anssi",
    journal = "Journal of New Music Research",
    keywords = "music transcription",
    month = "September",
    number = "3",
    pages = "269-282",
    title = "{A}utomatic music transcription as we know it today",
    volume = "33",
    year = "2004"
}

@inproceedings{Klapuri2005_EUSIPCO,
    author = "Klapuri, Anssi and Virtanen, Tuomas and Hel{\'e}n, Marko",
    address = "Antalya, Turkey",
    booktitle = "Proc. European signal processing conference",
    keywords = "instruments; interpolating state model",
    title = "Modeling musical sounds with an interpolating state model",
    year = "2005",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/ism.pdf"
}

@inproceedings{Klapuri2005_WASPAA,
    author = "Klapuri, Anssi",
    address = "New Paltz, New York",
    booktitle = "Proc. IEEE Workshop on Applications of Signal Processing to Audio and Acoustics",
    month = "Oct",
    title = "{A} perceptually motivated multiple-{F}0 estimation method",
    year = "2005"
}

@article{Klapuri2006_TASLP,
    author = "Klapuri, Anssi and Eronen, Antti and Astola, Jaakko",
    journal = "IEEE Trans. Audio, Speech, and Language Processing",
    keywords = "meter",
    number = "1",
    title = "Analysis of the meter of acoustic musical signals",
    volume = "14",
    year = "2006"
}

@inproceedings{Klapuri2006_ISMIR-06,
    author = "Klapuri, Anssi",
    address = "Victoria, Canada",
    booktitle = "7th International Conference on Music Information Retrieval (ISMIR-06)",
    month = "Oct",
    title = "Multiple fundamental frequency estimation by summing harmonic amplitudes",
    year = "2006"
}

@inproceedings{Klapuri2007_ICASSP,
    author = "Klapuri, Anssi",
    address = "Hawaii, USA",
    booktitle = "IEEE International Conference on Audio, Speech and Signal Processing (ICASSP)",
    keywords = "instruments; source-filter-decay model",
    title = "{A}nalysis of musical instrument sounds by source-filter-decay model",
    year = "2007"
}

@inproceedings{Klapuri2009_ISMIR 2009,
    author = "Klapuri, Anssi",
    address = "Kobe, Japan",
    booktitle = "Proc. 10th Int. Society for Music Information Retrieval Conf. (ISMIR 2009)",
    title = "{A} method for visualizing the pitch content of polyphonic music signals",
    year = "2009"
}

@inproceedings{Klapuri2009_SMC,
    author = "Klapuri, Anssi",
    address = "Porto, Portugal",
    booktitle = "6th Sound and Music Computing Conference",
    title = "{A} classification approach to multipitch analysis",
    year = "2009"
}

@inproceedings{Koppinen1997_IMTC,
    author = "Koppinen, Konsta and Yli-Hietanen, Jari and Astola, Jaakko",
    address = "Ottawa, Canada",
    booktitle = "IMTC Proceedings",
    pages = "54-59",
    title = "{O}ptimization of generalized predictors",
    volume = "1",
    year = "1997"
}

@inproceedings{Koppinen1998_EUSIPCO,
    author = {Koppinen, Konsta and Yli-Hietanen, Jari and H{\"a}ndel, P.},
    editor = "Theodoridis, S. et al.",
    booktitle = "Proceedings of EUSIPCO'98, 9th European Signal Processing Conference",
    pages = "161-164",
    title = "{D}esign of {M}ulti-{D}elay {P}redictive {F}ilters {U}sing {D}ynamic {P}rogramming",
    volume = "1",
    year = "1998"
}

@inproceedings{Koppinen2000_EUSIPCO,
    author = "Koppinen, Konsta and Astola, Jaakko",
    address = "Tampere, Finland",
    booktitle = "Signal Processing X Theories and Applications, Proceedings of EUSIPCO 2000, 10th European Signal Processing Conference",
    month = "September",
    pages = "2457-2460",
    title = "{G}eneralized {IIR} polynomial predictive filters",
    year = "2000"
}

@inproceedings{Koppinen2003_ICASSP,
    author = "Koppinen, Konsta",
    address = "Hong Kong",
    booktitle = "IEEE Proceedings of 2003 International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2003",
    month = "April",
    pages = "265-268",
    title = "{D}esign of narrowband fir filters with minimal noise gain using complex interpolation",
    year = "2003"
}

@article{Koppinen2004_SP_a,
    author = "Koppinen, Konsta",
    journal = "Signal Processing",
    pages = "549-560",
    title = "{S}ignal {P}rocessing",
    year = "2004"
}

@inproceedings{Korhonen2005_FINSIG,
    author = {Korhonen, Teemu and Pertil{\"a}, Pasi and Visa, Ari},
    address = "Kuopio, Finland",
    booktitle = "Proceedings of the 2005 Finnish Signal Processing Symposium - FINSIG'05",
    month = "August",
    pages = "12-15",
    title = "{P}article filtering in high clutter environment",
    year = "2005"
}

@inproceedings{Korhonen2007,
    author = {Korhonen, Teemu and Pertil{\"a}, Pasi},
    address = "Baltimore, USA",
    booktitle = "Multimodal Technologies for Perception of Humans, International Evaluation Workshops CLEAR 2007 and RT 2007",
    keywords = "Speaker tracking",
    month = "May",
    pages = "104-112",
    publisher = "LNCS",
    title = "{TUT} acoustic source tracking system 2007",
    year = "2007"
}

@inproceedings{Korhonen2008_LNCS,
    author = {Korhonen, Teemu and Pertil{\"a}, Pasi},
    booktitle = "Lecture Notes in Computer Science",
    pages = "104-112",
    title = "{TUT} acoustic source tracking system 2007",
    volume = "4625",
    year = "2008"
}

@inproceedings{Lahti2008_MIR 2008,
    author = {Lahti, Tommi and Hel{\'e}n, Marko and Vuorinen, Olli and V{\"a}yrynen, Eero and Partala, Juha and Peltola, Johannes and M{\"a}kel{\"a}, Satu-Marja},
    abstract = "State-of-the-art automatic analysis tools for personal audio con-tent management are discussed in this paper. Our main target is to create a system, which has several co-operating management tools for audio database and which improve the results of each other. Bayesian networks based audio classification algorithm provides classification into four main audio classes (silence, speech, music, and noise) and serves as a first step for other subsequent analysis tools. For speech analysis we propose an improved Bayesian information criterion based speaker segmen-tation and clustering algorithm applying also a combined gender and emotion detection algorithm utilizing prosodic features. For the other main classes it is often hard to device any general and well functional pre-categorization that would fit the unforesee-able types of user recorded data. For compensating the absence of analysis tools for these classes we propose the use of efficient audio similarity measure and query-by-example algorithm with database clustering capabilities. The experimental results show that the combined use of the algorithms is feasible in practice.",
    address = "Vancouver, Canada",
    booktitle = "ACM International Conference on Multimedia Information Retrieval (MIR 2008)",
    keywords = "audio content management",
    month = "October",
    title = "{O}n {E}nabling {T}echniques for {P}ersonal {A}udio {C}ontent {M}anagement",
    year = "2008"
}

@inproceedings{Löytynoja2009_EUSIPCO,
    author = {L{\"o}ytynoja, Antti and Pertil{\"a}, Pasi},
    address = "Glasgow, Scotland, UK",
    booktitle = "Proceedings of the 17th European Signal Processing Conference, Eusipco",
    keywords = "speaker tracking",
    month = "August",
    pages = "1418-1422",
    title = "{A} real-time talker localization implementation using multi-{PHAT} and particle filter",
    year = "2009"
}

@article{McKinney2007_JNMR,
    author = "McKinney, M. and Moelants, D. and Davies, M. E. P. and Klapuri, Anssi",
    abstract = "This is an extended analysis of eight different algorithms for musical tempo extraction and beat tracking. The algorithms participated in the 2006 Music Informa- tion Retrieval Evaluation eXchange (MIREX), where they were evaluated using a set of 140 musical excerpts, each with beats annotated by 40 different listeners. Performance metrics were constructed to measure the algorithms’ abilities to predict the most perceptually salient musical beats and tempi of the excerpts. Detailed results of the evaluation are presented here and algorithm performance is evaluated as a function of musical genre, the presence of percussion, musical meter and the most salient perceptual tempo of each excerpt.",
    journal = "Journal of New Music Research",
    number = "1",
    pages = "1-16",
    title = "{E}valuation of audio beat tracking and music tempo extraction algorithms",
    volume = "36",
    year = "2007"
}

@inproceedings{Mesaros2007_ISMIR,
    author = "Mesaros, Annamaria and Virtanen, Tuomas and Klapuri, Anssi",
    address = "Vienna, Austria",
    booktitle = "International Conference on Music Information Retrieval",
    keywords = "singer identification; separation",
    title = "Singer Identification in Polyphonic Music Using Vocal Separation and Pattern Recognition Methods",
    year = "2007",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/ismir2007mesaros.pdf"
}

@inproceedings{Mesaros2015_ICASSP,
    author = "Mesaros, Annamaria and Heittola, Toni and Dikmen, Onur and Virtanen, Tuomas",
    abstract = "Methods for detection of overlapping sound events in audio involve matrix factorization approaches, often assigning separated components to event classes. We present a method that bypasses the supervised construction of class models. The method learns the components as a non-negative dictionary in a coupled matrix factorization problem, where the spectral representation and the class activity annotation of the audio signal share the activation matrix. In testing, the dictionaries are used to estimate directly the class activations. For dealing with large amount of training data, two methods are proposed for reducing the size of the dictionary. The methods were tested on a database of real life recordings, and outperformed previous approaches by over 10\%.",
    booktitle = "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    keywords = "Dictionaries;Training;Testing;Event detection;Context;Acoustics;Accuracy;coupled non-negative matrix factorization;non-negative dictionaries;sound event detection",
    pages = "151-155",
    publisher = "IEEE",
    title = "Sound event detection in real life recordings using coupled matrix factorization of spectral representations and class activity annotations",
    doi = "10.1109/ICASSP.2015.7177950",
    year = "2015",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/mesaros\_icassp2015.pdf"
}

@article{Mesaros2016_AS,
    author = "Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas",
    abstract = "This paper presents and discusses various metrics proposed for evaluation of polyphonic sound event detection systems used in realistic situations where there are typically multiple sound sources active simultaneously. The system output in this case contains overlapping events, marked as multiple sounds detected as being active at the same time. The polyphonic system output requires a suitable procedure for evaluation against a reference. Metrics from neighboring fields such as speech recognition and speaker diarization can be used, but they need to be partially redefined to deal with the overlapping events. We present a review of the most common metrics in the field and the way they are adapted and interpreted in the polyphonic case. We discuss segment-based and event-based definitions of each metric and explain the consequences of instance-based and class-based averaging using a case study. In parallel, we provide a toolbox containing implementations of presented metrics.",
    journal = "Applied Sciences",
    number = "6",
    pages = "162",
    title = "{M}etrics for polyphonic sound event detection",
    url = "http://www.mdpi.com/2076-3417/6/6/162",
    volume = "6",
    year = "2016",
    doi = "10.3390/app6060162"
}

@inproceedings{Mikkonen2000_EUSIPCO,
    author = "Mikkonen, Tomi and Koppinen, Konsta",
    address = "Tampere, Finland",
    booktitle = "Eusipco 2000, X European Signal Processing Conference",
    month = "September",
    pages = "825-828",
    title = "{S}oft-{D}ecision {D}ecoding of {B}inary {B}lock {C}odes in {C}elp {S}peech {C}oding",
    year = "2000"
}

@inproceedings{Malik2017_SMC,
    author = "Malik, Miroslav and Adavanne, Sharath and Drossos, Konstantinos and Virtanen, Tuomas and Ticha, Dasa and Jarina, Roman",
    abstract = "This paper studies the emotion recognition from musical tracks in the 2-dimensional valence-arousal (V-A) emotional space. We propose a method based on convolutional (CNN) and recurrent neural networks (RNN), having significantly fewer parameters compared with state-of-the-art (SOTA) method for the same task. We utilize one CNN layer followed by two branches of RNNs trained separately for arousal and valence. The method was evaluated using the “MediaEval2015 emotion in music” dataset. We achieved an RMSE of 0.202 for arousal and 0.268 for valence, which is the best result reported on this dataset",
    booktitle = "Sound and Music Computing Conference",
    title = "{S}tacked convolutional and recurrent neural networks for music emotion recognition",
    url = "https://arxiv.org/pdf/1706.02292.pdf",
    year = "2017"
}

@inproceedings{Mäkinen2009_ICSIPA,
    author = {M{\"a}kinen, Toni and Pertil{\"a}, Pasi and Auranen, Pasi},
    address = "Kuala Lumpur, Malaysia",
    booktitle = "Proceedings of 2009 IEEE International Conference on Signal and Image Processing Applications, ICSIPA",
    keywords = "bullet state",
    month = "November",
    title = "{S}upersonic bullet state estimation using particle filtering",
    year = "2009"
}

@mastersthesis{Naithani2015_master,
    author = "Naithani, Gaurav",
    abstract = "Crying is the first means of communication for an infant through which it expresses its physiological and psychological needs. Infant cry analysis is the investigation of infant cry vocalizations in order to extract social and communicative information about infant behavior, and diagnostic information about infant health. This thesis is part of a larger study whose objective is to analyze the acoustic properties of infant cry signals and use it for early assessment of neurological developmental issues in infants. This thesis deals with two research problems in the context of infant cry signals: audio segmentation of cry recordings in order to extract relevant acoustic parts, and fundamental frequency (F0) estimation of the extracted acoustic regions. The extracted acoustic regions are relevant for extracting parameters useful for drawing correlation with developmental outcomes of the infants. Fundamental frequency (F0) is one such potentially useful parameter whose variation has been found to correlate with cases of neurological insults in infants. The cry recordings are captured in realistic hospital environments under varied contexts like infant crying out of hunger, pain etc. A hidden Markov model (HMM) based audio segmentation system is proposed. The performance of the system is evaluated for different configurations of HMM states, number of component Gaussians, and using different combinations of audio features. Frame based accuracy of 88.5 \% is achieved. YIN algorithm, a popular F0 estimation algorithm, is utilized to deal with the fundamental frequency estimation problem, and a method to discard unreliable F0 estimates is suggested. The statistics associated with distribution of F0 estimates corresponding to different components of cry signals are reported. This work would be followed up to find meaningful correlations between extracted F0 estimates and developmental outcomes of the infants. Moreover, other acoustic parameters would also be investigated for the same purpose.",
    institution = "Tampere University of Technology",
    title = "{A}coustic {A}nalysis of {I}nfant {C}ry {S}ignals",
    url = "https://dspace.cc.tut.fi/dpub/bitstream/handle/123456789/23141/Naithani.pdf?sequence=3{\\&}isAllowed=y",
    year = "2015",
    school = "Tampere University of Technology"
}

@mastersthesis{Lou2014_master,
    author = "Lou, Samuel Navarro",
    abstract = "Emotions in speech are a fundamental part of a natural dialog. In everyday life, vocal interaction with people often implies emotions as an intrinsic part of the conversation to a greater or lesser extent. Thus, the inclusion of emotions in human-machine dialog systems is crucial to achieve an acceptable degree of naturalness in the communication. This thesis focuses on automatic emotion conversion of speech, a technique whose aim is to transform an utterance produced in neutral style to a certain emotion state in a speaker independent context. Conversion of emotions represents a challenge in the sense that emotions a affect significantly all the parts of the human vocal production system, and in the conversion process all these factors must be taken into account carefully. The techniques used in the literature are based on voice conversion approaches, with minor modifications to create the sensation of emotion. In this thesis, the idea of voice conversion systems is used as well, but the usual regression process is divided in a two-step procedure that provides additional speaker normalization to remove the intrinsic speaker dependency of this kind of systems, using vocal tract length normalization as a pre-processing technique. In addition, a new method to convert the duration trend of the utterance and the intonation contour is proposed, taking into account the contextual information.",
    institution = "Tampere University of Technology",
    title = "{A}utomatic {C}onversion of {E}motions in {S}peech within a {S}peaker {I}ndependent {F}ramework",
    url = "https://dspace.cc.tut.fi/dpub/bitstream/handle/123456789/22171/Navarro.pdf?sequence=1{\\&}isAllowed=y",
    year = "2014",
    school = "Tampere University of Technology"
}

@mastersthesis{Parascandolo2015_master,
    author = "Parascandolo, Giambattista",
    abstract = "The objective of this thesis is to investigate how a deep learning model called recurrent neural network (RNN) performs in the task of detecting overlapping sound events in real life environments. Examples of such sound events include dog barking, footsteps, and crowd applauding. When several sound sources are active simultaneously, as it is often the case in everyday contexts, identifying individual sound events from their polyphonic mixture is a challenging task. Other factors such as noise and distortions contribute to making even more difficult to explicitly implement a computer program to solve the detection task. We present an approach to polyphonic sound event detection in real life recordings based on a RNN architecture called bidirectional long short term memory (BLSTM). A multilabel BLSTM RNN is trained to map the time-frequency representation of a mixture signal consisting of sounds from multiple sources, to binary activity indicators of each event class. Our method is tested on two large databases of recordings, both containing sound events from more than 60 different classes, and in one case from 10 different everyday contexts. Furthermore, in order to reduce overfitting we propose to use several data augmentation techniques: time stretching, sub-frame time shifting, and block mixing. The proposed approach outperforms the previous state-of-the-art method, despite using half of the parameters, and the results are further largely improved using the block mixing data augmentation technique. Overall, for the first dataset our approach reports an average F1-score of 65.5\% on 1 second blocks and 64.7\% on single frames, a relative improvement over previous state-of-the-art approach of 6.8\% and 15.1\% respectively. For the second dataset our system reports an average F1- score of 84.4\% on 1 second blocks and 85.1\% on single frames, a relative improvement over the baseline approach of 38.4\% and 35.9\% respectively.",
    institution = "Tampere University of Technology",
    title = "{R}ecurrent neural networks for polyphonic sound event detection",
    url = "https://dspace.cc.tut.fi/dpub/bitstream/handle/123456789/23570/Parascandolo.pdf?sequence=1{\\&}isAllowed=y",
    year = "2015",
    school = "Tampere University of Technology"
}

@inproceedings{Parviainen2003_ISPACS,
    author = "Parviainen, Mikko and Virtanen, Tuomas",
    address = "Awaji Island, Japan",
    booktitle = "Proceedings of 2003 IEEE International Symposium on Intelligent Signal Processing and Communication Systems, ISPACS 2003",
    keywords = "sinusoidal model",
    month = "December",
    pages = "127-132",
    title = "Two-channel separation of speech using direction-of-arrival estimation and sinusoids plus transients modeling",
    year = "2003",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/ispacs03.pdf"
}

@inproceedings{Parviainen2005_NSIP,
    author = {Parviainen, Mikko and Pertil{\"a}, Pasi and Korhonen, Teemu and Visa, Ari},
    address = "Sapporo, Japan",
    booktitle = "Proceedings of International Workshop on Nonlinear Signal and Image Processing, NSIP",
    month = "May",
    pages = "468-473",
    title = "{A} spatiotemporal approach for passive sound source localization - real-world experiments",
    year = "2005"
}

@inproceedings{Parviainen2006_MLMI,
    author = {Parviainen, Mikko and Pirinen, Tuomo and Pertil{\"a}, Pasi},
    booktitle = "Machine Learning for Multimodal Interaction, the Third International Workshop, MLMI",
    keywords = "Speaker tracking",
    pages = "225-235",
    title = "{A} speaker localization system for lecture room environment",
    volume = "4299",
    year = "2006"
}

@inproceedings{Adavanne2017_ICASSP 2017,
    author = "Adavanne, Sharath and Pertilä, Pasi and Virtanen, Tuomas",
    abstract = "This paper proposes to use low-level spatial features extracted from multichannel audio for sound event detection. We extend the convolutional recurrent neural network to handle more than one type of these multichannel features by learning from each of them separately in the initial stages. We show that instead of concatenating the features of each channel into a single feature vector the network learns sound events in multichannel audio better when they are presented as separate layers of a volume. Using the proposed spatial features over monaural features on the same network gives an absolute F-score improvement of 6.1\% on the publicly available TUT-SED 2016 dataset and 2.7\% on the TUT-SED 2009 dataset that is fifteen times larger",
    booktitle = "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2017)",
    keywords = "Sound event detection;multichannel audio;spatial features;convolutional recurrent neural network",
    title = "{S}ound event detection using spatial features and convolutional recurrent neural network",
    url = "https://arxiv.org/pdf/1706.02291.pdf",
    year = "2017"
}

@inproceedings{Pertilä2003_SPIE,
    author = {Pertil{\"a}, Pasi and Pirinen, Tuomo and Visa, Ari and Korhonen, Teemu},
    address = "Orlando, Florida, USA",
    booktitle = "Proceedings of SPIE, Unattended Ground Sensor Technologies and Applications V",
    month = "April",
    pages = "9-17",
    title = "{C}omparison of three post-processing methods for acoustic localization",
    year = "2003"
}

@inproceedings{Pertilä2004_ICA,
    author = {Pertil{\"a}, Pasi and Parviainen, Mikko and Korhonen, Teemu and Visa, Ari},
    address = "Sapporo, Japan",
    booktitle = "Proceedings of International Symposium on Communications and Information Technologies 2004, ISCIT",
    month = "October",
    pages = "1150-1154",
    title = "{A} spatiotemporal approach to passive sound source localization",
    year = "2004"
}

@inproceedings{Pertilä2005_ICA,
    author = {Pertil{\"a}, Pasi and Parviainen, Mikko and Korhonen, Teemu and Visa, Ari},
    address = "Hong Kong",
    booktitle = "Proceedings of 2005 International Symposium on Intelligent Signal Processing and Communication Systems, ISPACS",
    month = "December",
    pages = "745-748",
    title = "{M}oving sound source localization in large areas",
    year = "2005"
}

@inproceedings{Pertilä2007_LNCS,
    author = {Pertil{\"a}, Pasi and Korhonen, Teemu and Pirinen, Tuomo and Parviainen, Mikko},
    booktitle = "Lecture Notes in Computer Science",
    keywords = "Speaker tracking",
    pages = "127-136",
    title = "{TUT} acoustic source tracking system 2006",
    volume = "4122",
    year = "2007"
}

@inproceedings{Pertilä2007_ICASSP,
    author = {Pertil{\"a}, Pasi and Parviainen, Mikko},
    address = "Honolulu, Hawaii, USA",
    booktitle = "Proceedings of 2007 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP",
    month = "April",
    pages = "497-500",
    title = "{R}obust speaker localization in meeting room domain",
    volume = "4",
    year = "2007"
}

@inproceedings{Pertilä2008,
    author = {Pertil{\"a}, Pasi},
    address = "Pacific Grove, California, USA",
    booktitle = "Proceedings of the Forty-Second Asilomar Conference on Signals, Systems and Computers",
    month = "October",
    pages = "298-302",
    title = "{A}rray steered response time-alignment for propagation delay compensation for acoustic localization",
    year = "2008"
}

@article{Pertilä2008_JASM,
    author = {Pertil{\"a}, Pasi and Korhonen, Teemu and Visa, Ari},
    journal = "Eurasip Journal on Audio, Speech, and Music Processing",
    keywords = "Speaker tracking",
    number = "278185",
    title = "{M}easurement combination for acoustic source localization in a room environment",
    url = "http://www.hindawi.com/journals/asmp/aip.278185.html",
    volume = "2008",
    year = "2008"
}

@inproceedings{Pertilä2011_HSCMA'11,
    author = {Pertil{\"a}, Pasi and Mieskolainen, Mikael and H{\"a}m{\"a}l{\"a}inen, Matti S.},
    abstract = "The utilization of distributed microphone arrays in many speech processing applications such as beamforming and speaker localization rely on the precise knowledge of microphone locations. Several self- localization approaches have been presented in the literature but still a simple, accurate, and robust method for asynchronous devices is lacking. This work presents an analytical solution for estimating the positions and rotations of asynchronous loudspeaker equipped microphone arrays or devices. The method is based on emitting and receiving calibration signals from each device, and extracting the time of arrival (TOA) values. Utilizing the knowledge of array geometry in the TOA estimation is proposed to improve accuracy of translation. Results with measurements using four devices on a table surface demonstrates a mean translation error of 11 mm with standard deviation of 6 mm and mean z-axis rotation error of 0.11 (rad) with a standard deviation of 0.14 (rad) in contrast to computer vision annotations with 200 rotations and translation estimates.",
    booktitle = "In Proc. The Third Joint Workshop on Hands-free Speech Communication and Microphone Arrays (HSCMA'11)",
    keywords = "self-localization; self localization; microphone arrays; calibra- tion; localization; computer vision",
    title = "Closed-Form Self-Localization of Asynchronous Microphone Arrays",
    year = "2011"
}

@inproceedings{Pertusa2004_ICA,
    author = "Pertusa, Antonio and Klapuri, Anssi and I{\ n}esta, J.M.",
    editor = "Alberto Sanfeliu, Manuel Lazo",
    address = "Havana, Cuba",
    booktitle = "Progress in Pattern Recognition, Image Analysis and Applications: 10th Iberoamerican Congress on Pattern Recognition",
    pages = "869-879",
    title = "{R}ecognition of note onsets in digital music using semitone bands",
    url = "http://www.springerlink.com/content/q61t720x6575/",
    volume = "3773/2005",
    year = "2004"
}

@inproceedings{Pirinen2003_ICASSP,
    author = {Pirinen, Tuomo and Pertil{\"a}, Pasi and Visa, Ari},
    address = "Hong Kong",
    booktitle = "IEEE Proceedings of 2003 International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2003",
    month = "Hong Kong",
    title = "{T}oward intelligent sensors - reliability for time delay based direction of arrival estimates",
    year = "2003"
}

@inproceedings{Pirinen2003_SPIE,
    author = {Pirinen, Tuomo and Pertil{\"a}, Pasi and Visa, Ari},
    address = "Orlando, Florida, USA",
    booktitle = "Proceedings of SPIE, Unattended Ground Sensor Technologies and Applications V",
    month = "April",
    pages = "18-29",
    title = "{A} new method for outlier removal in time delay based direction of arrival estimates",
    year = "2003"
}

@inproceedings{Pirinen2004_ISCAS,
    author = {Pirinen, Tuomo and Yli-Hietanen, Jari and Pertil{\"a}, Pasi and Visa, Ari},
    address = "Vancouver, Canada",
    booktitle = "Proceedings of 2004 IEEE International Symposium on Circuits and Systems, ISCAS",
    month = "May",
    pages = "872 - 875",
    title = "Detection and compensation of sensor malfunction in time delay based direction of arrival estimation",
    year = "2004"
}

@inproceedings{Pirinen2005,
    author = {Pirinen, Tuomo and Pertil{\"a}, Pasi and Parviainen, Mikko},
    address = "Edinburgh, UK",
    booktitle = "Proceedings of the Rich Transcription 2005 Spring Meeting Recognition Evaluation",
    month = "July",
    pages = "93-99",
    title = "{T}he {TUT} 2005 source localization system",
    year = "2005"
}

@article{Pirinen2007,
    author = {Pirinen, Tuomo and Pertil{\"a}, Pasi and Korhonen, Teemu},
    journal = "Prosessori",
    number = "2",
    pages = "46-47",
    title = "{S}einille kasvaa korvat",
    year = "2007"
}

@inproceedings{Popa2012_ICASSP,
    author = "Popa, Victor and Sil{\'e}n, Hanna and Nurminen, Jani and Gabbouj, Moncef",
    abstract = "Many popular approaches to spectral conversion involve linear transformations determined for particular acoustic classes and compute the converted result as linear combination between different local transformations in an attempt to ensure a continuous conversion. These methods often produce over-smoothed spectra and parameter tracks. The proposed method computes an individual linear transformation for every feature vector based on a small neighborhood in the acoustic space thus preserving local details. The method effectively reduces the over-smoothing by eliminating undesired contributions from acoustically remote regions. The method is evaluated in listening tests against the well-known Gaussian Mixture Model based conversion, representative for the class of methods involving linear transformations. Perceptual results indicate a clear preference for the proposed scheme.",
    address = "Kyoto",
    booktitle = "ICASSP",
    keywords = "voice conversion;local linear transformation",
    month = "March",
    organization = "IEEE",
    title = "{L}ocal {L}inear {T}ransformation for {V}oice {C}onversion",
    year = "2012"
}

@inproceedings{Ryynänen2004_ISCA,
    author = {Ryyn{\"a}nen, Matti and Klapuri, Anssi},
    booktitle = "Proc. ISCA Tutorial and Research Workshop on Statistical and Perceptual Audio Processing",
    title = "Modelling of note events for singing transcription",
    year = "2004"
}

@inproceedings{Ryynänen2005_WASPAA,
    author = {Ryyn{\"a}nen, Matti and Klapuri, Anssi},
    address = "New Paltz, New York",
    booktitle = "Proc. IEEE Workshop on Applications of Signal Processing to Audio and Acoustics",
    month = "Oct",
    title = "Polyphonic music transcription using note event modeling",
    year = "2005"
}

@inproceedings{Ryynänen2006_ISMIR 2006,
    author = {Ryyn{\"a}nen, Matti and Klapuri, Anssi},
    address = "Victoria, Canada",
    booktitle = "Proc. 7th International Conference on Music Information Retrieval (ISMIR 2006)",
    month = "October",
    title = "{T}ranscription of the {S}inging {M}elody in {P}olyphonic {M}usic",
    year = "2006"
}

@inproceedings{Ryynänen2007_ICASSP,
    author = {Ryyn{\"a}nen, Matti and Klapuri, Anssi},
    address = "Hawaii, USA",
    booktitle = "IEEE International Conference on Audio, Speech and Signal Processing (ICASSP)",
    title = "{A}utomatic bass line transcription from streaming polyphonic audio",
    year = "2007"
}

@inproceedings{Ryynänen2008_ICASSP'08,
    author = {Ryyn{\"a}nen, Matti and Klapuri, Anssi},
    address = "Las Vegas, Nevada, USA",
    booktitle = "Proceedings of 2008 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP'08)",
    month = "April",
    title = "{Q}uery by {H}umming of {MIDI} and {A}udio {U}sing {L}ocality {S}ensitive {H}ashing",
    year = "2008"
}

@article{Ryynänen2008_CMJ,
    author = {Ryyn{\"a}nen, Matti and Klapuri, Anssi},
    journal = "Computer Music Journal",
    keywords = "Transcription",
    number = "3",
    title = "{A}utomatic {T}ranscription of {M}elody, {B}ass {L}ine, and {C}hords in {P}olyphonic {M}usic",
    volume = "32",
    year = "2008"
}

@inproceedings{Ryynänen2008_ICME,
    author = {Ryyn{\"a}nen, Matti and Virtanen, Tuomas and Paulus, Jouni and Klapuri, Anssi},
    address = "Hannover, Germany",
    booktitle = "IEEE International Conf. on Multimedia and Expo",
    month = "June",
    title = "Accompaniment separation and karaoke application based on automatic melody transcription",
    year = "2008",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/ryynanen\_icme08.pdf"
}

@mastersthesis{Gasulla2014_master,
    author = "Gasulla, Gerard Sanchez",
    abstract = "Speech is the basis of human communication: in everyday life we automatically decode speech into language regardless of who speaks. In a similar way, we have the ability to recognize di erent speakers, despite the linguistic content of the speech. Additionally to the voice individuality of the speaker, the particular prosody of speech involves relevant information concerning the identity, age, social group or economical status of the speaker, helping us identify the person to whom we are talking without seeing the speaker. Voice conversion systems deal with the conversion of a speech signal to sound as if it was uttered by another speaker. It has been an important amount of work in the conversion of the timber of the voice, the spectral features, meanwhile the conversion of pitch and the way it temporarily evolves, modeling the speaker dependent prosody, is mostly achieved by just controlling the level and range. This thesis focuses on prosody conversion, proposing an approach based on a wavelet transformation of the pitch contours. It has been performed a study of the wavelet domain, discerning among the di erent timing of the prosodic events, thus allowing an improved modeling of them. Consequently, the prosody conversion is achieved in the wavelet domain, using regression techniques originally developed for the spectral features conversion, in voice conversion systems.",
    school = "Tampere University of Technology",
    title = "{P}rosody and {W}avelets: {T}owards a natural speaking style conversion",
    url = "https://dspace.cc.tut.fi/dpub/bitstream/handle/123456789/22170/SanchezGasulla.pdf?sequence=1{\\&}isAllowed=y",
    year = "2014"
}

@inproceedings{Silén2007_ISCA,
    author = "Sil{\'e}n, Hanna and Helander, Elina and Koppinen, Konsta and Gabbouj, Moncef",
    address = "Bonn, Gemany",
    booktitle = "Proceedings of the 6th ISCA Workshop on Speech Synthesis",
    keywords = "speech synthesis",
    month = "August",
    pages = "310-315",
    title = "{B}uilding a {F}innish unit selection {TTS} system",
    year = "2007"
}

@inproceedings{Silén2008_InterSpecch,
    author = "Sil{\'e}n, Hanna and Helander, Elina and Nurminen, Jani and Gabbouj, Moncef",
    address = "Brisbane, Australia",
    booktitle = "Proceedings of the 9th Annual Conference of the International Speech Communication Associationa, Interspeech",
    keywords = "speech synthesis",
    month = "September",
    pages = "1853-1856",
    title = "{E}valuation of {F}innish unit selection and {HMM}-based speech synthesis",
    year = "2008"
}

@inproceedings{Silén2009_InterSpecch,
    author = "Sil{\'e}n, Hanna and Helander, Elina and Nurminen, Jani and Gabbouj, Moncef",
    abstract = "HMM-based speech synthesis offers a way to generate speech with different voice qualities. However, sometimes databases contain certain inherent voice qualities that need to be parametrized properly. One example of this is vocal fry typically occurring at the end of utterances. A popular mixed excitation vocoder for HMMbased speech synthesis is STRAIGHT. The standard STRAIGHT is optimized for modal voices and may not produce high quality with other voice types. Fortunately, due to the flexibility of STRAIGHT, different F0 and aperiodicity measures can be used in the synthesis without any inherent degradations in speech quality. We have replaced the STRAIGHT excitation with a representation based on a robust F0 measure and a carefully determined two-band voicing. According to our analysis-synthesis experiments, the new parameterization can improve the speech quality. In HMM-based speech synthesis, the quality is significantly improved especially due to the better modeling of vocal fry.",
    address = "Brighton, UK",
    booktitle = "Proceedings of the 10th Annual Conference of the International Speech Communication Associationa, Interspeech",
    keywords = "speech synthesis",
    month = "September",
    pages = "1775-1778",
    title = "{P}arameterization of vocal fry in {HMM}-based speech synthesis",
    url = "http://www.isca-speech.org/archive/interspeech\_2009/i09\_1775.html",
    year = "2009"
}

@inproceedings{Silén2010_InterSpecch,
    author = "Sil{\'e}n, Hanna and Helander, Elina and Nurminen, Jani and Koppinen, Konsta and Gabbouj, Moncef",
    abstract = "In hidden Markov model-based unit selection synthesis, the beneﬁts of both unit selection and statistical parametric speech synthesis are combined. However, conventional Viterbi algorithm is forced to do a selection also when no suitable units are available. This can drift the search and decrease the overall quality. Consequently, we propose to use robust Viterbi algorithm that can simultaneously detect bad units and select the best sequence. The unsuitable units are replaced using hidden Markov model-based synthesis. Evaluations indicate that the use of robust Viterbi algorithm combined with unit replacement increases the quality compared to the traditional algorithm.",
    booktitle = "Interspeech 2010",
    keywords = "speech synthesis;robust Viterbi algorithm;unit selection;hidden Markov models",
    title = "{U}sing {R}obust {V}iterbi {A}lgorithm and {HMM}-{M}odeling in {U}nit {S}election {TTS} to {R}eplace {U}nits of {P}oor {Q}uality",
    year = "2010"
}

@inproceedings{Sillanpää2000_EUSIPCO,
    author = {Sillanp{\"a}{\"a}, Jukka and Klapuri, Anssi and Sepp{\"a}nen, Jarno and Virtanen, Tuomas},
    booktitle = "Proceedings of the European Signal Processing Conference EUSIPCO",
    title = "Recognition of acoustic noise mixtures by combined bottom-up and top-down processing",
    year = "2000"
}

@mastersthesis{Subedi2014_master,
    author = "Subedi, Bishwa Prasad",
    abstract = "Given an audio query, such as polyphonic musical piece, this thesis address the problem of retrieving a matching (similar) musical score data from a collection of musical scores. There are different techniques for measuring similarity between any musical piece such as metadata based similarity measure, collaborative filtering and content-based similarity measure. In this thesis, we use the information in the digital music itself for similarity measures and this technique is known as content-based similarity measure. First we extract chroma features to represents musical segments. Chroma feature captures both melodic information and harmonic information and is robust to timbre variation. Tempo variation in the performance of a same song may cause dissimilarity between them. In order to address this issue we extract beat sequences and combine them with chroma features to obtain beat synchronous chroma features. Next, we use Dynamic Time Warping (DTW) algorithm. This algorithm first computes the DTW matrix between two feature sequences and calculates the cost of traversing from starting point to end point of the matrix. Minimum the cost value, more similar the musical segments are. The performance of DTW is improved by choosing suitable path constraints and path weight. Then, we implement LSH algorithm, which first indexes the data and then searches for a similar item. Processing time of LSH is shorter than that of DTW. For a smaller fragment of query audio, say 30 seconds, LSH outperformed DTW. Performance of LSH depends on the number of hash tables, number of projections per table and width of the projection. Both algorithms were applied in two types of data sets, RWC (where audio and midi are from the same source) and TUT (where audio and midi are from different sources). The contribution of this thesis is twofold. First we proposed a suitable feature representation of a musical segment for melodic similarity. And then we apply two different similarity measure algorithms and enhance their performances. This thesis work also includes development of mobile application capable of recording audio from surroundings and displaying its acoustic features in real time.",
    institution = "Tampere University of Technology",
    title = "{A}udio-{B}ased {R}etrieval of {M}usical {S}core {D}ata",
    url = "https://dspace.cc.tut.fi/dpub/bitstream/handle/123456789/22388/Subedi.pdf?sequence=1{\\&}isAllowed=y",
    year = "2014",
    school = "Tampere University of Technology"
}

@inproceedings{Drgas2015_LVA_ICA,
    author = "Drgas, Szymon and Virtanen, Tuomas",
    booktitle = "12th International Conference on Latent Variable Analysis and Signal Separation",
    title = "{S}peaker verification using adaptive dictionaries in non-negative spectrogram deconvolution",
    url = "http://link.springer.com/chapter/10.1007\%2F978-3-319-22482-4\_54",
    year = "2015"
}

@article{Gemmeke2011_TASLP,
    author = "Gemmeke, Jort and Virtanen, Tuomas and Hurmalainen, Antti",
    abstract = "This paper proposes to use exemplar-based sparse representations for noise robust automatic speech recognition. First, we describe how speech can be modelled as a linear combination of a small number of exemplars from a large speech exemplar dictionary. The exemplars are time-frequency patches of real speech, each spanning multiple time frames. We then propose to model speech corrupted by additive noise as a linear combination of noise and speech exemplars, and we derive an algorithm for recovering this sparse linear combination of exemplars from the observed noisy speech. We describe how the framework can be used for doing hybrid exemplar-based/HMM recognition by using the exemplar-activations together with the phonetic information associated with the exemplars. As an alternative to hybrid recognition, the framework also allows us to take a source separation approach which enables exemplar-based feature enhancement as well as missing data mask estimation. We evaluate the performance of these exemplar-based methods in connected digit recognition on the AURORA-2 database. Our results show that the hybrid system performed substantially better than source separation or missing data mask estimation at lower SNRs, achieving up to 57.1\% accuracy at SNR= -5 dB. Although not as effective as two baseline recognisers at higher SNRs, the novel approach offers a promising direction of future research on exemplar-based ASR.",
    journal = "IEEE Transactions on Audio, Speech, and Language Processing",
    keywords = "speech recognition",
    month = "September",
    number = "7",
    pages = "2067-2080",
    title = "Exemplar-based Sparse Representations for Noise Robust Automatic Speech Recognition",
    volume = "19",
    year = "2011",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/gemmeke\_taslp11.pdf"
}

@phdthesis{Korhonen2010_phd,
    author = "Korhonen, Teemu",
    institution = "Tampere University of Technology",
    title = "Acoustic Source Localization Utilizing Reflective Surfaces",
    year = "2010",
    school = "Tampere University of Technology"
}

@inproceedings{Saarelainen2000_EUSIPCO,
    author = "Saarelainen, Teemu and Yli-Hietanen, Jari",
    abstract = "The use of small sensor arrays in modern signal processing systems has recently become more common due to the increase in computational processing power and interest in intelligent sensing and surveillance. However, not much information is available on the design of small sensor arrays having arbitrary geometry, that effectively can accomplish these tasks. In this paper we address the problem of designing such small sensor array systems for angle of arrival (AOA) estimation algorithms. Two different cost functions are derived and their applicability is demonstrated in simulation. The accuracy of the AOA estimates is also studied for two different array configurations.",
    booktitle = "2000 10th European Signal Processing Conference",
    pages = "1-4",
    title = "{A} design method for small sensor arrays in angle of arrival estimation",
    year = "2000"
}

@phdthesis{Barker2017_phd,
    author = "Barker, Tom",
    title = "Non-Negative Factorisation Techniques for Sound Source Separation",
    school = "Tampere University of Technology",
    publisher = "Tampere University of Technology",
    series = "Publication / Tampere University of Technology",
    year = "2017"
}

@article{Simsekli2015_DSP,
    author = "Simsekli, Umut and Virtanen, Tuomas and Cemgil, Ali Taylan",
    journal = "Digital Signal Processing",
    title = "{N}on-negative {T}ensor {F}actorization {M}odels for {B}ayesian {A}udio {P}rocessing",
    url = "http://www.sciencedirect.com/science/article/pii/S105120041500086X",
    year = "2015"
}

@mastersthesis{Valenti2016_master,
    author = "Valenti, Michele",
    abstract = "In this thesis we investigate the use of deep neural networks applied to the ﬁeld of computational audio scene analysis, in particular to acoustic scene classiﬁcation. This task concerns the recognition of an acoustic scene, like a park or a home, performed by an artiﬁcial system. In our work we examine the use of deep models aiming to give a contribution in one of their use cases which is, in our opinion, one of the most poorly explored. The neural architecture we propose in this work is a convolutional neural network speciﬁcally designed to work on a time-frequency audio representation known as log-mel spectrogram. The network output is an array of prediction scores, each of which is associated with one class of a set of 15 predeﬁned classes. In addition, the architecture features batch normalization, a recently proposed regularization technique used to enhance the network performance and to speed up its training. We also investigate the use of different audio sequence lengths as classiﬁcation unit for our network. Thanks to these experiments we observe that, for our artiﬁcial system, the recognition of long sequences is not easier than of medium-length sequences, hence highlighting a counterintuitive behaviour. Moreover, we introduce a training procedure which aims to make the best of small datasets by using all the labeled data available for the network training. This procedure, possible under particular circumstances, constitutes a trade-off between an accurate training stop and an increased data representation available to the network. Finally, we compare our model to other systems, proving that its recognition ability can outperform either other neural architectures as well as other state-of-the-art statistical classiﬁers, like support vector machines and Gaussian mixture models. The proposed system reaches good accuracy scores on two different databases collected in 2013 and 2016. The best accuracy scores, obtained according to two cross-validation setups, are 77\% and 79\% respectively. These scores constitute a 22\% and 6.1\% accuracy increment with respect to the correspondent baselines published together with datasets.",
    institution = "Tampere University of Technology",
    title = "{C}onvolutional neural networks for acoustic scene classiﬁcati",
    url = "https://dspace.cc.tut.fi/dpub/bitstream/handle/123456789/24346/Valenti.pdf?sequence=3{\\&}isAllowed=y",
    year = "2016",
    school = "Tampere University of Technology"
}

@inproceedings{Virtanen2001_AES,
    author = "Virtanen, Tuomas",
    address = "Amsterdam, The Netherlands",
    booktitle = "Audio Engineering Society, Convention Paper, Presented at the 110th Convention",
    keywords = "sinusoidal model",
    month = "May",
    title = "Accure Sinusoidal Model Analysis and Parameter Redustion by Fusion of Componets",
    year = "2001"
}

@inproceedings{Virtanen2003_DAFx,
    author = "Virtanen, Tuomas",
    address = "London, England",
    booktitle = "Proceedings of the 6th International Conference on Digital Audio Effects DAFx-03",
    keywords = "sinusoidal model",
    month = "September",
    pages = "35-40",
    title = "Algorithm for the separation of harmonic sounds with time-frequency smoothness constraint",
    year = "2003",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/dafx2003.pdf"
}

@inproceedings{Virtanen2004_ITRW,
    author = "Virtanen, Tuomas",
    editor = "Tutorial, ISCA and on Statistical, Research Workshop and Processing, Perceptual Audio",
    keywords = "sparse coding; NMF",
    title = "Separation of Sound Sources by Convolutive Sparse Coding",
    year = "2004",
    booktitle = "ISCA Tutorial and Research Workshop (ITRW) on Statistical and Perceptual Audio Processing",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/sapa2004.pdf"
}

@inproceedings{Virtanen2006_InterSpecch,
    author = "Virtanen, Tuomas",
    address = "Pittsburgh, USA",
    booktitle = "proc. Interspeech",
    keywords = "speech recognition; HMM;",
    title = "Speech Recognition Using Factorial Hidden Markov Models for Separation in the Feature Space",
    year = "2006",
    url = "https://homepages.tuni.fi/tuomas.virtanen/papers/speech/icslp06.pdf"
}
