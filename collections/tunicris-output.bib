@article{91be230eb3bd4086884c849b4196e2db,
title     = "A pipeline for stochastic and controlled generation of realistic language input for simulating infant language acquisition",
abstract  = "Computational models of early language development involve implementing theories of learning as functional learning algorithms, exposing these models to realistic language input, and comparing learning outcomes to those in infants. While recent research has made major strides in developing more powerful learning models and evaluation protocols grounded in infant data, models are still predominantly trained with non-naturalistic input data, such as crowd-sourced read speech or text transcripts. This is due to the lack of suitable child-directed speech (CDS) corpora in terms of scale and quality. In parallel, the question of how properties and individual variability in language input affect learning outcomes is an active area of empirical research, underlining the need for realistic yet controllable data for modeling such phenomena. This paper presents a solution to the training data problem through stochastic generation of naturalistic CDS data using statistical models, thereby enabling controlled computational simulations with naturalistic input. We provide a proof-of-concept demonstration of the approach by showing how naturalistic CDS transcripts can be generated with a language model conditioned on recipient information (here, infant age), and how text-to-speech systems can be used to convert the transcripts to high-quality speech with a controllable speaking style. We also conduct modeling experiments with generated speech corpora by varying different aspects of the data, showing how this maps into different learning outcomes, thereby demonstrating the feasibility of the approach for controlled language learning simulations. Finally, we discuss the limitations of using synthetic data in general, and of the present proof-of-concept pipeline in particular.",
keywords  = "Computational modeling, Language development, Language resources, Speech processing",
author    = "Okko R{\"a}s{\"a}nen and Daniil Kocharov",
note      = "Publisher Copyright: {\textcopyright} The Author(s) 2025.",
year      = "2025",
month     = oct,
doi       = "10.3758/s13428-025-02772-6",
language  = "English",
volume    = "57",
journal   = "BEHAVIOR RESEARCH METHODS",
issn      = "1554-351X",
publisher = "Springer Nature",
number    = "10"
}

@article{4bf4dbb8e4bd4af4a79328947bef7f24,
title     = "Computer Audition: From Task-Specific Machine Learning to Foundation Models",
abstract  = "Foundation models (FMs) are increasingly spearheading recent advances on a variety of tasks that fall under the purview of computer auditionâ€”i.e., the use of machines to understand sounds. They feature several advantages over traditional pipelines: among others, the ability to consolidate multiple tasks in a single model, the option to leverage knowledge from other modalities, and the readily available interaction with human users. Naturally, these promises have created substantial excitement in the audio community and have led to a wave of early attempts to build new, general-purpose FMs for audio. In the present contribution, we give an overview of computational audio analysis as it transitions from traditional pipelines toward auditory FMs. Our work highlights the key operating principles that underpin those models and showcases how they can accommodate multiple tasks that the audio community previously tackled separately.",
keywords  = "Acoustic scene classification, artificial intelligence (AI), audio captioning (AC), computational audio analysis, computer audition, foundation models (FMs), large audio models, machine listening, sound event detection (SED)",
author    = "Andreas Triantafyllopoulos and Iosif Tsangko and Alexander Gebhard and Annamaria Mesaros and Tuomas Virtanen and Schuller, \{Bj{\"o}rn W.\}",
note      = "Publisher Copyright: {\textcopyright} 1963-2012 IEEE.",
year      = "2025",
month     = aug,
doi       = "10.1109/JPROC.2025.3593952",
language  = "English",
volume    = "113",
pages     = "317 -- 343",
journal   = "Proceedings of the IEEE",
issn      = "0018-9219",
publisher = "Institute of Electrical and Electronics Engineers Inc.",
number    = "4"
}


@article{e76165b7eddb4394b46c995afd97df2b,
title     = "Simulating prenatal language exposure in computational models: An exploration study",
abstract  = "Researchers have hypothesized that infant language learning starts from the third trimester of pregnancy. This is supported by studies with fetuses and newborns showing discrimination/preference for their native language. Jointly with empirical research, initial computational modeling studies have investigated whether learning language patterns from speech input benefits from auditory prenatal language exposure (PLE), showing some advantages for prior adaptation to speech-like patterns. However, these modeling studies have not modeled prenatal speech input in an ecologically representative manner regarding quality or quantity. This study describes an ecologically representative framework for modeling PLE for full-term and preterm infants. The approach is based on empirical estimates of the amount of prenatal speech input together with a model of speech signal attenuation from the external air to the fetus{\textquoteright} auditory system. Using this framework, we conduct language learning simulations with computational models that learn from acoustic speech input in an unsupervised manner. We compare the effects of PLE to standard learning from only postnatal input on various early language phenomena. The results show how incorporating PLE can affect models{\textquoteright} learning outcomes, including differences between full-term and preterm conditions. Moreover, PLE duration might influence model behavior, depending on the linguistic capability being tested. While the inclusion of PLE did not improve the compatibility of the tested models with empirical infant data, our study highlights the relevance of PLE as a factor in modeling studies. Moreover, it provides a basic framework for modeling the prenatal period in future computational studies.",
keywords  = "Child language development, Computational modeling, Language acquisition, Prenatal language exposure",
author    = "\{Cruz Bland{\'o}n\}, \{Mar{\'i}a Andrea\} and Nayeli Gonzalez-Gomez and Marvin Lavechin and Okko R{\"a}s{\"a}nen",
note      = "Publisher Copyright: {\textcopyright} 2024 The Authors",
year      = "2025",
month     = mar,
doi       = "10.1016/j.cognition.2024.106044",
language  = "English",
volume    = "256",
journal   = "COGNITION",
issn      = "0010-0277",
publisher = "Elsevier B.V."
}


@article{459d90f0ad0a4e5e9690a8fb802052b7,
title     = "A model of early word acquisition based on realistic-scale audiovisual naming events",
abstract  = "Infants gradually learn to parse continuous speech into words and connect names with objects, yet the mechanisms behind development of early word perception skills remain unknown. We studied the extent to which early words can be acquired through statistical learning from regularities in audiovisual sensory input. We simulated word learning in infants up to 12 months of age in a realistic setting, using a model that solely learns from statistical regularities in unannotated raw speech and pixel-level visual input. Crucially, the quantity of object naming events was carefully designed to match that accessible to infants of comparable ages. Results show that the model effectively learns to recognize words and associate them with corresponding visual objects, with a vocabulary growth rate comparable to that observed in infants. The findings support the viability of general statistical learning for early word perception, demonstrating how learning can operate without assuming any prior linguistic capabilities.",
keywords  = "Associative learning, Computational modeling, Statistical learning, Word acquisition",
author    = "Khazar Khorrami and Okko R{\"a}s{\"a}nen",
note      = "Publisher Copyright: {\textcopyright} 2024 The Authors",
year      = "2025",
month     = feb,
doi       = "10.1016/j.specom.2024.103169",
language  = "English",
volume    = "167",
journal   = "Speech Communication",
issn      = "0167-6393",
publisher = "Elsevier B.V."
}


@inproceedings{bbbcdf086d9e4cfca6ce8974b4570b45,
title     = "A decade of DCASE: Achievements, practices, evaluations and future challenges",
abstract  = "This paper introduces briefly the history and growth of the Detection and Classification of Acoustic Scenes and Events (DCASE) challenge, workshop, research area and research community. Created in 2013 as a data evaluation challenge, DCASE has become a major research topic in the Audio and Acoustic Signal Processing area. Its success comes from a combination of factors: the challenge offers a large variety of tasks that are renewed each year; and the workshop offers a channel for dissemination of related work, engaging a young and dynamic community. At the same time, DCASE faces its own challenges, growing and expanding to different areas. One of the core principles of DCASE is open science and reproducibility: publicly available datasets, baseline systems, technical reports and workshop publications. While the DCASE challenge and workshop are independent of IEEE SPS, the challenge receives annual endorsement from the AASP TC, and the DCASE community contributes significantly to the ICASSP flagship conference and the success of SPS in many of its activities.",
keywords  = "AASP Challenges, DCASE Challenge, DCASE Workshop",
author    = "Annamaria Mesaros and Romain Serizel and Toni Heittola and Tuomas Virtanen and Plumbley, \{Mark D.\}",
note      = "Publisher Copyright: {\textcopyright} 2025 IEEE.; IEEE International Conference on Acoustics, Speech, and Signal Processing ; Conference date: 06-04-2025 Through 11-04-2025",
year      = "2025",
doi       = "10.1109/ICASSP49660.2025.10887673",
language  = "English",
series    = "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing",
publisher = "IEEE",
pages     = "1--5",
editor    = "Rao, \{Bhaskar D\} and Isabel Trancoso and Gaurav Sharma and Mehta, \{Neelesh B.\}",
booktitle = "ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
address   = "United States"
}


@article{259be4634a1e4ba792d9ae32d5164b7f,
title     = "Automatic Classification of Strain in the Singing Voice Using Machine Learning",
author    = "Yuanyuan Liu and \{Kiran Reddy\}, Mittapalle and Yagnavajjula, \{Madhu Keerthana\} and Okko R{\"a}s{\"a}nen and Paavo Alku and Tero Ik{\"a}valko and Tua Hakanp{\"a}{\"a} and Aleksi {\"O}yry and Anne-Maria Laukkanen",
year      = "2025",
doi       = "10.1016/j.jvoice.2025.03.040",
language  = "English",
journal   = "Journal of Voice",
issn      = "0892-1997",
publisher = "Elsevier"
}


@article{215a27c0e6fc43d3b519c7d384dcec1c,
title     = "AVCaps: An Audio-Visual Dataset With Modality-Specific Captions",
abstract  = "This paper introduces AVCaps, an audio-visual dataset that contains separate textual captions for the audio, visual, and audio-visual contents of video clips. The dataset contains 2061 video clips constituting a total of 28.8 hours. We provide up to 5 captions for the audio, visual, and audio-visual content of each clip, crowdsourced separately. Existing datasets focus on a single modality or do not provide modality-specific captions, limiting the study of how each modality contributes to overall comprehension in multimodal settings. Our dataset addresses this critical gap in multimodal research by offering a resource for studying how audio and visual content are captioned individually, as well as how audio-visual content is captioned in relation to these individual modalities. Crowdsourced audio-visual captions are prone to favor visual content over audio content. To avoid this we use large language models (LLMs) to generate three balanced audio-visual captions for each clip based on the crowdsourced captions. We present captioning and retrieval experiments to illustrate the effectiveness of modality-specific captions in evaluating model performance. Specifically, we show that the modality-specific captions allow us to quantitatively assess how well a model understands audio and visual information from a given video. Notably, we find that a model trained on the balanced LLM-generated audio-visual captions captures audio information more effectively compared to a model trained on crowdsourced audio-visual captions. This model achieves a 14\% higher Sentence-BERT similarity on crowdsourced audio captions compared to a model trained on crowdsourced audio-visual captions, which are typically more biased towards visual information. We also discuss the possibilities in multimodal representation learning, question answering, developing new video captioning metrics, and generative AI that this dataset unlocks. The dataset is available publicly at Zenodo and Hugging Face.",
keywords  = "Audio-visual, AVCaps, Captioning, Dataset, Multimodal, Retrieval",
author    = "Parthasaarathy Sudarsanam and Irene Martin-Morato and Aapo Hakala and Tuomas Virtanen",
note      = "Publisher Copyright: {\textcopyright} 2020 IEEE.",
year      = "2025",
doi       = "10.1109/OJSP.2025.3578296",
language  = "English",
volume    = "6",
pages     = "691--704",
journal   = "IEEE Open Journal of Signal Processing",
issn      = "2644-1322",
publisher = "IEEE"
}


@inproceedings{b99fc1baff3e48f8b424e72515aed3f8,
title     = "Gen-A: Generalizing Ambisonics Neural Encoding to Unseen Microphone Arrays",
abstract  = "Using deep neural networks (DNNs) for encoding of microphone array (MA) signals to the Ambisonics spatial audio format can surpass certain limitations of established conventional methods, but existing DNN-based methods need to be trained separately for each MA. This paper proposes a DNN-based method for Ambisonics encoding that can generalize to arbitrary MA geometries unseen during training. The method takes as inputs the MA geometry and MA signals and uses a multi-level encoder consisting of separate paths for geometry and signal data, where geometry features inform the signal encoder at each level. The method is validated in simulated anechoic and reverberant conditions with one and two sources. The results indicate improvement over conventional encoding across the whole frequency range for dry scenes, while for reverberant scenes the improvement is frequency-dependent.",
keywords  = "Ambisonics, deep learning, microphone array, Spatial audio",
author    = "Mikko Heikkinen and Archontis Politis and Konstantinos Drossos and Tuomas Virtanen",
note      = "Publisher Copyright: {\textcopyright} 2025 IEEE.; IEEE International Conference on Acoustics, Speech, and Signal Processing ; Conference date: 06-04-2025 Through 11-04-2025",
year      = "2025",
doi       = "10.1109/ICASSP49660.2025.10887869",
language  = "English",
series    = "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing",
publisher = "IEEE",
pages     = "1--5",
editor    = "Rao, \{Bhaskar D\} and Isabel Trancoso and Gaurav Sharma and Mehta, \{Neelesh B.\}",
booktitle = "ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
address   = "United States"
}
